{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf922e13",
   "metadata": {
    "papermill": {
     "duration": 0.004389,
     "end_time": "2023-10-30T01:36:19.334050",
     "exception": false,
     "start_time": "2023-10-30T01:36:19.329661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- focus on nn modelling\n",
    "- save checkpoint files \n",
    "- separate test inference *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909059ab",
   "metadata": {
    "papermill": {
     "duration": 0.003907,
     "end_time": "2023-10-30T01:36:19.342155",
     "exception": false,
     "start_time": "2023-10-30T01:36:19.338248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# common class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc02f4ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T01:36:19.352915Z",
     "iopub.status.busy": "2023-10-30T01:36:19.351425Z",
     "iopub.status.idle": "2023-10-30T01:36:38.443869Z",
     "shell.execute_reply": "2023-10-30T01:36:38.441715Z"
    },
    "papermill": {
     "duration": 19.101466,
     "end_time": "2023-10-30T01:36:38.447526",
     "exception": false,
     "start_time": "2023-10-30T01:36:19.346060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append('../input/iterativestratification')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning import LightningModule, LightningDataModule, Trainer, seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "574266c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T01:36:38.458760Z",
     "iopub.status.busy": "2023-10-30T01:36:38.458303Z",
     "iopub.status.idle": "2023-10-30T01:36:38.493374Z",
     "shell.execute_reply": "2023-10-30T01:36:38.492377Z"
    },
    "papermill": {
     "duration": 0.04399,
     "end_time": "2023-10-30T01:36:38.495851",
     "exception": false,
     "start_time": "2023-10-30T01:36:38.451861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer\n",
    "\n",
    "#####\n",
    "class MyModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(config[\"num_columns\"])\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(config[\"num_columns\"], 512))\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(512)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(512, 512))\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(512)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(512, config[\"last_num\"]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu1(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu2(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        x = torch.squeeze(x)\n",
    "        \n",
    "        return x\n",
    "#####\n",
    "\n",
    "\n",
    "class MyModule(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([4.0]))\n",
    "        self.model = MyModel()\n",
    "        self.log_outputs = {}\n",
    "        self.validation_step_outputs = []\n",
    "        self.train_step_outputs = []\n",
    "        \n",
    "    #####\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr = config[\"lr\"])\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.9)\n",
    "        return [optimizer], [scheduler]\n",
    "    #####\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        preds = self.forward(inputs)        \n",
    "        loss = self.loss_fn(preds, targets)        \n",
    "        self.train_step_outputs.append(loss)\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        preds = self.forward(inputs)\n",
    "        loss = self.loss_fn(preds, targets)\n",
    "        output = {\"targets\": targets.detach(), \"preds\": preds.detach(), \"loss\": loss.detach()}\n",
    "        self.validation_step_outputs.append(output)\n",
    "                \n",
    "        return output\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        preds = self.forward(inputs)\n",
    "                \n",
    "        return preds\n",
    "    \n",
    "    def on_train_start(self) -> None:\n",
    "        self.print(f\"Train start\")\n",
    "        return super().on_train_start()\n",
    "    \n",
    "    def on_train_end(self) -> None:\n",
    "        self.print(\" \")\n",
    "        return super().on_train_end()\n",
    "    \n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        train_loss = torch.stack([x for x in self.train_step_outputs]).mean()\n",
    "        self.log_dict({\"loss\": train_loss})\n",
    "        self.log_outputs[\"loss\"] = train_loss\n",
    "        \n",
    "        train_loss     = self.log_outputs[\"loss\"]\n",
    "        valid_loss     = self.log_outputs[\"valid_loss\"]\n",
    "        self.print(f\"loss: {train_loss:.3f} - val_loss: {valid_loss:.3f}\")\n",
    "        \n",
    "        return super().on_train_epoch_end()\n",
    "        \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        valid_loss = torch.stack([x[\"loss\"] for x in self.validation_step_outputs]).mean()\n",
    "        \n",
    "        self.log_dict({\"valid_loss\": valid_loss})\n",
    "        self.log_outputs[\"valid_loss\"] = valid_loss\n",
    "\n",
    "        return super().on_validation_epoch_end()\n",
    "    \n",
    "    \n",
    "class MyDataModule(LightningModule):\n",
    "    def __init__(self, train, target, feats, fold, batch_size = 32):\n",
    "        super(MyDataModule, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train = train\n",
    "        self.target = target\n",
    "        self.feats = feats\n",
    "        self.fold = fold\n",
    "        self.x_train = None\n",
    "        self.x_valid = None\n",
    "        self.y_train = None\n",
    "        self.y_valid = None\n",
    "        \n",
    "    #####\n",
    "    def split_train_valid_df(self):\n",
    "        skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "        for n, (tr_index, val_index) in enumerate(skf.split(self.train, self.target)):\n",
    "            if n == self.fold: \n",
    "                x_train = self.train.loc[tr_index].reset_index(drop=True)\n",
    "                x_valid = self.train.loc[val_index].reset_index(drop=True)\n",
    "                y_train = self.target.loc[tr_index].reset_index(drop=True)\n",
    "                y_valid = self.target.loc[val_index].reset_index(drop=True)\n",
    "        \n",
    "        return x_train, x_valid, y_train, y_valid\n",
    "    #####\n",
    "    \n",
    "    def setup(self, stage):\n",
    "        x_tr, x_va, y_tr, y_va = self.split_train_valid_df()\n",
    "        self.x_train = x_tr\n",
    "        self.x_valid = x_va\n",
    "        self.y_train = y_tr \n",
    "        self.y_valid = y_va\n",
    "        \n",
    "    def get_dataframe(self, phase):\n",
    "        assert phase in [\"train\", \"valid\"]\n",
    "        if phase == \"train\":\n",
    "            return self.x_train, self.y_train\n",
    "        elif phase == \"valid\":\n",
    "            return self.x_valid, self.y_valid\n",
    "        \n",
    "    def get_ds(self, phase):\n",
    "        x, y = self.get_dataframe(phase)\n",
    "        return MyDataset(df = x, target = y, feats = self.feats, phase = phase)\n",
    "        \n",
    "    def get_loader(self, phase):\n",
    "        assert phase in [\"train\", \"valid\"]\n",
    "        ds = self.get_ds(phase = phase)\n",
    "        return DataLoader(ds, batch_size = self.batch_size, num_workers = 4,\n",
    "                        shuffle = True if phase == \"train\" else False,\n",
    "                        drop_last = True if phase == \"train\" else False)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self.get_loader(\"train\")\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.get_loader(\"valid\")\n",
    "    \n",
    "    \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, target, feats, phase = \"train\"):\n",
    "        self.phase = phase \n",
    "        self.feats = feats\n",
    "        self.data = df[feats]\n",
    "        self.target = target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.phase in ['train', \"valid\"]:\n",
    "            return self.data.values[index].astype(float), self.target.values[index].astype(float)\n",
    "        elif self.phase == 'test':\n",
    "            return self.data.values[index].astype(float), 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf970ee",
   "metadata": {
    "papermill": {
     "duration": 0.00392,
     "end_time": "2023-10-30T01:36:38.504123",
     "exception": false,
     "start_time": "2023-10-30T01:36:38.500203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "458f1b44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T01:36:38.514850Z",
     "iopub.status.busy": "2023-10-30T01:36:38.513819Z",
     "iopub.status.idle": "2023-10-30T01:36:38.938958Z",
     "shell.execute_reply": "2023-10-30T01:36:38.936560Z"
    },
    "papermill": {
     "duration": 0.436715,
     "end_time": "2023-10-30T01:36:38.944872",
     "exception": false,
     "start_time": "2023-10-30T01:36:38.508157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 72) (5, 72)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "    nc = np.bincount(y_true)\n",
    "    return log_loss(y_true, y_pred, sample_weight = 1/nc[y_true], eps=1e-15)\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\n",
    "greeks = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/greeks.csv')\n",
    "\n",
    "train['EJ'] = train['EJ'].map({'A': 0, 'B': 1})\n",
    "test['EJ']  = test['EJ'].map({'A': 0, 'B': 1})\n",
    "\n",
    "# process epsilon\n",
    "train = pd.merge(train, greeks, on = \"Id\", how = \"inner\")\n",
    "train_stratify = train[[\"Class\", \"Beta\", \"Delta\", \"Gamma\"]] \n",
    "train[\"Epsilon_ordinal\"] = train[\"Epsilon\"].map(lambda x: datetime.strptime(x,'%m/%d/%Y').toordinal() if x != \"Unknown\" else np.nan)\n",
    "\n",
    "org_features = [n for n in train.columns if n not in ['Class', 'Id', 'Alpha', \"Beta\", \"Gamma\", \"Delta\", \"Epsilon\"]]\n",
    "test_times = pd.DataFrame([train.Epsilon_ordinal.max() + 1] * len(test), columns = [\"Epsilon_ordinal\"])\n",
    "final_test = pd.concat((test, test_times), axis=1)\n",
    "\n",
    "# fill missing value\n",
    "train.fillna(-999, inplace=True)\n",
    "final_test.fillna(-999, inplace=True)\n",
    "\n",
    "# add pca columns\n",
    "pca_feat_num = 15\n",
    "pca_cols = [\"pca\"+str(i+1) for i in range(pca_feat_num)]\n",
    "pca = PCA(n_components=pca_feat_num,random_state=42)\n",
    "pca_train = pca.fit_transform(train[org_features])\n",
    "pca_test = pca.transform(final_test[org_features])\n",
    "pca_train = pd.DataFrame(pca_train, columns=pca_cols)\n",
    "pca_test = pd.DataFrame(pca_test, columns=pca_cols)\n",
    "train = pd.concat([train, pca_train],axis=1)\n",
    "final_test = pd.concat([final_test, pca_test],axis=1)\n",
    "\n",
    "scalar = MinMaxScaler()\n",
    "cons_feats = org_features + pca_cols\n",
    "normalize_train = scalar.fit_transform(train[cons_feats])\n",
    "normalize_train = pd.DataFrame(normalize_train, columns = cons_feats)\n",
    "normalize_test = scalar.transform(final_test[cons_feats])\n",
    "normalize_test = pd.DataFrame(normalize_test, columns = cons_feats)\n",
    "\n",
    "print(normalize_train.shape, normalize_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83570bb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T01:36:38.970040Z",
     "iopub.status.busy": "2023-10-30T01:36:38.969188Z",
     "iopub.status.idle": "2023-10-30T01:36:38.978561Z",
     "shell.execute_reply": "2023-10-30T01:36:38.977164Z"
    },
    "papermill": {
     "duration": 0.027815,
     "end_time": "2023-10-30T01:36:38.981550",
     "exception": false,
     "start_time": "2023-10-30T01:36:38.953735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"feats\": cons_feats,\n",
    "    \"n_splits\" : 5,\n",
    "    \"train_data\": normalize_train,\n",
    "    \"fold_y\": train_stratify,\n",
    "    \"target\": train.Class,\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 10,\n",
    "    \"lr\": 0.01,\n",
    "    \"metric_function\" : balanced_log_loss,\n",
    "    \"last_num\":  1,\n",
    "}\n",
    "\n",
    "config[\"num_columns\"] = len(config[\"feats\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf46c3f",
   "metadata": {
    "papermill": {
     "duration": 0.004175,
     "end_time": "2023-10-30T01:36:38.993386",
     "exception": false,
     "start_time": "2023-10-30T01:36:38.989211",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96c308ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T01:36:39.003771Z",
     "iopub.status.busy": "2023-10-30T01:36:39.003324Z",
     "iopub.status.idle": "2023-10-30T01:37:25.028543Z",
     "shell.execute_reply": "2023-10-30T01:37:25.026871Z"
    },
    "papermill": {
     "duration": 46.034417,
     "end_time": "2023-10-30T01:37:25.031971",
     "exception": false,
     "start_time": "2023-10-30T01:36:38.997554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train start\n",
      "loss: 0.775 - val_loss: 0.932\n",
      "loss: 0.647 - val_loss: 0.772\n",
      "loss: 0.563 - val_loss: 0.812\n",
      "loss: 0.519 - val_loss: 0.958\n",
      "loss: 0.481 - val_loss: 1.255\n",
      " \n",
      "Train start\n",
      "loss: 0.878 - val_loss: 0.773\n",
      "loss: 0.661 - val_loss: 0.765\n",
      "loss: 0.571 - val_loss: 0.725\n",
      "loss: 0.529 - val_loss: 0.685\n",
      "loss: 0.489 - val_loss: 0.690\n",
      "loss: 0.452 - val_loss: 0.685\n",
      "loss: 0.420 - val_loss: 0.703\n",
      " \n",
      "Train start\n",
      "loss: 0.998 - val_loss: 0.865\n",
      "loss: 0.770 - val_loss: 0.816\n",
      "loss: 0.644 - val_loss: 0.782\n",
      "loss: 0.579 - val_loss: 0.748\n",
      "loss: 0.536 - val_loss: 0.712\n",
      "loss: 0.512 - val_loss: 0.718\n",
      "loss: 0.470 - val_loss: 0.699\n",
      "loss: 0.436 - val_loss: 0.713\n",
      "loss: 0.415 - val_loss: 0.705\n",
      "loss: 0.391 - val_loss: 0.693\n",
      " \n",
      "Train start\n",
      "loss: 1.028 - val_loss: 0.808\n",
      "loss: 0.786 - val_loss: 0.694\n",
      "loss: 0.658 - val_loss: 0.685\n",
      "loss: 0.574 - val_loss: 0.657\n",
      "loss: 0.518 - val_loss: 0.668\n",
      "loss: 0.474 - val_loss: 0.673\n",
      "loss: 0.447 - val_loss: 0.683\n",
      " \n",
      "Train start\n",
      "loss: 0.927 - val_loss: 0.842\n",
      "loss: 0.765 - val_loss: 0.752\n",
      "loss: 0.654 - val_loss: 0.672\n",
      "loss: 0.578 - val_loss: 0.644\n",
      "loss: 0.521 - val_loss: 0.609\n",
      "loss: 0.482 - val_loss: 0.602\n",
      "loss: 0.456 - val_loss: 0.570\n",
      "loss: 0.421 - val_loss: 0.583\n",
      "loss: 0.397 - val_loss: 0.598\n",
      "loss: 0.380 - val_loss: 0.584\n",
      " \n"
     ]
    }
   ],
   "source": [
    "seed_everything(42, workers=True)\n",
    "    \n",
    "for fold in range(config[\"n_splits\"]):\n",
    "    callbacks = []\n",
    "    es_callback = EarlyStopping(monitor='valid_loss', patience=3)\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"valid_loss\", dirpath=f\"./checkpoints-{fold}\", filename=f\"model\", save_top_k=1, mode=\"min\",)\n",
    "    callbacks.append(es_callback)\n",
    "    callbacks.append(checkpoint_callback)\n",
    "\n",
    "    # train\n",
    "    trainer = Trainer(max_epochs = config[\"epochs\"], callbacks=callbacks, enable_progress_bar = False, log_every_n_steps = 10)\n",
    "    model = MyModule().to(\"cpu\", dtype=float)\n",
    "    data_module = MyDataModule(train = config[\"train_data\"], feats = config[\"feats\"], \n",
    "                           fold = fold, target = config[\"target\"], batch_size = config[\"batch_size\"])\n",
    "    trainer.fit(model, datamodule = data_module)\n",
    "    \n",
    "    valid_loader = data_module.get_loader(\"valid\")\n",
    "    if fold == 0:\n",
    "        valid_preds = trainer.predict(model, dataloaders = valid_loader, ckpt_path = \"best\")\n",
    "\n",
    "        valid_preds = torch.cat(valid_preds)\n",
    "        valid_target = data_module.y_valid.values        \n",
    "    else:\n",
    "        tmp_preds = trainer.predict(model, dataloaders = valid_loader, ckpt_path = \"best\")\n",
    "        tmp_preds = torch.cat(tmp_preds)\n",
    "        tmp_target = data_module.y_valid.values\n",
    "        \n",
    "        valid_target = np.hstack((valid_target, tmp_target))\n",
    "        valid_preds = torch.cat((valid_preds, tmp_preds))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5374eb51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T01:37:25.049705Z",
     "iopub.status.busy": "2023-10-30T01:37:25.048470Z",
     "iopub.status.idle": "2023-10-30T01:37:25.065240Z",
     "shell.execute_reply": "2023-10-30T01:37:25.064062Z"
    },
    "papermill": {
     "duration": 0.028454,
     "end_time": "2023-10-30T01:37:25.067828",
     "exception": false,
     "start_time": "2023-10-30T01:37:25.039374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3629146930933221\n"
     ]
    }
   ],
   "source": [
    "print(config[\"metric_function\"](valid_target, torch.sigmoid(valid_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82e806",
   "metadata": {
    "papermill": {
     "duration": 0.007194,
     "end_time": "2023-10-30T01:37:25.082218",
     "exception": false,
     "start_time": "2023-10-30T01:37:25.075024",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bb8b6c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T01:37:25.100448Z",
     "iopub.status.busy": "2023-10-30T01:37:25.099077Z",
     "iopub.status.idle": "2023-10-30T01:37:25.229282Z",
     "shell.execute_reply": "2023-10-30T01:37:25.228082Z"
    },
    "papermill": {
     "duration": 0.142091,
     "end_time": "2023-10-30T01:37:25.232260",
     "exception": false,
     "start_time": "2023-10-30T01:37:25.090169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_test = torch.tensor(normalize_test[cons_feats].values.astype(float))\n",
    "\n",
    "for each_fold in range(config[\"n_splits\"]):\n",
    "    test_model = MyModule.load_from_checkpoint(f\"checkpoints-{each_fold}/model.ckpt\").to(\"cpu\", dtype=float)\n",
    "    test_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_preds = test_model(input_test)\n",
    "    \n",
    "test_preds /= config[\"n_splits\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e961f0",
   "metadata": {
    "papermill": {
     "duration": 0.006652,
     "end_time": "2023-10-30T01:37:25.246123",
     "exception": false,
     "start_time": "2023-10-30T01:37:25.239471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 82.337706,
   "end_time": "2023-10-30T01:37:28.130065",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-30T01:36:05.792359",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
