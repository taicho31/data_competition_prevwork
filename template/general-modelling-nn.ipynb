{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8324a3b7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-17T01:02:48.871576Z",
     "iopub.status.busy": "2025-04-17T01:02:48.871132Z",
     "iopub.status.idle": "2025-04-17T01:03:12.502466Z",
     "shell.execute_reply": "2025-04-17T01:03:12.501356Z"
    },
    "papermill": {
     "duration": 23.637382,
     "end_time": "2025-04-17T01:03:12.504180",
     "exception": false,
     "start_time": "2025-04-17T01:02:48.866798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning import LightningModule, LightningDataModule, Trainer, seed_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa41229",
   "metadata": {
    "papermill": {
     "duration": 0.002135,
     "end_time": "2025-04-17T01:03:12.509216",
     "exception": false,
     "start_time": "2025-04-17T01:03:12.507081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ad59fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T01:03:12.516054Z",
     "iopub.status.busy": "2025-04-17T01:03:12.515162Z",
     "iopub.status.idle": "2025-04-17T01:03:12.707056Z",
     "shell.execute_reply": "2025-04-17T01:03:12.704052Z"
    },
    "papermill": {
     "duration": 0.197527,
     "end_time": "2025-04-17T01:03:12.709226",
     "exception": false,
     "start_time": "2025-04-17T01:03:12.511699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 72) (5, 72)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\n",
    "greeks = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/greeks.csv')\n",
    "\n",
    "train['EJ'] = train['EJ'].map({'A': 0, 'B': 1})\n",
    "test['EJ']  = test['EJ'].map({'A': 0, 'B': 1})\n",
    "\n",
    "# process epsilon\n",
    "train = pd.merge(train, greeks, on = \"Id\", how = \"inner\")\n",
    "train_stratify = train[[\"Class\", \"Beta\", \"Delta\", \"Gamma\"]] \n",
    "train[\"Epsilon_ordinal\"] = train[\"Epsilon\"].map(lambda x: datetime.strptime(x,'%m/%d/%Y').toordinal() if x != \"Unknown\" else np.nan)\n",
    "\n",
    "org_features = [n for n in train.columns if n not in ['Class', 'Id', 'Alpha', \"Beta\", \"Gamma\", \"Delta\", \"Epsilon\"]]\n",
    "test_times = pd.DataFrame([train.Epsilon_ordinal.max() + 1] * len(test), columns = [\"Epsilon_ordinal\"])\n",
    "final_test = pd.concat((test, test_times), axis=1)\n",
    "\n",
    "# fill missing value\n",
    "train.fillna(-999, inplace=True)\n",
    "final_test.fillna(-999, inplace=True)\n",
    "\n",
    "# add pca columns\n",
    "pca_feat_num = 15\n",
    "pca_cols = [\"pca\"+str(i+1) for i in range(pca_feat_num)]\n",
    "pca = PCA(n_components=pca_feat_num,random_state=42)\n",
    "pca_train = pca.fit_transform(train[org_features])\n",
    "pca_test = pca.transform(final_test[org_features])\n",
    "pca_train = pd.DataFrame(pca_train, columns=pca_cols)\n",
    "pca_test = pd.DataFrame(pca_test, columns=pca_cols)\n",
    "train = pd.concat([train, pca_train],axis=1)\n",
    "final_test = pd.concat([final_test, pca_test],axis=1)\n",
    "\n",
    "scalar = MinMaxScaler()\n",
    "cons_feats = org_features + pca_cols\n",
    "normalize_train = scalar.fit_transform(train[cons_feats])\n",
    "normalize_train = pd.DataFrame(normalize_train, columns = cons_feats)\n",
    "normalize_test = scalar.transform(final_test[cons_feats])\n",
    "normalize_test = pd.DataFrame(normalize_test, columns = cons_feats)\n",
    "\n",
    "print(normalize_train.shape, normalize_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf81596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T01:03:12.719014Z",
     "iopub.status.busy": "2025-04-17T01:03:12.718657Z",
     "iopub.status.idle": "2025-04-17T01:03:12.725113Z",
     "shell.execute_reply": "2025-04-17T01:03:12.723302Z"
    },
    "papermill": {
     "duration": 0.013768,
     "end_time": "2025-04-17T01:03:12.726937",
     "exception": false,
     "start_time": "2025-04-17T01:03:12.713169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def balanced_log_loss(y_true, y_pred):\n",
    "    nc = np.bincount(y_true)\n",
    "    return log_loss(y_true, y_pred, sample_weight = 1/nc[y_true], eps=1e-15)\n",
    "\n",
    "config = {\n",
    "    \"n_splits\" : 5,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 10,\n",
    "    \"metric_function\" : balanced_log_loss,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ecb222f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T01:03:12.735991Z",
     "iopub.status.busy": "2025-04-17T01:03:12.735643Z",
     "iopub.status.idle": "2025-04-17T01:03:12.762067Z",
     "shell.execute_reply": "2025-04-17T01:03:12.760747Z"
    },
    "papermill": {
     "duration": 0.0331,
     "end_time": "2025-04-17T01:03:12.763842",
     "exception": false,
     "start_time": "2025-04-17T01:03:12.730742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer\n",
    "\n",
    "class MyModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(len(cons_feats))\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(len(cons_feats), 512))\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(512)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(512, 512))\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(512)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(512, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu1(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu2(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        x = torch.squeeze(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class MyModule(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([4.0]))\n",
    "        self.model = MyModel()\n",
    "        self.log_outputs = {}\n",
    "        self.validation_step_outputs = []\n",
    "        self.train_step_outputs = []\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr = 0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.9)\n",
    "        return [optimizer], [scheduler]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        preds = self.forward(inputs)        \n",
    "        loss = self.loss_fn(preds, targets)        \n",
    "        self.train_step_outputs.append(loss)\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        preds = self.forward(inputs)\n",
    "        loss = self.loss_fn(preds, targets)\n",
    "        output = {\"targets\": targets.detach(), \"preds\": preds.detach(), \"loss\": loss.detach()}\n",
    "        self.validation_step_outputs.append(output)\n",
    "                \n",
    "        return output\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        preds = self.forward(inputs)\n",
    "                \n",
    "        return preds\n",
    "    \n",
    "    def on_train_start(self) -> None:\n",
    "        self.print(\"Train start\")\n",
    "        return super().on_train_start()\n",
    "    \n",
    "    def on_train_end(self) -> None:\n",
    "        self.print(\"Train end\")\n",
    "        return super().on_train_end()\n",
    "    \n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        train_loss = torch.stack([x for x in self.train_step_outputs]).mean()\n",
    "        self.log_dict({\"loss\": train_loss})\n",
    "        self.log_outputs[\"loss\"] = train_loss\n",
    "        \n",
    "        train_loss     = self.log_outputs[\"loss\"]\n",
    "        valid_loss     = self.log_outputs[\"valid_loss\"]\n",
    "        self.print(f\"loss: {train_loss:.3f} - val_loss: {valid_loss:.3f}\")\n",
    "        \n",
    "        return super().on_train_epoch_end()\n",
    "        \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        valid_loss = torch.stack([x[\"loss\"] for x in self.validation_step_outputs]).mean()\n",
    "        \n",
    "        self.log_dict({\"valid_loss\": valid_loss})\n",
    "        self.log_outputs[\"valid_loss\"] = valid_loss\n",
    "\n",
    "        return super().on_validation_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca864682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T01:03:12.770869Z",
     "iopub.status.busy": "2025-04-17T01:03:12.769922Z",
     "iopub.status.idle": "2025-04-17T01:03:12.780627Z",
     "shell.execute_reply": "2025-04-17T01:03:12.779610Z"
    },
    "papermill": {
     "duration": 0.015582,
     "end_time": "2025-04-17T01:03:12.782063",
     "exception": false,
     "start_time": "2025-04-17T01:03:12.766481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyDataModule(LightningDataModule):\n",
    "    def __init__(self, x, y = None, batch_size = 32):\n",
    "        super(MyDataModule, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def get_ds(self, phase):\n",
    "        return MyDataset(df = self.x, target = self.y, phase = phase)\n",
    "        \n",
    "    def get_loader(self, phase):\n",
    "        assert phase in [\"train\", \"valid\", \"test\"]\n",
    "        ds = self.get_ds(phase = phase)\n",
    "        return DataLoader(ds, batch_size = self.batch_size, num_workers = 4,\n",
    "                        shuffle = True if phase == \"train\" else False,\n",
    "                        drop_last = True if phase == \"train\" else False)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self.get_loader(\"train\")\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.get_loader(\"valid\")\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.get_loader(\"test\")\n",
    "\n",
    "\n",
    "class CombinedDataModule(LightningDataModule):\n",
    "    def __init__(self, train_module: MyDataModule, valid_module: MyDataModule):\n",
    "        super().__init__()\n",
    "        self.train_module = train_module\n",
    "        self.valid_module = valid_module\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_module.train_dataloader()\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.valid_module.val_dataloader()\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, target, phase = \"train\"):\n",
    "        self.phase = phase \n",
    "        self.data = df\n",
    "        self.target = target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.phase in ['train', \"valid\"]:\n",
    "            return self.data.values[index].astype(float), self.target.values[index].astype(float)\n",
    "        elif self.phase == 'test':\n",
    "            return self.data.values[index].astype(float), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6207d5ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T01:03:12.789414Z",
     "iopub.status.busy": "2025-04-17T01:03:12.788271Z",
     "iopub.status.idle": "2025-04-17T01:04:17.418789Z",
     "shell.execute_reply": "2025-04-17T01:04:17.417434Z"
    },
    "papermill": {
     "duration": 64.635838,
     "end_time": "2025-04-17T01:04:17.420559",
     "exception": false,
     "start_time": "2025-04-17T01:03:12.784721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "2025-04-17 01:03:15.803926: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744851796.092050      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744851796.177773      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train start\n",
      "loss: 0.835 - val_loss: 0.953\n",
      "loss: 0.695 - val_loss: 0.834\n",
      "loss: 0.613 - val_loss: 0.753\n",
      "loss: 0.559 - val_loss: 0.722\n",
      "loss: 0.513 - val_loss: 0.672\n",
      "loss: 0.473 - val_loss: 0.657\n",
      "loss: 0.447 - val_loss: 0.683\n",
      "loss: 0.421 - val_loss: 0.762\n",
      "loss: 0.396 - val_loss: 0.779\n",
      "Train end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train start\n",
      "loss: 0.762 - val_loss: 0.997\n",
      "loss: 0.664 - val_loss: 0.901\n",
      "loss: 0.579 - val_loss: 0.820\n",
      "loss: 0.525 - val_loss: 0.775\n",
      "loss: 0.475 - val_loss: 0.751\n",
      "loss: 0.439 - val_loss: 0.720\n",
      "loss: 0.410 - val_loss: 0.701\n",
      "loss: 0.392 - val_loss: 0.685\n",
      "loss: 0.372 - val_loss: 0.672\n",
      "loss: 0.358 - val_loss: 0.664\n",
      "Train end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train start\n",
      "loss: 0.879 - val_loss: 0.949\n",
      "loss: 0.706 - val_loss: 0.916\n",
      "loss: 0.617 - val_loss: 0.872\n",
      "loss: 0.553 - val_loss: 0.838\n",
      "loss: 0.511 - val_loss: 0.817\n",
      "loss: 0.476 - val_loss: 0.792\n",
      "loss: 0.455 - val_loss: 0.764\n",
      "loss: 0.431 - val_loss: 0.748\n",
      "loss: 0.409 - val_loss: 0.734\n",
      "loss: 0.387 - val_loss: 0.726\n",
      "Train end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train start\n",
      "loss: 0.809 - val_loss: 1.023\n",
      "loss: 0.672 - val_loss: 0.948\n",
      "loss: 0.579 - val_loss: 0.870\n",
      "loss: 0.524 - val_loss: 0.818\n",
      "loss: 0.483 - val_loss: 0.780\n",
      "loss: 0.449 - val_loss: 0.753\n",
      "loss: 0.421 - val_loss: 0.731\n",
      "loss: 0.397 - val_loss: 0.717\n",
      "loss: 0.374 - val_loss: 0.712\n",
      "loss: 0.354 - val_loss: 0.700\n",
      "Train end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train start\n",
      "loss: 0.802 - val_loss: 0.932\n",
      "loss: 0.640 - val_loss: 0.843\n",
      "loss: 0.572 - val_loss: 0.750\n",
      "loss: 0.523 - val_loss: 0.683\n",
      "loss: 0.480 - val_loss: 0.631\n",
      "loss: 0.447 - val_loss: 0.602\n",
      "loss: 0.422 - val_loss: 0.570\n",
      "loss: 0.395 - val_loss: 0.548\n",
      "loss: 0.375 - val_loss: 0.527\n",
      "loss: 0.359 - val_loss: 0.507\n",
      "Train end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF metric:  0.3595554945276908\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42, workers=True)\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "test_module = MyDataModule(normalize_test[cons_feats], batch_size = config[\"batch_size\"])\n",
    "\n",
    "test_preds = []\n",
    "valid_preds = []\n",
    "valid_targets = []\n",
    "for fold, (tr_index, val_index) in enumerate(skf.split(normalize_train[cons_feats], train.Class)):\n",
    "    callbacks = [EarlyStopping(monitor='valid_loss', patience=3),]\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"valid_loss\",\n",
    "        dirpath=f\"./checkpoints-{fold}\",\n",
    "        filename=f\"model\", save_top_k=1, mode=\"min\",)\n",
    "    callbacks.append(checkpoint_callback)\n",
    "\n",
    "    # train\n",
    "    trainer = Trainer(max_epochs = config[\"epochs\"], callbacks=callbacks, enable_progress_bar = False, log_every_n_steps = 10)\n",
    "    model = MyModule().to(\"cpu\", dtype=float)\n",
    "    train_module = MyDataModule(normalize_train[cons_feats].loc[tr_index], train.Class.loc[tr_index], batch_size = config[\"batch_size\"])\n",
    "    valid_module = MyDataModule(normalize_train[cons_feats].loc[val_index], train.Class.loc[val_index], batch_size = config[\"batch_size\"])\n",
    "    data_module = CombinedDataModule(train_module, valid_module)\n",
    "    trainer.fit(model, datamodule = data_module)\n",
    "    \n",
    "    best_model = MyModule.load_from_checkpoint(checkpoint_callback.best_model_path).to(\"cpu\", dtype=float)\n",
    "\n",
    "    # validation\n",
    "    valid_preds.append(torch.cat(trainer.predict(best_model, valid_module.get_loader(\"valid\"))))\n",
    "    valid_targets.append(valid_module.y.values)\n",
    "\n",
    "    # test\n",
    "    test_preds.append(torch.sigmoid(torch.cat(trainer.predict(best_model, test_module.get_loader(\"test\")))))\n",
    "\n",
    "valid_targets = np.hstack(valid_targets)\n",
    "valid_preds = torch.cat(valid_preds)\n",
    "\n",
    "test_preds = torch.mean(torch.vstack(test_preds), axis=0)\n",
    "\n",
    "print(\"OOF metric: \", config[\"metric_function\"](valid_targets, torch.sigmoid(valid_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d9d32e",
   "metadata": {
    "papermill": {
     "duration": 0.005114,
     "end_time": "2025-04-17T01:04:17.431176",
     "exception": false,
     "start_time": "2025-04-17T01:04:17.426062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 5687476,
     "sourceId": 52784,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 97.299699,
   "end_time": "2025-04-17T01:04:20.776575",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-17T01:02:43.476876",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
