{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- improve linear model and nn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "import json\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    print('Reading train.csv file....')\n",
    "    train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\n",
    "\n",
    "    print('Reading test.csv file....')\n",
    "    test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n",
    "\n",
    "    print('Reading train_labels.csv file....')\n",
    "    train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\n",
    "\n",
    "    print('Reading specs.csv file....')\n",
    "    specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n",
    "\n",
    "    print('Reading sample_submission.csv file....')\n",
    "    sample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n",
    "    return train, test, train_labels, specs, sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_title(train, test, train_labels):\n",
    "    # encode title\n",
    "    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n",
    "    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n",
    "    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n",
    "    # make a list with all the unique 'titles' from the train and test set\n",
    "    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n",
    "    # make a list with all the unique 'event_code' from the train and test set\n",
    "    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n",
    "    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n",
    "    # make a list with all the unique worlds from the train and test set\n",
    "    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n",
    "    # create a dictionary numerating the titles\n",
    "    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n",
    "    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n",
    "    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n",
    "    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n",
    "    # replace the text titles with the number titles from the dict\n",
    "    train['title'] = train['title'].map(activities_map)\n",
    "    test['title'] = test['title'].map(activities_map)\n",
    "    train['world'] = train['world'].map(activities_world)\n",
    "    test['world'] = test['world'].map(activities_world)\n",
    "    train_labels['title'] = train_labels['title'].map(activities_map)\n",
    "    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n",
    "    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n",
    "    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n",
    "    # convert text into datetime\n",
    "    train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "    test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "    train[\"misses\"] = train[\"event_data\"].apply(lambda x: json.loads(x)[\"misses\"] if \"\\\"misses\\\"\" in x else np.nan)\n",
    "    test[\"misses\"] = test[\"event_data\"].apply(lambda x: json.loads(x)[\"misses\"] if \"\\\"misses\\\"\" in x else np.nan)\n",
    "    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code, activities_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(user_sample, test_set=False):\n",
    "    '''\n",
    "    The user_sample is a DataFrame from train or test where the only one \n",
    "    installation_id is filtered\n",
    "    And the test_set parameter is related with the labels processing, that is only requered\n",
    "    if test_set=False\n",
    "    '''\n",
    "    # Constants and parameters declaration\n",
    "    last_activity = 0\n",
    "    \n",
    "    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "    \n",
    "    # new features: time spent in each activity\n",
    "    last_session_time_sec = 0\n",
    "    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n",
    "    all_assessments = []\n",
    "    accumulated_accuracy_group = 0\n",
    "    accumulated_accuracy = 0\n",
    "    accumulated_correct_attempts = 0 \n",
    "    accumulated_uncorrect_attempts = 0\n",
    "    accumulated_actions = 0\n",
    "    counter = 0\n",
    "    time_first_activity = float(user_sample['timestamp'].values[0])\n",
    "    durations = []\n",
    "    durations_game = []\n",
    "    durations_activity = []\n",
    "    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n",
    "    last_game_time_title = {'lgt_' + title: 0 for title in assess_titles}\n",
    "    ac_game_time_title = {'agt_' + title: 0 for title in assess_titles}\n",
    "    ac_true_attempts_title = {'ata_' + title: 0 for title in assess_titles}\n",
    "    ac_false_attempts_title = {'afa_' + title: 0 for title in assess_titles}\n",
    "    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n",
    "    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n",
    "    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n",
    "    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n",
    "    session_count = 0\n",
    "    miss = 0\n",
    "    crys_game_true = 0; crys_game_false = 0\n",
    "    tree_game_true = 0; tree_game_false = 0\n",
    "    magma_game_true = 0; magma_game_false = 0\n",
    "    crys_game_acc = []; tree_game_acc = []; magma_game_acc = []\n",
    "    crys_act_true = 0; crys_act_false = 0\n",
    "    tree_act_true = 0; tree_act_false = 0\n",
    "    magma_act_true = 0; magma_act_false = 0\n",
    "    crys_act_acc = []; tree_act_acc = []; magma_act_acc = []\n",
    "    \n",
    "    # itarates through each session of one instalation_id\n",
    "    for i, session in user_sample.groupby('game_session', sort=False):\n",
    "        # i = game_session_id\n",
    "        # session is a DataFrame that contain only one game_session\n",
    "        \n",
    "        # get some sessions information\n",
    "        session_type = session['type'].iloc[0]\n",
    "        session_title = session['title'].iloc[0]\n",
    "        session_title_text = activities_labels[session_title]  \n",
    "        session_world = session[\"world\"].iloc[0]\n",
    "            \n",
    "        # for each assessment, and only this kind off session, the features below are processed\n",
    "        # and a register are generated      \n",
    "        \n",
    "        if (session_type == 'Assessment') & (test_set or len(session)>1):\n",
    "            # search for event_code 4100, that represents the assessments trial\n",
    "            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n",
    "            # then, check the numbers of wins and the number of losses\n",
    "            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n",
    "            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n",
    "            # copy a dict to use as feature template, it's initialized with some itens: \n",
    "            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "            features = user_activities_count.copy()\n",
    "            features.update(last_accuracy_title.copy())\n",
    "            features.update(event_code_count.copy())\n",
    "            features.update(title_count.copy())\n",
    "            features.update(event_id_count.copy())\n",
    "            features.update(title_event_code_count.copy())\n",
    "            features.update(last_game_time_title.copy())\n",
    "            features.update(ac_game_time_title.copy())\n",
    "            features.update(ac_true_attempts_title.copy())\n",
    "            features.update(ac_false_attempts_title.copy())\n",
    "            features['installation_session_count'] = session_count\n",
    "            \n",
    "            variety_features = [('var_event_code', event_code_count), \n",
    "                                ('var_event_id', event_id_count), \n",
    "                                ('var_title', title_count), \n",
    "                                ('var_title_event_code', title_event_code_count)]\n",
    "            \n",
    "            for name, dict_counts in variety_features:\n",
    "                arr = np.array(list(dict_counts.values()))\n",
    "                features[name] = np.count_nonzero(arr)\n",
    "                \n",
    "            # get installation_id for aggregated features\n",
    "            features['installation_id'] = session['installation_id'].iloc[-1]\n",
    "            # add title as feature, remembering that title represents the name of the game\n",
    "            features['session_title'] = session['title'].iloc[0]\n",
    "            # the 4 lines below add the feature of the history of the trials of this player\n",
    "            # this is based on the all time attempts so far, at the moment of this assessment\n",
    "            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n",
    "            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n",
    "            accumulated_correct_attempts += true_attempts \n",
    "            accumulated_uncorrect_attempts += false_attempts\n",
    "            \n",
    "            # ----------------------------------------------\n",
    "            ac_true_attempts_title['ata_' + session_title_text] += true_attempts\n",
    "            ac_false_attempts_title['afa_' + session_title_text] += false_attempts\n",
    "            \n",
    "            \n",
    "            last_game_time_title['lgt_' + session_title_text] = session['game_time'].iloc[-1]\n",
    "            ac_game_time_title['agt_' + session_title_text] += session['game_time'].iloc[-1]\n",
    "            # ----------------------------------------------\n",
    "            \n",
    "            # the time spent in the app so far\n",
    "            if durations == []:\n",
    "                features['duration_mean'] = 0\n",
    "                features['duration_std'] = 0\n",
    "                features['last_duration'] = 0\n",
    "                features['duration_max'] = 0\n",
    "            else:\n",
    "                features['duration_mean'] = np.mean(durations)\n",
    "                features['duration_std'] = np.std(durations)\n",
    "                features['last_duration'] = durations[-1]\n",
    "                features['duration_max'] = np.max(durations)\n",
    "            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            \n",
    "            if durations_game == []:\n",
    "                features['duration_game_mean'] = 0\n",
    "                features['duration_game_std'] = 0\n",
    "                features['game_last_duration'] = 0\n",
    "                features['game_max_duration'] = 0\n",
    "            else:\n",
    "                features['duration_game_mean'] = np.mean(durations_game)\n",
    "                features['duration_game_std'] = np.std(durations_game)\n",
    "                features['game_last_duration'] = durations_game[-1]\n",
    "                features['game_max_duration'] = np.max(durations_game)\n",
    "                \n",
    "            if durations_activity == []:\n",
    "                features['duration_activity_mean'] = 0\n",
    "                features['duration_activity_std'] = 0\n",
    "                features['game_activity_duration'] = 0\n",
    "                features['game_activity_max'] = 0\n",
    "            else:\n",
    "                features['duration_activity_mean'] = np.mean(durations_activity)\n",
    "                features['duration_activity_std'] = np.std(durations_activity)\n",
    "                features['game_activity_duration'] = durations_activity[-1]\n",
    "                features['game_activity_max'] = np.max(durations_activity)\n",
    "                \n",
    "            features[\"misses\"] = miss\n",
    "            if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                features[\"game_true\"] = crys_game_true\n",
    "                features[\"game_false\"] = crys_game_false\n",
    "                features['game_accuracy'] = crys_game_true / (crys_game_true + crys_game_false) if (crys_game_true + crys_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(crys_game_acc) if len(crys_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = crys_game_acc[-1] if len(crys_game_acc) >=1 else 0\n",
    "                features[\"act_true\"] = crys_act_true\n",
    "                features[\"act_false\"] = crys_act_false\n",
    "                features['act_accuracy'] = crys_act_true / (crys_act_true + crys_act_false) if (crys_act_true + crys_act_false) != 0 else 0\n",
    "                features[\"act_accuracy_std\"] = np.std(crys_act_acc) if len(crys_act_acc) >=1 else 0\n",
    "                features[\"last_act_acc\"] = crys_act_acc[-1] if len(crys_act_acc) >=1 else 0\n",
    "            elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                features[\"game_true\"] = tree_game_true\n",
    "                features[\"game_false\"] = tree_game_false\n",
    "                features['game_accuracy'] = tree_game_true / (tree_game_true + tree_game_false) if (tree_game_true + tree_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(tree_game_acc) if len(tree_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = tree_game_acc[-1] if len(tree_game_acc) >=1 else 0\n",
    "                features[\"act_true\"] = tree_act_true\n",
    "                features[\"act_false\"] = tree_act_false\n",
    "                features['act_accuracy'] = tree_act_true / (tree_act_true + tree_act_false) if (tree_act_true + tree_act_false) != 0 else 0\n",
    "                features[\"act_accuracy_std\"] = np.std(tree_act_acc) if len(tree_act_acc) >=1 else 0\n",
    "                features[\"last_act_acc\"] = tree_act_acc[-1] if len(tree_act_acc) >=1 else 0\n",
    "            elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                features[\"game_true\"] = magma_game_true\n",
    "                features[\"game_false\"] = magma_game_false\n",
    "                features['game_accuracy'] = magma_game_true / (magma_game_true + magma_game_false) if (magma_game_true + magma_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(magma_game_acc) if len(magma_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = magma_game_acc[-1] if len(magma_game_acc) >=1 else 0\n",
    "                features[\"act_true\"] = magma_act_true\n",
    "                features[\"act_false\"] = magma_act_false\n",
    "                features['act_accuracy'] = magma_act_true / (magma_act_true + magma_act_false) if (magma_act_true + magma_act_false) != 0 else 0\n",
    "                features[\"act_accuracy_std\"] = np.std(magma_act_acc) if len(magma_act_acc) >=1 else 0\n",
    "                features[\"last_act_acc\"] = magma_act_acc[-1] if len(magma_act_acc) >=1 else 0\n",
    "            \n",
    "            # the accuracy is the all time wins divided by the all time attempts\n",
    "            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n",
    "            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n",
    "            accumulated_accuracy += accuracy\n",
    "            last_accuracy_title['acc_' + session_title_text] = accuracy\n",
    "            # a feature of the current accuracy categorized\n",
    "            # it is a counter of how many times this player was in each accuracy group\n",
    "            if accuracy == 0:\n",
    "                features['accuracy_group'] = 0\n",
    "            elif accuracy == 1:\n",
    "                features['accuracy_group'] = 3\n",
    "            elif accuracy == 0.5:\n",
    "                features['accuracy_group'] = 2\n",
    "            else:\n",
    "                features['accuracy_group'] = 1\n",
    "            features.update(accuracy_groups)\n",
    "            accuracy_groups[features['accuracy_group']] += 1\n",
    "            # mean of the all accuracy groups of this player\n",
    "            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n",
    "            accumulated_accuracy_group += features['accuracy_group']\n",
    "            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n",
    "            features['accumulated_actions'] = accumulated_actions\n",
    "            \n",
    "            # there are some conditions to allow this features to be inserted in the datasets\n",
    "            # if it's a test set, all sessions belong to the final dataset\n",
    "            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n",
    "            # that means, must exist an event_code 4100 or 4110\n",
    "            if test_set:\n",
    "                all_assessments.append(features)\n",
    "            elif true_attempts+false_attempts > 0:\n",
    "                all_assessments.append(features)\n",
    "                \n",
    "            counter += 1\n",
    "            \n",
    "        if session_type == 'Game':\n",
    "            durations_game.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            true = session['event_data'].str.contains('true').sum()\n",
    "            false = session['event_data'].str.contains('false').sum() \n",
    "            durations_game.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                crys_game_true += true\n",
    "                crys_game_false += false\n",
    "                crys_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "            elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                tree_game_true += true\n",
    "                tree_game_false += false\n",
    "                tree_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "            elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                magma_game_true += true\n",
    "                magma_game_false += false\n",
    "                magma_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        if session_type == 'Activity':\n",
    "            durations_activity.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            true = session['event_data'].str.contains('true').sum()\n",
    "            false = session['event_data'].str.contains('false').sum() \n",
    "            durations_activity.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                crys_act_true += true\n",
    "                crys_act_false += false\n",
    "                crys_act_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "            elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                tree_act_true += true\n",
    "                tree_act_false += false\n",
    "                tree_act_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "            elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                magma_act_true += true\n",
    "                magma_act_false += false\n",
    "                magma_act_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "            else:\n",
    "                pass    \n",
    "        \n",
    "        session_count += 1\n",
    "        # this piece counts how many actions was made in each event_code so far\n",
    "        def update_counters(counter: dict, col: str):\n",
    "                num_of_session_count = Counter(session[col])\n",
    "                for k in num_of_session_count.keys():\n",
    "                    x = k\n",
    "                    if col == 'title':\n",
    "                        x = activities_labels[k]\n",
    "                    counter[x] += num_of_session_count[k]\n",
    "                return counter\n",
    "            \n",
    "        event_code_count = update_counters(event_code_count, \"event_code\")\n",
    "        event_id_count = update_counters(event_id_count, \"event_id\")\n",
    "        title_count = update_counters(title_count, 'title')\n",
    "        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n",
    "\n",
    "        # counts how many actions the player has done so far, used in the feature of the same name\n",
    "        accumulated_actions += len(session)\n",
    "        if last_activity != session_type:\n",
    "            user_activities_count[session_type] += 1\n",
    "            last_activitiy = session_type \n",
    "                        \n",
    "    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n",
    "    if test_set:\n",
    "        return all_assessments[-1]\n",
    "    # in the train_set, all assessments goes to the dataset\n",
    "    return all_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test(train, test):\n",
    "    compiled_train = []\n",
    "    compiled_test = []\n",
    "    compiled_test_his = []\n",
    "    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total = 17000):\n",
    "        compiled_train += get_data(user_sample)\n",
    "    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n",
    "        test_data = get_data(user_sample, test_set = True)\n",
    "        compiled_test.append(test_data)\n",
    "    for i, (ins_id, user_sample) in tqdm(enumerate(test.groupby('installation_id', sort = False)), total = 1000):\n",
    "        compiled_test_his += get_data(user_sample)\n",
    "    reduce_train = pd.DataFrame(compiled_train)\n",
    "    reduce_test = pd.DataFrame(compiled_test)\n",
    "    reduce_test_his = pd.DataFrame(compiled_test_his)\n",
    "    \n",
    "    return reduce_train, reduce_test, reduce_test_his"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thank to Bruno\n",
    "def eval_qwk_lgb_regr(y_pred, train_t):\n",
    "    \"\"\"\n",
    "    Fast cappa eval function for lgb.\n",
    "    \"\"\"\n",
    "    dist = Counter(train_t['accuracy_group'])\n",
    "    for k in dist:\n",
    "        dist[k] /= len(train_t)\n",
    "    \n",
    "    acum = 0\n",
    "    bound = {}\n",
    "    for i in range(3):\n",
    "        acum += dist[i]\n",
    "        bound[i] = np.percentile(y_pred, acum * 100)\n",
    "\n",
    "    def classify(x):\n",
    "        if x <= bound[0]:\n",
    "            return 0\n",
    "        elif x <= bound[1]:\n",
    "            return 1\n",
    "        elif x <= bound[2]:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    y_pred = np.array(list(map(classify, y_pred)))\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def predict(sample_submission, y_pred):\n",
    "    sample_submission['accuracy_group'] = y_pred\n",
    "    sample_submission['accuracy_group'] = sample_submission['accuracy_group'].astype(int)\n",
    "    sample_submission.to_csv('submission.csv', index = False)\n",
    "    print(sample_submission['accuracy_group'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_assessment(reduce_train):\n",
    "    used_idx = []\n",
    "    for iid in set(reduce_train['installation_id']):\n",
    "        list_ = list(reduce_train[reduce_train['installation_id']==iid].index)\n",
    "        cur = random.choices(list_, k = 1)[0]\n",
    "        used_idx.append(cur)\n",
    "    reduce_train_t = reduce_train.loc[used_idx]\n",
    "    print(\"used validation data: \", len(used_idx))\n",
    "    return reduce_train_t, used_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each validation fold extract one random observation for each installation id to simulate the test set\n",
    "def run_lgb(reduce_train, reduce_test, features):\n",
    "    # features found in initial bayesian optimization\n",
    "    params = {'boosting_type': 'gbdt', \n",
    "              'metric': 'rmse', \n",
    "              'objective': 'regression', \n",
    "              'eval_metric': 'cappa', \n",
    "              'n_jobs': -1, \n",
    "              'seed': 42, \n",
    "              'num_leaves': 26, \n",
    "              'learning_rate': 0.077439684887749, \n",
    "              'max_depth': 33, \n",
    "              'lambda_l1': 3.27791989030057, \n",
    "              'lambda_l2': 1.3047627805931334, \n",
    "              'bagging_fraction': 0.896924978584253, \n",
    "              'bagging_freq': 1, \n",
    "              'colsample_bytree': 0.8710772167017853}\n",
    "\n",
    "    kf = GroupKFold(n_splits = 5)\n",
    "    target = 'accuracy_group'\n",
    "    oof_pred = np.zeros(len(reduce_train))\n",
    "    y_pred = np.zeros(len(reduce_test))\n",
    "    ind = []\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train, groups = reduce_train['installation_id'])):\n",
    "        print('Fold:', fold + 1)\n",
    "        x_train, x_val = reduce_train[features].iloc[tr_ind], reduce_train[features].iloc[val_ind]\n",
    "        y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n",
    "        x_train.drop('installation_id', inplace = True, axis = 1)\n",
    "        train_set = lgb.Dataset(x_train, y_train, categorical_feature = ['session_title'])\n",
    "\n",
    "\n",
    "        x_val, idx_val = get_random_assessment(x_val)\n",
    "        ind.extend(idx_val)\n",
    "        x_val.drop('installation_id', inplace = True, axis = 1)\n",
    "        y_val = y_val.loc[idx_val]\n",
    "        val_set = lgb.Dataset(x_val, y_val, categorical_feature = ['session_title'])\n",
    "\n",
    "        model = lgb.train(params, train_set, num_boost_round = 100000, early_stopping_rounds = 100, \n",
    "                         valid_sets = [train_set, val_set], verbose_eval = 100)\n",
    "\n",
    "        oof_pred[idx_val] = model.predict(x_val)\n",
    "        y_pred += model.predict(reduce_test[[x for x in features if x not in ['installation_id']]]) / kf.n_splits\n",
    "    oof_rmse_score = np.sqrt(mean_squared_error(reduce_train[target][ind], oof_pred[ind]))\n",
    "    oof_cohen_score = cohen_kappa_score(reduce_train[target][ind], eval_qwk_lgb_regr(oof_pred[ind], reduce_train), weights = 'quadratic')\n",
    "    print('Our oof rmse score is:', oof_rmse_score)\n",
    "    print('Our oof cohen kappa score is:', oof_cohen_score)\n",
    "    return y_pred, oof_rmse_score, oof_cohen_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each validation fold extract one random observation for each installation id to simulate the test set\n",
    "def run_lr(reduce_train, reduce_test, features):\n",
    "    kf = GroupKFold(n_splits = 5)\n",
    "    target = 'accuracy_group'\n",
    "    oof_pred = np.zeros(len(reduce_train))\n",
    "    y_pred = np.zeros(len(reduce_test))\n",
    "    ind = []\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train, groups = reduce_train['installation_id'])):\n",
    "        print('Fold:', fold + 1)\n",
    "        x_train, x_val = reduce_train[features].iloc[tr_ind], reduce_train[features].iloc[val_ind]\n",
    "        y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n",
    "        x_train.drop('installation_id', inplace = True, axis = 1)\n",
    "\n",
    "        x_val, idx_val = get_random_assessment(x_val)\n",
    "        ind.extend(idx_val)\n",
    "        x_val.drop('installation_id', inplace = True, axis = 1)\n",
    "        y_val = y_val.loc[idx_val]\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(x_train, y_train)  \n",
    "\n",
    "        oof_pred[idx_val] = model.predict(x_val)\n",
    "        y_pred += model.predict(reduce_test[[x for x in features if x not in ['installation_id']]]) / kf.n_splits\n",
    "    oof_rmse_score = np.sqrt(mean_squared_error(reduce_train[target][ind], oof_pred[ind]))\n",
    "    oof_cohen_score = cohen_kappa_score(reduce_train[target][ind], eval_qwk_lgb_regr(oof_pred[ind], reduce_train), weights = 'quadratic')\n",
    "    print('Our oof rmse score is:', oof_rmse_score)\n",
    "    print('Our oof cohen kappa score is:', oof_cohen_score)\n",
    "    return y_pred, oof_rmse_score, oof_cohen_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def run_nn(reduce_train, reduce_test, features):\n",
    "    kf = GroupKFold(n_splits = 5)\n",
    "    target = 'accuracy_group'\n",
    "    oof_pred = np.zeros(len(reduce_train))\n",
    "    y_pred = np.zeros(len(reduce_test))\n",
    "    ind = []\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train, groups = reduce_train['installation_id'])):\n",
    "        print('Fold:', fold + 1)\n",
    "        x_train, x_val = reduce_train[features].iloc[tr_ind], reduce_train[features].iloc[val_ind]\n",
    "        y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n",
    "        x_train.drop('installation_id', inplace = True, axis = 1)\n",
    "\n",
    "        x_val, idx_val = get_random_assessment(x_val)\n",
    "        ind.extend(idx_val)\n",
    "        x_val.drop('installation_id', inplace = True, axis = 1)\n",
    "        y_val = y_val.loc[idx_val]\n",
    "        \n",
    "        verbosity = 100\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(x_train.shape[1],)),\n",
    "            tf.keras.layers.Dense(200, activation='relu'), #, kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(100, activation='tanh'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(25, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1, activation='relu')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mse')\n",
    "        #print(model.summary())\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('./nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "        \n",
    "        model.fit(x_train, \n",
    "                y_train, \n",
    "                validation_data=(x_val, y_val),\n",
    "                epochs=100,\n",
    "                 callbacks=[save_best, early_stop])\n",
    "        model.load_weights('./nn_model.w8')\n",
    "        \n",
    "        oof_pred[idx_val] = model.predict(x_val).reshape(x_val.shape[0],)\n",
    "        y_pred += model.predict(reduce_test[[x for x in features if x not in ['installation_id']]]).reshape(reduce_test.shape[0],) / kf.n_splits\n",
    "    oof_rmse_score = np.sqrt(mean_squared_error(reduce_train[target][ind], oof_pred[ind]))\n",
    "    oof_cohen_score = cohen_kappa_score(reduce_train[target][ind], eval_qwk_lgb_regr(oof_pred[ind], reduce_train), weights = 'quadratic')\n",
    "    print('Our oof rmse score is:', oof_rmse_score)\n",
    "    print('Our oof cohen kappa score is:', oof_cohen_score)\n",
    "    return y_pred, oof_rmse_score, oof_cohen_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(reduce_train, reduce_test):\n",
    "    features = [i for i in reduce_train.columns if i not in [\"installation_id\", \"accuracy_group\"]]\n",
    "    categoricals = ['session_title']\n",
    "    features = features.copy()\n",
    "    new_train = reduce_train.copy()\n",
    "    new_test = reduce_test.copy()\n",
    "    if len(categoricals) > 0:\n",
    "        for cat in categoricals:\n",
    "            enc = OneHotEncoder()\n",
    "            train_cats = enc.fit_transform(new_train[[cat]])\n",
    "            test_cats = enc.transform(new_test[[cat]])\n",
    "            cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.active_features_]\n",
    "            features += cat_cols\n",
    "            train_cats = pd.DataFrame(train_cats.toarray(), columns=cat_cols)\n",
    "            test_cats = pd.DataFrame(test_cats.toarray(), columns=cat_cols)\n",
    "            new_train = pd.concat([new_train, train_cats], axis=1)\n",
    "            new_test = pd.concat([new_test, test_cats], axis=1)\n",
    "        scalar = MinMaxScaler()\n",
    "        new_train[features] = scalar.fit_transform(new_train[features])\n",
    "        new_test[features] = scalar.transform(new_test[features])\n",
    "    new_train = new_train.drop([\"session_title\"], axis=1)\n",
    "    new_test = new_test.drop([\"session_title\"], axis=1)\n",
    "    return new_train, new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_correlated_features(reduce_train):\n",
    "    counter = 0\n",
    "    to_remove = []\n",
    "    for feat_a in features:\n",
    "        for feat_b in features:\n",
    "            if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n",
    "                c = np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1]\n",
    "                if c > 0.995:\n",
    "                    counter += 1\n",
    "                    to_remove.append(feat_b)\n",
    "                    print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))\n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to exclude columns from the train and test set if the mean is different, also adjust test column by a factor to simulate the same distribution\n",
    "def exclude(reduce_train, reduce_test, features):\n",
    "    to_exclude = [] \n",
    "    ajusted_test = reduce_test.copy()\n",
    "    for feature in features:\n",
    "        if feature not in ['accuracy_group', 'installation_id', 'session_title']:\n",
    "            data = reduce_train[feature]\n",
    "            train_mean = data.mean()\n",
    "            data = ajusted_test[feature] \n",
    "            test_mean = data.mean()\n",
    "            try:\n",
    "                ajust_factor = train_mean / test_mean\n",
    "                if ajust_factor > 10 or ajust_factor < 0.1:# or error > 0.01:\n",
    "                    to_exclude.append(feature)\n",
    "                    print(feature)\n",
    "                else:\n",
    "                    ajusted_test[feature] *= ajust_factor\n",
    "            except:\n",
    "                to_exclude.append(feature)\n",
    "                print(feature)\n",
    "    return to_exclude, ajusted_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train.csv file....\n",
      "Reading test.csv file....\n",
      "Reading train_labels.csv file....\n",
      "Reading specs.csv file....\n",
      "Reading sample_submission.csv file....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504994fe40dc44a38a3e3d63e5acbde1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=17000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9289056b2e14466081aa09972b6544e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0410a24260cc4109952f20d4bb62237f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train, test, train_labels, specs, sample_submission = read_data()\n",
    "train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code, activities_world = encode_title(train, test, train_labels)\n",
    "reduce_train, reduce_test, reduce_his_test = get_train_and_test(train, test)\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted from feature elimination first round script\n",
    "old_features = list(reduce_train.columns[0:95]) + list(reduce_train.columns[882:])\n",
    "el_features = ['accuracy_group', 'accuracy', 'installation_id']\n",
    "old_features = [col for col in old_features if col not in el_features]\n",
    "event_id_features = list_of_event_id #list(reduce_train.columns[95:479])\n",
    "title_event_code_cross = all_title_event_code #list(reduce_train.columns[479:882])\n",
    "features = old_features + event_id_features + title_event_code_cross\n",
    "lr_features = features.copy()\n",
    "nn_features = features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: FEAT_A: Clip FEAT_B: 27253bdc - Correlation: 0.9999999999999999\n",
      "2: FEAT_A: 2050 FEAT_B: 2040 - Correlation: 0.9965259434878118\n",
      "3: FEAT_A: 2050 FEAT_B: 37c53127 - Correlation: 1.0\n",
      "4: FEAT_A: 2050 FEAT_B: 2b9272f4 - Correlation: 0.9999839030068793\n",
      "5: FEAT_A: 2050 FEAT_B: 08fd73f3 - Correlation: 0.9966123918733654\n",
      "6: FEAT_A: 2050 FEAT_B: 73757a5e - Correlation: 0.9998050146713992\n",
      "7: FEAT_A: 2050 FEAT_B: dcaede90 - Correlation: 0.9965259434878118\n",
      "8: FEAT_A: 2050 FEAT_B: 26fd2d99 - Correlation: 0.9965084543995759\n",
      "9: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_2040 - Correlation: 0.9965259434878118\n",
      "10: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_2050 - Correlation: 1.0\n",
      "11: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_3021 - Correlation: 0.9998050146713992\n",
      "12: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_2020 - Correlation: 0.9965084543995759\n",
      "13: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_2030 - Correlation: 0.9966123918733654\n",
      "14: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_3121 - Correlation: 0.9999839030068793\n",
      "15: FEAT_A: 4230 FEAT_B: 4235 - Correlation: 0.9999995197498746\n",
      "16: FEAT_A: 4230 FEAT_B: ad148f58 - Correlation: 0.9999999999999998\n",
      "17: FEAT_A: 4230 FEAT_B: 85de926c - Correlation: 0.9999995197498746\n",
      "18: FEAT_A: 4230 FEAT_B: Bubble Bath_4235 - Correlation: 0.9999995197498746\n",
      "19: FEAT_A: 4230 FEAT_B: Bubble Bath_4230 - Correlation: 0.9999999999999998\n",
      "20: FEAT_A: 5000 FEAT_B: 5010 - Correlation: 0.9991849213605333\n",
      "21: FEAT_A: 5000 FEAT_B: 71e712d8 - Correlation: 0.9991849213605333\n",
      "22: FEAT_A: 5000 FEAT_B: a6d66e51 - Correlation: 1.0\n",
      "23: FEAT_A: 5000 FEAT_B: Watering Hole (Activity)_5000 - Correlation: 1.0\n",
      "24: FEAT_A: 5000 FEAT_B: Watering Hole (Activity)_5010 - Correlation: 0.9991849213605333\n",
      "25: FEAT_A: 3110 FEAT_B: 3010 - Correlation: 0.9999293402893735\n",
      "26: FEAT_A: 3120 FEAT_B: 3020 - Correlation: 0.9998761417908972\n",
      "27: FEAT_A: 3121 FEAT_B: 3021 - Correlation: 0.9999098200487934\n",
      "28: FEAT_A: 4031 FEAT_B: 1996c610 - Correlation: 1.0\n",
      "29: FEAT_A: 4031 FEAT_B: Dino Drink_4031 - Correlation: 1.0\n",
      "30: FEAT_A: 2000 FEAT_B: installation_session_count - Correlation: 1.0\n",
      "31: FEAT_A: 4050 FEAT_B: a1192f43 - Correlation: 0.9999999999999999\n",
      "32: FEAT_A: 4050 FEAT_B: Crystals Rule_4050 - Correlation: 0.9999999999999999\n",
      "33: FEAT_A: 2020 FEAT_B: 2030 - Correlation: 0.9959933262816534\n",
      "34: FEAT_A: 4220 FEAT_B: 1340b8d7 - Correlation: 1.0\n",
      "35: FEAT_A: 4220 FEAT_B: Bubble Bath_4220 - Correlation: 1.0\n",
      "36: FEAT_A: Slop Problem FEAT_B: Slop Problem_2000 - Correlation: 1.0\n",
      "37: FEAT_A: Pirate's Tale FEAT_B: Pirate's Tale_2000 - Correlation: 0.9999999999999999\n",
      "38: FEAT_A: Treasure Map FEAT_B: Treasure Map_2000 - Correlation: 1.0\n",
      "39: FEAT_A: Honey Cake FEAT_B: Honey Cake_2000 - Correlation: 1.0\n",
      "40: FEAT_A: Tree Top City - Level 3 FEAT_B: Tree Top City - Level 3_2000 - Correlation: 1.0\n",
      "41: FEAT_A: Rulers FEAT_B: Rulers_2000 - Correlation: 1.0\n",
      "42: FEAT_A: Welcome to Lost Lagoon! FEAT_B: Welcome to Lost Lagoon!_2000 - Correlation: 1.0\n",
      "43: FEAT_A: Ordering Spheres FEAT_B: Ordering Spheres_2000 - Correlation: 1.0\n",
      "44: FEAT_A: Magma Peak - Level 1 FEAT_B: Magma Peak - Level 1_2000 - Correlation: 1.0\n",
      "45: FEAT_A: Crystal Caves - Level 3 FEAT_B: Crystal Caves - Level 3_2000 - Correlation: 1.0\n",
      "46: FEAT_A: Balancing Act FEAT_B: Balancing Act_2000 - Correlation: 1.0\n",
      "47: FEAT_A: Bottle Filler (Activity) FEAT_B: bb3e370b - Correlation: 0.9950043311420306\n",
      "48: FEAT_A: Bottle Filler (Activity) FEAT_B: Bottle Filler (Activity)_4030 - Correlation: 0.9950043311420306\n",
      "49: FEAT_A: Lifting Heavy Things FEAT_B: Lifting Heavy Things_2000 - Correlation: 0.9999999999999999\n",
      "50: FEAT_A: Magma Peak - Level 2 FEAT_B: Magma Peak - Level 2_2000 - Correlation: 1.0\n",
      "51: FEAT_A: Crystal Caves - Level 2 FEAT_B: Crystal Caves - Level 2_2000 - Correlation: 1.0\n",
      "52: FEAT_A: Tree Top City - Level 1 FEAT_B: Tree Top City - Level 1_2000 - Correlation: 0.9999999999999999\n",
      "53: FEAT_A: 12 Monkeys FEAT_B: 12 Monkeys_2000 - Correlation: 1.0\n",
      "54: FEAT_A: Crystal Caves - Level 1 FEAT_B: Crystal Caves - Level 1_2000 - Correlation: 1.0\n",
      "55: FEAT_A: Costume Box FEAT_B: Costume Box_2000 - Correlation: 1.0\n",
      "56: FEAT_A: Tree Top City - Level 2 FEAT_B: Tree Top City - Level 2_2000 - Correlation: 1.0\n",
      "57: FEAT_A: Heavy, Heavier, Heaviest FEAT_B: Heavy, Heavier, Heaviest_2000 - Correlation: 1.0\n",
      "58: FEAT_A: lgt_Mushroom Sorter (Assessment) FEAT_B: agt_Mushroom Sorter (Assessment) - Correlation: 0.9954655260051172\n",
      "59: FEAT_A: ata_Mushroom Sorter (Assessment) FEAT_B: 6c930e6e - Correlation: 0.9985426312249887\n",
      "60: FEAT_A: ata_Mushroom Sorter (Assessment) FEAT_B: Mushroom Sorter (Assessment)_2030 - Correlation: 0.9985426312249887\n",
      "61: FEAT_A: ata_Chest Sorter (Assessment) FEAT_B: e4f1efe6 - Correlation: 0.9981999245282789\n",
      "62: FEAT_A: ata_Chest Sorter (Assessment) FEAT_B: 3afb49e6 - Correlation: 0.9996140232445475\n",
      "63: FEAT_A: ata_Chest Sorter (Assessment) FEAT_B: Chest Sorter (Assessment)_3121 - Correlation: 0.9981999245282789\n",
      "64: FEAT_A: ata_Chest Sorter (Assessment) FEAT_B: Chest Sorter (Assessment)_3021 - Correlation: 0.9996140232445475\n",
      "65: FEAT_A: ata_Cart Balancer (Assessment) FEAT_B: a8876db3 - Correlation: 0.9999170128095376\n",
      "66: FEAT_A: ata_Cart Balancer (Assessment) FEAT_B: Cart Balancer (Assessment)_3021 - Correlation: 0.9999170128095376\n",
      "67: FEAT_A: ata_Bird Measurer (Assessment) FEAT_B: 7525289a - Correlation: 0.9981555049446889\n",
      "68: FEAT_A: ata_Bird Measurer (Assessment) FEAT_B: 45d01abe - Correlation: 1.0\n",
      "69: FEAT_A: ata_Bird Measurer (Assessment) FEAT_B: Bird Measurer (Assessment)_3021 - Correlation: 1.0\n",
      "70: FEAT_A: ata_Bird Measurer (Assessment) FEAT_B: Bird Measurer (Assessment)_3121 - Correlation: 0.9981555049446889\n",
      "71: FEAT_A: afa_Mushroom Sorter (Assessment) FEAT_B: 160654fd - Correlation: 0.9999996751514757\n",
      "72: FEAT_A: afa_Mushroom Sorter (Assessment) FEAT_B: 88d4a5be - Correlation: 0.9989293679278667\n",
      "73: FEAT_A: afa_Mushroom Sorter (Assessment) FEAT_B: Mushroom Sorter (Assessment)_3120 - Correlation: 0.9989293679278667\n",
      "74: FEAT_A: afa_Mushroom Sorter (Assessment) FEAT_B: Mushroom Sorter (Assessment)_3020 - Correlation: 0.9999996751514757\n",
      "75: FEAT_A: afa_Chest Sorter (Assessment) FEAT_B: ea296733 - Correlation: 0.9999880332296727\n",
      "76: FEAT_A: afa_Chest Sorter (Assessment) FEAT_B: df4fe8b6 - Correlation: 0.9972434688333254\n",
      "77: FEAT_A: afa_Chest Sorter (Assessment) FEAT_B: Chest Sorter (Assessment)_3020 - Correlation: 0.9999880332296727\n",
      "78: FEAT_A: afa_Chest Sorter (Assessment) FEAT_B: Chest Sorter (Assessment)_3120 - Correlation: 0.9972434688333254\n",
      "79: FEAT_A: afa_Bird Measurer (Assessment) FEAT_B: e37a2b78 - Correlation: 0.9983241920747599\n",
      "80: FEAT_A: afa_Bird Measurer (Assessment) FEAT_B: ad2fc29c - Correlation: 0.9991605930121138\n",
      "81: FEAT_A: afa_Bird Measurer (Assessment) FEAT_B: 17113b36 - Correlation: 0.998391883995427\n",
      "82: FEAT_A: afa_Bird Measurer (Assessment) FEAT_B: Bird Measurer (Assessment)_3120 - Correlation: 0.9983241920747599\n",
      "83: FEAT_A: afa_Bird Measurer (Assessment) FEAT_B: Bird Measurer (Assessment)_3020 - Correlation: 0.9991605930121138\n",
      "84: FEAT_A: afa_Bird Measurer (Assessment) FEAT_B: Bird Measurer (Assessment)_4110 - Correlation: 0.998391883995427\n",
      "85: FEAT_A: var_event_id FEAT_B: var_title_event_code - Correlation: 0.9995254184300616\n",
      "86: FEAT_A: e694a35b FEAT_B: Fireworks (Activity)_4020 - Correlation: 1.0\n",
      "87: FEAT_A: 49ed92e9 FEAT_B: bd701df8 - Correlation: 0.9993109138888533\n",
      "88: FEAT_A: 49ed92e9 FEAT_B: Watering Hole (Activity)_3110 - Correlation: 0.9993109138888533\n",
      "89: FEAT_A: 49ed92e9 FEAT_B: Watering Hole (Activity)_3010 - Correlation: 1.0\n",
      "90: FEAT_A: a0faea5d FEAT_B: Bubble Bath_4070 - Correlation: 1.0\n",
      "91: FEAT_A: 4901243f FEAT_B: Fireworks (Activity)_2000 - Correlation: 1.0\n",
      "92: FEAT_A: 53c6e11a FEAT_B: Leaf Leader_2075 - Correlation: 0.9999999999999999\n",
      "93: FEAT_A: 5290eab1 FEAT_B: 04df9b66 - Correlation: 0.9998190477466209\n",
      "94: FEAT_A: 5290eab1 FEAT_B: Cauldron Filler (Assessment)_3120 - Correlation: 1.0\n",
      "95: FEAT_A: 5290eab1 FEAT_B: Cauldron Filler (Assessment)_3020 - Correlation: 0.9998190477466209\n",
      "96: FEAT_A: 3d8c61b0 FEAT_B: Happy Camel_4030 - Correlation: 0.9999999999999999\n",
      "97: FEAT_A: 05ad839b FEAT_B: Happy Camel_4090 - Correlation: 1.0\n",
      "98: FEAT_A: 6aeafed4 FEAT_B: Bubble Bath_4090 - Correlation: 1.0\n",
      "99: FEAT_A: 532a2afb FEAT_B: Cauldron Filler (Assessment)_2020 - Correlation: 1.0\n",
      "100: FEAT_A: 33505eae FEAT_B: 2a512369 - Correlation: 0.9994585292841953\n",
      "101: FEAT_A: 33505eae FEAT_B: Leaf Leader_3010 - Correlation: 0.9999999999999998\n",
      "102: FEAT_A: 33505eae FEAT_B: Leaf Leader_3110 - Correlation: 0.9994585292841953\n",
      "103: FEAT_A: 2c4e6db0 FEAT_B: All Star Sorting_2020 - Correlation: 1.0\n",
      "104: FEAT_A: b2e5b0f1 FEAT_B: ecaab346 - Correlation: 0.999849464604504\n",
      "105: FEAT_A: b2e5b0f1 FEAT_B: b74258a0 - Correlation: 0.999849464604504\n",
      "106: FEAT_A: b2e5b0f1 FEAT_B: Cart Balancer (Assessment)_3121 - Correlation: 0.999849464604504\n",
      "107: FEAT_A: b2e5b0f1 FEAT_B: Cart Balancer (Assessment)_2030 - Correlation: 0.999849464604504\n",
      "108: FEAT_A: b2e5b0f1 FEAT_B: Cart Balancer (Assessment)_2010 - Correlation: 1.0\n",
      "109: FEAT_A: 6043a2b4 FEAT_B: All Star Sorting_4090 - Correlation: 0.9999999999999999\n",
      "110: FEAT_A: 832735e1 FEAT_B: ab3136ba - Correlation: 0.9998637945770242\n",
      "111: FEAT_A: 832735e1 FEAT_B: Dino Dive_3010 - Correlation: 1.0\n",
      "112: FEAT_A: 832735e1 FEAT_B: Dino Dive_3110 - Correlation: 0.9998637945770242\n",
      "113: FEAT_A: ab4ec3a4 FEAT_B: Dino Drink_4080 - Correlation: 1.0\n",
      "114: FEAT_A: 262136f4 FEAT_B: Leaf Leader_4020 - Correlation: 1.0\n",
      "115: FEAT_A: 84538528 FEAT_B: Sandcastle Builder (Activity)_4020 - Correlation: 1.0\n",
      "116: FEAT_A: 4a09ace1 FEAT_B: Scrub-A-Dub_2083 - Correlation: 1.0\n",
      "117: FEAT_A: d122731b FEAT_B: Cart Balancer (Assessment)_4100 - Correlation: 0.9999999999999998\n",
      "118: FEAT_A: 69fdac0a FEAT_B: 8d7e386c - Correlation: 0.9996590210382708\n",
      "119: FEAT_A: 69fdac0a FEAT_B: Happy Camel_3110 - Correlation: 0.9999999999999999\n",
      "120: FEAT_A: 69fdac0a FEAT_B: Happy Camel_3010 - Correlation: 0.9996590210382708\n",
      "121: FEAT_A: 7372e1a5 FEAT_B: Chow Time_4070 - Correlation: 1.0\n",
      "122: FEAT_A: d88e8f25 FEAT_B: ac92046e - Correlation: 0.9999763070332106\n",
      "123: FEAT_A: d88e8f25 FEAT_B: Scrub-A-Dub_3020 - Correlation: 1.0\n",
      "124: FEAT_A: d88e8f25 FEAT_B: Scrub-A-Dub_3120 - Correlation: 0.9999763070332106\n",
      "125: FEAT_A: 3ddc79c3 FEAT_B: e720d930 - Correlation: 0.9998920962508026\n",
      "126: FEAT_A: 3ddc79c3 FEAT_B: 3323d7e9 - Correlation: 0.9998606654761331\n",
      "127: FEAT_A: 3ddc79c3 FEAT_B: 7cf1bc53 - Correlation: 0.99823323877902\n",
      "128: FEAT_A: 3ddc79c3 FEAT_B: Crystals Rule_3121 - Correlation: 0.9998920962508026\n",
      "129: FEAT_A: 3ddc79c3 FEAT_B: Crystals Rule_2020 - Correlation: 0.99823323877902\n",
      "130: FEAT_A: 3ddc79c3 FEAT_B: Crystals Rule_3021 - Correlation: 0.9999999999999998\n",
      "131: FEAT_A: 3ddc79c3 FEAT_B: Crystals Rule_2030 - Correlation: 0.9998606654761331\n",
      "132: FEAT_A: 709b1251 FEAT_B: e3ff61fb - Correlation: 0.9995444786291265\n",
      "133: FEAT_A: 709b1251 FEAT_B: 7961e599 - Correlation: 0.9950004881933643\n",
      "134: FEAT_A: 709b1251 FEAT_B: Dino Dive_2020 - Correlation: 0.9950004881933643\n",
      "135: FEAT_A: 709b1251 FEAT_B: Dino Dive_3121 - Correlation: 1.0\n",
      "136: FEAT_A: 709b1251 FEAT_B: Dino Dive_3021 - Correlation: 0.9995444786291265\n",
      "137: FEAT_A: 6f445b57 FEAT_B: Chow Time_4080 - Correlation: 1.0\n",
      "138: FEAT_A: 08ff79ad FEAT_B: Egg Dropper (Activity)_4090 - Correlation: 1.0\n",
      "139: FEAT_A: a7640a16 FEAT_B: Happy Camel_4070 - Correlation: 1.0\n",
      "140: FEAT_A: 56bcd38d FEAT_B: Chicken Balancer (Activity)_4030 - Correlation: 0.9999999999999998\n",
      "141: FEAT_A: c2baf0bd FEAT_B: Happy Camel_2020 - Correlation: 1.0\n",
      "142: FEAT_A: f56e0afc FEAT_B: Bird Measurer (Assessment)_2000 - Correlation: 1.0\n",
      "143: FEAT_A: 6cf7d25c FEAT_B: 15f99afc - Correlation: 0.9994848234397947\n",
      "144: FEAT_A: 6cf7d25c FEAT_B: Pan Balance_3010 - Correlation: 1.0\n",
      "145: FEAT_A: 6cf7d25c FEAT_B: Pan Balance_3110 - Correlation: 0.9994848234397947\n",
      "146: FEAT_A: 9ee1c98c FEAT_B: Sandcastle Builder (Activity)_4021 - Correlation: 0.9999999999999999\n",
      "147: FEAT_A: 3afde5dd FEAT_B: b012cd7f - Correlation: 0.9999689260314981\n",
      "148: FEAT_A: 3afde5dd FEAT_B: e5c9df6f - Correlation: 0.9990910601838011\n",
      "149: FEAT_A: 3afde5dd FEAT_B: Leaf Leader_3021 - Correlation: 1.0\n",
      "150: FEAT_A: 3afde5dd FEAT_B: Leaf Leader_2030 - Correlation: 0.9999689260314981\n",
      "151: FEAT_A: 3afde5dd FEAT_B: Leaf Leader_3121 - Correlation: 0.9990910601838011\n",
      "152: FEAT_A: e080a381 FEAT_B: Pan Balance_4090 - Correlation: 1.0\n",
      "153: FEAT_A: 5859dfb6 FEAT_B: 90ea0bac - Correlation: 0.998105247261057\n",
      "154: FEAT_A: 5859dfb6 FEAT_B: Bubble Bath_3120 - Correlation: 0.9999999999999998\n",
      "155: FEAT_A: 5859dfb6 FEAT_B: Bubble Bath_3020 - Correlation: 0.998105247261057\n",
      "156: FEAT_A: 4e5fc6f5 FEAT_B: Cart Balancer (Assessment)_4090 - Correlation: 1.0\n",
      "157: FEAT_A: 1cf54632 FEAT_B: Bubble Bath_2000 - Correlation: 0.9999999999999999\n",
      "158: FEAT_A: 46b50ba8 FEAT_B: Happy Camel_4095 - Correlation: 0.9999999999999998\n",
      "159: FEAT_A: 5de79a6a FEAT_B: 31973d56 - Correlation: 0.9973945339840646\n",
      "160: FEAT_A: 5de79a6a FEAT_B: Cart Balancer (Assessment)_3020 - Correlation: 1.0\n",
      "161: FEAT_A: 5de79a6a FEAT_B: Cart Balancer (Assessment)_3120 - Correlation: 0.9973945339840646\n",
      "162: FEAT_A: 6f8106d9 FEAT_B: Dino Drink_4090 - Correlation: 1.0\n",
      "163: FEAT_A: 731c0cbe FEAT_B: Bird Measurer (Assessment)_4090 - Correlation: 1.0\n",
      "164: FEAT_A: 1f19558b FEAT_B: daac11b0 - Correlation: 0.9991356096406655\n",
      "165: FEAT_A: 1f19558b FEAT_B: ca11f653 - Correlation: 0.998316427341082\n",
      "166: FEAT_A: 1f19558b FEAT_B: All Star Sorting_3121 - Correlation: 1.0\n",
      "167: FEAT_A: 1f19558b FEAT_B: All Star Sorting_3021 - Correlation: 0.9991356096406655\n",
      "168: FEAT_A: 1f19558b FEAT_B: All Star Sorting_2030 - Correlation: 0.998316427341082\n",
      "169: FEAT_A: 37db1c2f FEAT_B: Happy Camel_4045 - Correlation: 1.0\n",
      "170: FEAT_A: f6947f54 FEAT_B: Bird Measurer (Assessment)_2030 - Correlation: 1.0\n",
      "171: FEAT_A: a1e4395d FEAT_B: a52b92d5 - Correlation: 0.9991003891313368\n",
      "172: FEAT_A: a1e4395d FEAT_B: Mushroom Sorter (Assessment)_3110 - Correlation: 0.9991003891313368\n",
      "173: FEAT_A: a1e4395d FEAT_B: Mushroom Sorter (Assessment)_3010 - Correlation: 1.0\n",
      "174: FEAT_A: 90efca10 FEAT_B: Bottle Filler (Activity)_4020 - Correlation: 1.0\n",
      "175: FEAT_A: a2df0760 FEAT_B: Happy Camel_4035 - Correlation: 0.9999999999999999\n",
      "176: FEAT_A: e79f3763 FEAT_B: Bug Measurer (Activity)_4030 - Correlation: 1.0\n",
      "177: FEAT_A: 8f094001 FEAT_B: 3bb91dda - Correlation: 0.9960166932956102\n",
      "178: FEAT_A: 8f094001 FEAT_B: c54cf6c5 - Correlation: 0.9958447429109037\n",
      "179: FEAT_A: 8f094001 FEAT_B: 1beb320a - Correlation: 0.9980774800878145\n",
      "180: FEAT_A: 8f094001 FEAT_B: Bubble Bath_2020 - Correlation: 0.9980774800878145\n",
      "181: FEAT_A: 8f094001 FEAT_B: Bubble Bath_2025 - Correlation: 0.9958447429109037\n",
      "182: FEAT_A: 8f094001 FEAT_B: Bubble Bath_4020 - Correlation: 0.9960166932956102\n",
      "183: FEAT_A: 8f094001 FEAT_B: Bubble Bath_4045 - Correlation: 1.0\n",
      "184: FEAT_A: a44b10dc FEAT_B: Flower Waterer (Activity)_4070 - Correlation: 1.0\n",
      "185: FEAT_A: 29f54413 FEAT_B: Leaf Leader_2060 - Correlation: 0.9999999999999998\n",
      "186: FEAT_A: 86ba578b FEAT_B: Leaf Leader_2070 - Correlation: 1.0\n",
      "187: FEAT_A: c7f7f0e1 FEAT_B: Bug Measurer (Activity)_2000 - Correlation: 1.0\n",
      "188: FEAT_A: 3b2048ee FEAT_B: Leaf Leader_4095 - Correlation: 1.0\n",
      "189: FEAT_A: 3babcb9b FEAT_B: 5154fc30 - Correlation: 0.999986924640935\n",
      "190: FEAT_A: 3babcb9b FEAT_B: 86c924c4 - Correlation: 0.9988970478897697\n",
      "191: FEAT_A: 3babcb9b FEAT_B: Crystals Rule_3110 - Correlation: 1.0\n",
      "192: FEAT_A: 3babcb9b FEAT_B: Crystals Rule_4020 - Correlation: 0.9988970478897697\n",
      "193: FEAT_A: 3babcb9b FEAT_B: Crystals Rule_3010 - Correlation: 0.999986924640935\n",
      "194: FEAT_A: e5734469 FEAT_B: 89aace00 - Correlation: 0.9998406115110344\n",
      "195: FEAT_A: e5734469 FEAT_B: Dino Drink_3120 - Correlation: 0.9998406115110344\n",
      "196: FEAT_A: e5734469 FEAT_B: Dino Drink_3020 - Correlation: 1.0\n",
      "197: FEAT_A: 5d042115 FEAT_B: Flower Waterer (Activity)_4030 - Correlation: 1.0\n",
      "198: FEAT_A: 5b49460a FEAT_B: 155f62a4 - Correlation: 0.9999999999999999\n",
      "199: FEAT_A: 5b49460a FEAT_B: Chest Sorter (Assessment)_2000 - Correlation: 0.9999999999999999\n",
      "200: FEAT_A: 5b49460a FEAT_B: Chest Sorter (Assessment)_2020 - Correlation: 0.9999999999999999\n",
      "201: FEAT_A: acf5c23f FEAT_B: Cart Balancer (Assessment)_4070 - Correlation: 1.0\n",
      "202: FEAT_A: f806dc10 FEAT_B: Dino Drink_2020 - Correlation: 1.0\n",
      "203: FEAT_A: a592d54e FEAT_B: 1c178d24 - Correlation: 0.9973125883100831\n",
      "204: FEAT_A: a592d54e FEAT_B: 250513af - Correlation: 0.9973020736885158\n",
      "205: FEAT_A: a592d54e FEAT_B: cf7638f3 - Correlation: 0.9969813738295747\n",
      "206: FEAT_A: a592d54e FEAT_B: Pan Balance_2030 - Correlation: 0.9973125883100831\n",
      "207: FEAT_A: a592d54e FEAT_B: Pan Balance_3121 - Correlation: 0.9969813738295747\n",
      "208: FEAT_A: a592d54e FEAT_B: Pan Balance_3021 - Correlation: 0.9973020736885158\n",
      "209: FEAT_A: a592d54e FEAT_B: Pan Balance_2020 - Correlation: 0.9999999999999999\n",
      "210: FEAT_A: f5b8c21a FEAT_B: 58a0de5c - Correlation: 0.9977888184537717\n",
      "211: FEAT_A: f5b8c21a FEAT_B: 9b4001e4 - Correlation: 0.9976837802056778\n",
      "212: FEAT_A: f5b8c21a FEAT_B: Air Show_3121 - Correlation: 0.9977888184537717\n",
      "213: FEAT_A: f5b8c21a FEAT_B: Air Show_3021 - Correlation: 0.9976837802056778\n",
      "214: FEAT_A: f5b8c21a FEAT_B: Air Show_2030 - Correlation: 1.0\n",
      "215: FEAT_A: 0086365d FEAT_B: Pan Balance_4010 - Correlation: 0.9999999999999999\n",
      "216: FEAT_A: 9d4e7b25 FEAT_B: Cart Balancer (Assessment)_4040 - Correlation: 1.0\n",
      "217: FEAT_A: f3cd5473 FEAT_B: Pan Balance_4070 - Correlation: 1.0\n",
      "218: FEAT_A: b7530680 FEAT_B: e9c52111 - Correlation: 0.998817689964623\n",
      "219: FEAT_A: b7530680 FEAT_B: Bottle Filler (Activity)_2030 - Correlation: 0.998817689964623\n",
      "220: FEAT_A: b7530680 FEAT_B: Bottle Filler (Activity)_2020 - Correlation: 1.0\n",
      "221: FEAT_A: b1d5101d FEAT_B: All Star Sorting_4095 - Correlation: 1.0\n",
      "222: FEAT_A: 7da34a02 FEAT_B: Mushroom Sorter (Assessment)_4070 - Correlation: 0.9999999999999998\n",
      "223: FEAT_A: 28520915 FEAT_B: d3268efa - Correlation: 0.9989055023166136\n",
      "224: FEAT_A: 28520915 FEAT_B: b5053438 - Correlation: 0.999090516610188\n",
      "225: FEAT_A: 28520915 FEAT_B: Cauldron Filler (Assessment)_3021 - Correlation: 0.9989055023166136\n",
      "226: FEAT_A: 28520915 FEAT_B: Cauldron Filler (Assessment)_3121 - Correlation: 0.999090516610188\n",
      "227: FEAT_A: 28520915 FEAT_B: Cauldron Filler (Assessment)_2030 - Correlation: 1.0\n",
      "228: FEAT_A: bcceccc6 FEAT_B: Air Show_4070 - Correlation: 1.0\n",
      "229: FEAT_A: 7ab78247 FEAT_B: b80e5e84 - Correlation: 0.9998336590281087\n",
      "230: FEAT_A: 7ab78247 FEAT_B: Egg Dropper (Activity)_3110 - Correlation: 0.9998336590281087\n",
      "231: FEAT_A: 7ab78247 FEAT_B: Egg Dropper (Activity)_3010 - Correlation: 1.0\n",
      "232: FEAT_A: b2dba42b FEAT_B: 1bb5fbdb - Correlation: 0.9999521729413294\n",
      "233: FEAT_A: b2dba42b FEAT_B: Sandcastle Builder (Activity)_3010 - Correlation: 1.0\n",
      "234: FEAT_A: b2dba42b FEAT_B: Sandcastle Builder (Activity)_3110 - Correlation: 0.9999521729413294\n",
      "235: FEAT_A: de26c3a6 FEAT_B: Flower Waterer (Activity)_4020 - Correlation: 0.9999999999999999\n",
      "236: FEAT_A: 5e3ea25a FEAT_B: Crystals Rule_4070 - Correlation: 1.0\n",
      "237: FEAT_A: 37937459 FEAT_B: Sandcastle Builder (Activity)_4090 - Correlation: 0.9999999999999998\n",
      "238: FEAT_A: a1bbe385 FEAT_B: f28c589a - Correlation: 0.999953679734225\n",
      "239: FEAT_A: a1bbe385 FEAT_B: Air Show_3110 - Correlation: 1.0\n",
      "240: FEAT_A: a1bbe385 FEAT_B: Air Show_3010 - Correlation: 0.999953679734225\n",
      "241: FEAT_A: 8ac7cce4 FEAT_B: Leaf Leader_2000 - Correlation: 1.0\n",
      "242: FEAT_A: 3dfd4aa4 FEAT_B: c74f40cd - Correlation: 0.9963968989064971\n",
      "243: FEAT_A: 3dfd4aa4 FEAT_B: 83c6c409 - Correlation: 0.9999970893851743\n",
      "244: FEAT_A: 3dfd4aa4 FEAT_B: 28ed704e - Correlation: 0.9996618525211791\n",
      "245: FEAT_A: 3dfd4aa4 FEAT_B: 9d29771f - Correlation: 0.9963727918475914\n",
      "246: FEAT_A: 3dfd4aa4 FEAT_B: Mushroom Sorter (Assessment)_4025 - Correlation: 0.9996618525211791\n",
      "247: FEAT_A: 3dfd4aa4 FEAT_B: Mushroom Sorter (Assessment)_2035 - Correlation: 0.9999970893851743\n",
      "248: FEAT_A: 3dfd4aa4 FEAT_B: Mushroom Sorter (Assessment)_3121 - Correlation: 0.9963968989064971\n",
      "249: FEAT_A: 3dfd4aa4 FEAT_B: Mushroom Sorter (Assessment)_3021 - Correlation: 0.9963727918475914\n",
      "250: FEAT_A: 3dfd4aa4 FEAT_B: Mushroom Sorter (Assessment)_2020 - Correlation: 1.0\n",
      "251: FEAT_A: 3d63345e FEAT_B: Cart Balancer (Assessment)_4035 - Correlation: 1.0\n",
      "252: FEAT_A: 0413e89d FEAT_B: 15eb4a7d - Correlation: 0.9997266832893074\n",
      "253: FEAT_A: 0413e89d FEAT_B: Bubble Bath_3010 - Correlation: 1.0\n",
      "254: FEAT_A: 0413e89d FEAT_B: Bubble Bath_3110 - Correlation: 0.9997266832893074\n",
      "255: FEAT_A: 5c3d2b2f FEAT_B: Scrub-A-Dub_4020 - Correlation: 1.0\n",
      "256: FEAT_A: 5a848010 FEAT_B: Scrub-A-Dub_2080 - Correlation: 1.0\n",
      "257: FEAT_A: f54238ee FEAT_B: Fireworks (Activity)_4090 - Correlation: 1.0\n",
      "258: FEAT_A: 598f4598 FEAT_B: Flower Waterer (Activity)_4025 - Correlation: 0.9999999999999998\n",
      "259: FEAT_A: 9de5e594 FEAT_B: 28a4eb9a - Correlation: 0.9995923561196808\n",
      "260: FEAT_A: 9de5e594 FEAT_B: Dino Dive_3120 - Correlation: 0.9995923561196808\n",
      "261: FEAT_A: 9de5e594 FEAT_B: Dino Dive_3020 - Correlation: 1.0\n",
      "262: FEAT_A: 756e5507 FEAT_B: Chicken Balancer (Activity)_2000 - Correlation: 1.0\n",
      "263: FEAT_A: 48349b14 FEAT_B: Crystals Rule_2000 - Correlation: 1.0\n",
      "264: FEAT_A: 499edb7c FEAT_B: Chicken Balancer (Activity)_4020 - Correlation: 1.0\n",
      "265: FEAT_A: 7f0836bf FEAT_B: a29c5338 - Correlation: 0.9986531654717627\n",
      "266: FEAT_A: 7f0836bf FEAT_B: Dino Drink_3110 - Correlation: 1.0\n",
      "267: FEAT_A: 7f0836bf FEAT_B: Dino Drink_3010 - Correlation: 0.9986531654717627\n",
      "268: FEAT_A: 3bfd1a65 FEAT_B: db02c830 - Correlation: 0.9999982205265872\n",
      "269: FEAT_A: 3bfd1a65 FEAT_B: Mushroom Sorter (Assessment)_2025 - Correlation: 0.9999982205265872\n",
      "270: FEAT_A: 3bfd1a65 FEAT_B: Mushroom Sorter (Assessment)_2000 - Correlation: 1.0\n",
      "271: FEAT_A: f7e47413 FEAT_B: f71c4741 - Correlation: 0.9999426890770878\n",
      "272: FEAT_A: f7e47413 FEAT_B: Scrub-A-Dub_3010 - Correlation: 0.9999426890770878\n",
      "273: FEAT_A: f7e47413 FEAT_B: Scrub-A-Dub_3110 - Correlation: 1.0\n",
      "274: FEAT_A: 804ee27f FEAT_B: Pan Balance_4020 - Correlation: 1.0\n",
      "275: FEAT_A: d2278a3b FEAT_B: Bottle Filler (Activity)_2000 - Correlation: 1.0\n",
      "276: FEAT_A: ea321fb1 FEAT_B: 84b0e0c8 - Correlation: 0.9993007600205108\n",
      "277: FEAT_A: ea321fb1 FEAT_B: Chicken Balancer (Activity)_3010 - Correlation: 1.0\n",
      "278: FEAT_A: ea321fb1 FEAT_B: Chicken Balancer (Activity)_3110 - Correlation: 0.9993007600205108\n",
      "279: FEAT_A: 0d18d96c FEAT_B: Mushroom Sorter (Assessment)_4035 - Correlation: 1.0\n",
      "280: FEAT_A: 6077cc36 FEAT_B: Bird Measurer (Assessment)_4080 - Correlation: 0.9999999999999998\n",
      "281: FEAT_A: cf82af56 FEAT_B: Scrub-A-Dub_4070 - Correlation: 1.0\n",
      "282: FEAT_A: 736f9581 FEAT_B: 9b23e8ee - Correlation: 0.9999999999999999\n",
      "283: FEAT_A: 736f9581 FEAT_B: Egg Dropper (Activity)_2020 - Correlation: 0.9999999999999999\n",
      "284: FEAT_A: 736f9581 FEAT_B: Egg Dropper (Activity)_2000 - Correlation: 0.9999999999999999\n",
      "285: FEAT_A: 392e14df FEAT_B: Cauldron Filler (Assessment)_4100 - Correlation: 1.0\n",
      "286: FEAT_A: 9b01374f FEAT_B: Flower Waterer (Activity)_2000 - Correlation: 1.0\n",
      "287: FEAT_A: 51102b85 FEAT_B: Bird Measurer (Assessment)_4030 - Correlation: 1.0\n",
      "288: FEAT_A: cb6010f8 FEAT_B: 56817e2b - Correlation: 0.999047573942978\n",
      "289: FEAT_A: cb6010f8 FEAT_B: 47026d5f - Correlation: 0.9996901555713447\n",
      "290: FEAT_A: cb6010f8 FEAT_B: Chow Time_3121 - Correlation: 0.9999999999999999\n",
      "291: FEAT_A: cb6010f8 FEAT_B: Chow Time_2030 - Correlation: 0.999047573942978\n",
      "292: FEAT_A: cb6010f8 FEAT_B: Chow Time_3021 - Correlation: 0.9996901555713447\n",
      "293: FEAT_A: a16a373e FEAT_B: Bird Measurer (Assessment)_4070 - Correlation: 1.0\n",
      "294: FEAT_A: f32856e4 FEAT_B: Leaf Leader_2020 - Correlation: 1.0\n",
      "295: FEAT_A: bdf49a58 FEAT_B: 1375ccb7 - Correlation: 0.9993801763820347\n",
      "296: FEAT_A: bdf49a58 FEAT_B: Bird Measurer (Assessment)_3010 - Correlation: 0.9993801763820347\n",
      "297: FEAT_A: bdf49a58 FEAT_B: Bird Measurer (Assessment)_3110 - Correlation: 1.0\n",
      "298: FEAT_A: d3f1e122 FEAT_B: Bottle Filler (Activity)_4035 - Correlation: 1.0\n",
      "299: FEAT_A: 2b058fe3 FEAT_B: Cauldron Filler (Assessment)_2010 - Correlation: 1.0\n",
      "300: FEAT_A: 7423acbc FEAT_B: e04fb33d - Correlation: 0.999628997408126\n",
      "301: FEAT_A: 7423acbc FEAT_B: Air Show_3120 - Correlation: 0.999628997408126\n",
      "302: FEAT_A: 7423acbc FEAT_B: Air Show_3020 - Correlation: 1.0\n",
      "303: FEAT_A: 4ef8cdd3 FEAT_B: Chow Time_4020 - Correlation: 1.0\n",
      "304: FEAT_A: 6c517a88 FEAT_B: Dino Drink_4070 - Correlation: 1.0\n",
      "305: FEAT_A: 6bf9e3e1 FEAT_B: Happy Camel_4040 - Correlation: 0.9999999999999999\n",
      "306: FEAT_A: bbfe0445 FEAT_B: 56cd3b43 - Correlation: 0.9996926215355526\n",
      "307: FEAT_A: bbfe0445 FEAT_B: Flower Waterer (Activity)_3110 - Correlation: 1.0\n",
      "308: FEAT_A: bbfe0445 FEAT_B: Flower Waterer (Activity)_3010 - Correlation: 0.9996926215355526\n",
      "309: FEAT_A: 9c5ef70c FEAT_B: Pan Balance_2000 - Correlation: 1.0\n",
      "310: FEAT_A: 5c2f29ca FEAT_B: Cart Balancer (Assessment)_4020 - Correlation: 1.0\n",
      "311: FEAT_A: d51b1749 FEAT_B: Happy Camel_2080 - Correlation: 1.0\n",
      "312: FEAT_A: 119b5b02 FEAT_B: Dino Dive_4080 - Correlation: 1.0\n",
      "313: FEAT_A: d2e9262e FEAT_B: 2fb91ec1 - Correlation: 0.9991434495208743\n",
      "314: FEAT_A: d2e9262e FEAT_B: Watering Hole (Activity)_4020 - Correlation: 0.9999999999999998\n",
      "315: FEAT_A: d2e9262e FEAT_B: Watering Hole (Activity)_4025 - Correlation: 0.9991434495208743\n",
      "316: FEAT_A: 363d3849 FEAT_B: 9e4c8c7b - Correlation: 0.9992130941883633\n",
      "317: FEAT_A: 363d3849 FEAT_B: All Star Sorting_3110 - Correlation: 0.9992130941883633\n",
      "318: FEAT_A: 363d3849 FEAT_B: All Star Sorting_3010 - Correlation: 1.0\n",
      "319: FEAT_A: 8b757ab8 FEAT_B: 44cb4907 - Correlation: 0.999835058794711\n",
      "320: FEAT_A: 8b757ab8 FEAT_B: Crystals Rule_3120 - Correlation: 1.0\n",
      "321: FEAT_A: 8b757ab8 FEAT_B: Crystals Rule_3020 - Correlation: 0.999835058794711\n",
      "322: FEAT_A: c7128948 FEAT_B: Mushroom Sorter (Assessment)_4040 - Correlation: 1.0\n",
      "323: FEAT_A: 77ead60d FEAT_B: 4d911100 - Correlation: 0.9998475927724766\n",
      "324: FEAT_A: 77ead60d FEAT_B: 16dffff1 - Correlation: 0.9984674845132689\n",
      "325: FEAT_A: 77ead60d FEAT_B: Dino Drink_3121 - Correlation: 0.9998475927724766\n",
      "326: FEAT_A: 77ead60d FEAT_B: Dino Drink_2030 - Correlation: 0.9984674845132689\n",
      "327: FEAT_A: 77ead60d FEAT_B: Dino Drink_3021 - Correlation: 0.9999999999999999\n",
      "328: FEAT_A: 16667cc5 FEAT_B: Chicken Balancer (Activity)_4080 - Correlation: 0.9999999999999999\n",
      "329: FEAT_A: 7040c096 FEAT_B: Scrub-A-Dub_4010 - Correlation: 1.0\n",
      "330: FEAT_A: 8fee50e2 FEAT_B: Bird Measurer (Assessment)_4020 - Correlation: 1.0\n",
      "331: FEAT_A: ecc6157f FEAT_B: Cart Balancer (Assessment)_4080 - Correlation: 0.9999999999999999\n",
      "332: FEAT_A: 26a5a3dd FEAT_B: All Star Sorting_4080 - Correlation: 1.0\n",
      "333: FEAT_A: 8d84fa81 FEAT_B: Bubble Bath_4010 - Correlation: 1.0\n",
      "334: FEAT_A: 47f43a44 FEAT_B: Flower Waterer (Activity)_4090 - Correlation: 1.0\n",
      "335: FEAT_A: 29a42aea FEAT_B: Bubble Bath_4080 - Correlation: 1.0\n",
      "336: FEAT_A: cdd22e43 FEAT_B: Chicken Balancer (Activity)_4035 - Correlation: 1.0\n",
      "337: FEAT_A: b88f38da FEAT_B: beb0a7b9 - Correlation: 0.9999125179829755\n",
      "338: FEAT_A: b88f38da FEAT_B: Fireworks (Activity)_3010 - Correlation: 0.9999125179829755\n",
      "339: FEAT_A: b88f38da FEAT_B: Fireworks (Activity)_3110 - Correlation: 1.0\n",
      "340: FEAT_A: 93b353f2 FEAT_B: Chest Sorter (Assessment)_4100 - Correlation: 0.9999999999999998\n",
      "341: FEAT_A: 1cc7cfca FEAT_B: All Star Sorting_4030 - Correlation: 1.0\n",
      "342: FEAT_A: e7e44842 FEAT_B: Watering Hole (Activity)_4090 - Correlation: 0.9999999999999999\n",
      "343: FEAT_A: 562cec5f FEAT_B: Chest Sorter (Assessment)_4025 - Correlation: 1.0\n",
      "344: FEAT_A: 3edf6747 FEAT_B: Cauldron Filler (Assessment)_4035 - Correlation: 0.9999999999999999\n",
      "345: FEAT_A: 792530f8 FEAT_B: Dino Drink_4030 - Correlation: 1.0\n",
      "346: FEAT_A: 070a5291 FEAT_B: Bird Measurer (Assessment)_4100 - Correlation: 1.0\n",
      "347: FEAT_A: dcb55a27 FEAT_B: Air Show_4110 - Correlation: 1.0\n",
      "348: FEAT_A: 67aa2ada FEAT_B: Leaf Leader_4090 - Correlation: 1.0\n",
      "349: FEAT_A: 15a43e5b FEAT_B: Bottle Filler (Activity)_4070 - Correlation: 1.0\n",
      "350: FEAT_A: 38074c54 FEAT_B: 222660ff - Correlation: 0.9999999999999998\n",
      "351: FEAT_A: 38074c54 FEAT_B: Chest Sorter (Assessment)_2010 - Correlation: 0.9999999999999998\n",
      "352: FEAT_A: 38074c54 FEAT_B: Chest Sorter (Assessment)_2030 - Correlation: 0.9999999999999998\n",
      "353: FEAT_A: 65a38bf7 FEAT_B: 7ad3efc6 - Correlation: 0.9999786265690429\n",
      "354: FEAT_A: 65a38bf7 FEAT_B: Cart Balancer (Assessment)_2000 - Correlation: 0.9999786265690429\n",
      "355: FEAT_A: 65a38bf7 FEAT_B: Cart Balancer (Assessment)_2020 - Correlation: 1.0\n",
      "356: FEAT_A: 0ce40006 FEAT_B: Happy Camel_4080 - Correlation: 1.0\n",
      "357: FEAT_A: 923afab1 FEAT_B: 2dcad279 - Correlation: 0.9998567985670082\n",
      "358: FEAT_A: 923afab1 FEAT_B: Cauldron Filler (Assessment)_3110 - Correlation: 0.9998567985670082\n",
      "359: FEAT_A: 923afab1 FEAT_B: Cauldron Filler (Assessment)_3010 - Correlation: 1.0\n",
      "360: FEAT_A: 9e34ea74 FEAT_B: Egg Dropper (Activity)_4070 - Correlation: 1.0\n",
      "361: FEAT_A: d185d3ea FEAT_B: Chow Time_4035 - Correlation: 1.0\n",
      "362: FEAT_A: 25fa8af4 FEAT_B: Mushroom Sorter (Assessment)_4100 - Correlation: 1.0\n",
      "363: FEAT_A: a5be6304 FEAT_B: Mushroom Sorter (Assessment)_2010 - Correlation: 1.0\n",
      "364: FEAT_A: 828e68f9 FEAT_B: Cart Balancer (Assessment)_3110 - Correlation: 1.0\n",
      "365: FEAT_A: 7ec0c298 FEAT_B: Chow Time_3010 - Correlation: 0.9999999999999999\n",
      "366: FEAT_A: c952eb01 FEAT_B: Watering Hole (Activity)_4070 - Correlation: 1.0\n",
      "367: FEAT_A: a8a78786 FEAT_B: c7fe2a55 - Correlation: 0.9981452039350778\n",
      "368: FEAT_A: a8a78786 FEAT_B: 36fa3ebe - Correlation: 0.9979559120623818\n",
      "369: FEAT_A: a8a78786 FEAT_B: Happy Camel_3121 - Correlation: 0.9999999999999999\n",
      "370: FEAT_A: a8a78786 FEAT_B: Happy Camel_2030 - Correlation: 0.9979559120623818\n",
      "371: FEAT_A: a8a78786 FEAT_B: Happy Camel_3021 - Correlation: 0.9981452039350778\n",
      "372: FEAT_A: 0d1da71f FEAT_B: Chow Time_3110 - Correlation: 1.0\n",
      "373: FEAT_A: cfbd47c8 FEAT_B: Chow Time_4030 - Correlation: 1.0\n",
      "374: FEAT_A: ec138c1c FEAT_B: Bird Measurer (Assessment)_2020 - Correlation: 0.9999999999999999\n",
      "375: FEAT_A: 2230fab4 FEAT_B: 0330ab6a - Correlation: 0.9998673365188064\n",
      "376: FEAT_A: 2230fab4 FEAT_B: Chow Time_3120 - Correlation: 0.9999999999999999\n",
      "377: FEAT_A: 2230fab4 FEAT_B: Chow Time_3020 - Correlation: 0.9998673365188064\n",
      "378: FEAT_A: c51d8688 FEAT_B: 907a054b - Correlation: 0.9999667370361688\n",
      "379: FEAT_A: c51d8688 FEAT_B: Pan Balance_3120 - Correlation: 1.0\n",
      "380: FEAT_A: c51d8688 FEAT_B: Pan Balance_3020 - Correlation: 0.9999667370361688\n",
      "381: FEAT_A: d3640339 FEAT_B: Dino Dive_4090 - Correlation: 1.0\n",
      "382: FEAT_A: c1cac9a2 FEAT_B: Scrub-A-Dub_2081 - Correlation: 0.9999999999999998\n",
      "383: FEAT_A: 87d743c1 FEAT_B: Dino Dive_4010 - Correlation: 1.0\n",
      "384: FEAT_A: 022b4259 FEAT_B: Bug Measurer (Activity)_4025 - Correlation: 0.9999999999999998\n",
      "385: FEAT_A: 4a4c3d21 FEAT_B: Bird Measurer (Assessment)_4025 - Correlation: 1.0\n",
      "386: FEAT_A: fd20ea40 FEAT_B: Leaf Leader_4010 - Correlation: 0.9999999999999998\n",
      "387: FEAT_A: 795e4a37 FEAT_B: Cart Balancer (Assessment)_3010 - Correlation: 1.0\n",
      "388: FEAT_A: bc8f2793 FEAT_B: Pan Balance_4035 - Correlation: 0.9999999999999999\n",
      "389: FEAT_A: 2a444e03 FEAT_B: Pan Balance_4030 - Correlation: 1.0\n",
      "390: FEAT_A: 3ccd3f02 FEAT_B: 3dcdda7f - Correlation: 0.9977337946782758\n",
      "391: FEAT_A: 3ccd3f02 FEAT_B: Chest Sorter (Assessment)_3010 - Correlation: 0.9977337946782758\n",
      "392: FEAT_A: 3ccd3f02 FEAT_B: Chest Sorter (Assessment)_3110 - Correlation: 1.0\n",
      "393: FEAT_A: 6d90d394 FEAT_B: Scrub-A-Dub_2000 - Correlation: 0.9999999999999999\n",
      "394: FEAT_A: 29bdd9ba FEAT_B: Dino Dive_2000 - Correlation: 1.0\n",
      "395: FEAT_A: 5e812b27 FEAT_B: Sandcastle Builder (Activity)_4030 - Correlation: 1.0\n",
      "396: FEAT_A: 7d093bf9 FEAT_B: Chow Time_2000 - Correlation: 0.9999999999999999\n",
      "397: FEAT_A: fcfdffb6 FEAT_B: Flower Waterer (Activity)_4022 - Correlation: 1.0\n",
      "398: FEAT_A: 02a42007 FEAT_B: Fireworks (Activity)_4030 - Correlation: 1.0\n",
      "399: FEAT_A: cc5087a3 FEAT_B: Crystals Rule_4010 - Correlation: 1.0\n",
      "400: FEAT_A: 857f21c0 FEAT_B: Bubble Bath_4040 - Correlation: 1.0\n",
      "401: FEAT_A: 5f5b2617 FEAT_B: Bottle Filler (Activity)_4080 - Correlation: 1.0\n",
      "402: FEAT_A: d88ca108 FEAT_B: Air Show_2070 - Correlation: 1.0\n",
      "403: FEAT_A: e7561dd2 FEAT_B: Pan Balance_4025 - Correlation: 0.9999999999999998\n",
      "404: FEAT_A: 0db6d71d FEAT_B: Chest Sorter (Assessment)_4020 - Correlation: 1.0\n",
      "405: FEAT_A: 46cd75b4 FEAT_B: Chicken Balancer (Activity)_4022 - Correlation: 0.9999999999999998\n",
      "406: FEAT_A: 55115cbd FEAT_B: 6f4adc4b - Correlation: 0.9997831892615398\n",
      "407: FEAT_A: 55115cbd FEAT_B: Bubble Bath_3021 - Correlation: 0.9997831892615398\n",
      "408: FEAT_A: 55115cbd FEAT_B: Bubble Bath_3121 - Correlation: 0.9999999999999999\n",
      "409: FEAT_A: 5be391b5 FEAT_B: Dino Drink_4010 - Correlation: 1.0\n",
      "410: FEAT_A: 8af75982 FEAT_B: Happy Camel_4020 - Correlation: 1.0\n",
      "411: FEAT_A: 1575e76c FEAT_B: Air Show_2020 - Correlation: 1.0\n",
      "412: FEAT_A: 6088b756 FEAT_B: Dino Dive_2070 - Correlation: 1.0\n",
      "413: FEAT_A: 763fc34e FEAT_B: e57dd7af - Correlation: 0.9972721980394412\n",
      "414: FEAT_A: 763fc34e FEAT_B: Leaf Leader_3120 - Correlation: 0.9972721980394412\n",
      "415: FEAT_A: 763fc34e FEAT_B: Leaf Leader_3020 - Correlation: 1.0\n",
      "416: FEAT_A: 19967db1 FEAT_B: Chow Time_4090 - Correlation: 1.0\n",
      "417: FEAT_A: d38c2fd7 FEAT_B: Bird Measurer (Assessment)_4035 - Correlation: 0.9999999999999999\n",
      "418: FEAT_A: 9ed8f6da FEAT_B: Dino Drink_2075 - Correlation: 0.9999999999999999\n",
      "419: FEAT_A: 99abe2bb FEAT_B: Bubble Bath_2080 - Correlation: 1.0\n",
      "420: FEAT_A: 74e5f8a7 FEAT_B: Dino Drink_4020 - Correlation: 1.0\n",
      "421: FEAT_A: 63f13dd7 FEAT_B: Chow Time_2020 - Correlation: 1.0\n",
      "422: FEAT_A: 28f975ea FEAT_B: Air Show_4020 - Correlation: 1.0\n",
      "423: FEAT_A: 9554a50b FEAT_B: Cauldron Filler (Assessment)_4080 - Correlation: 0.9999999999999999\n",
      "424: FEAT_A: 5f0eb72c FEAT_B: Mushroom Sorter (Assessment)_4020 - Correlation: 0.9999999999999998\n",
      "425: FEAT_A: 5e109ec3 FEAT_B: Cart Balancer (Assessment)_4030 - Correlation: 1.0\n",
      "426: FEAT_A: 77261ab5 FEAT_B: Sandcastle Builder (Activity)_2000 - Correlation: 0.9999999999999999\n",
      "427: FEAT_A: 611485c5 FEAT_B: Fireworks (Activity)_4080 - Correlation: 1.0\n",
      "428: FEAT_A: c58186bf FEAT_B: Sandcastle Builder (Activity)_4035 - Correlation: 1.0\n",
      "429: FEAT_A: 76babcde FEAT_B: Dino Dive_4070 - Correlation: 1.0\n",
      "430: FEAT_A: ecc36b7f FEAT_B: Bubble Bath_4095 - Correlation: 1.0\n",
      "431: FEAT_A: eb2c19cd FEAT_B: Mushroom Sorter (Assessment)_4090 - Correlation: 1.0\n",
      "432: FEAT_A: e64e2cfd FEAT_B: Watering Hole (Activity)_2000 - Correlation: 0.9999999999999998\n",
      "433: FEAT_A: e4d32835 FEAT_B: Pan Balance_4080 - Correlation: 0.9999999999999998\n",
      "434: FEAT_A: c277e121 FEAT_B: b120f2ac - Correlation: 0.9999983835744551\n",
      "435: FEAT_A: c277e121 FEAT_B: d45ed6a1 - Correlation: 0.9979692879543319\n",
      "436: FEAT_A: c277e121 FEAT_B: All Star Sorting_2025 - Correlation: 0.9999983835744551\n",
      "437: FEAT_A: c277e121 FEAT_B: All Star Sorting_3020 - Correlation: 1.0\n",
      "438: FEAT_A: c277e121 FEAT_B: All Star Sorting_3120 - Correlation: 0.9979692879543319\n",
      "439: FEAT_A: d06f75b5 FEAT_B: 895865f3 - Correlation: 0.9991496470458585\n",
      "440: FEAT_A: d06f75b5 FEAT_B: Bubble Bath_2035 - Correlation: 1.0\n",
      "441: FEAT_A: d06f75b5 FEAT_B: Bubble Bath_2030 - Correlation: 0.9991496470458585\n",
      "442: FEAT_A: 91561152 FEAT_B: Cauldron Filler (Assessment)_4025 - Correlation: 1.0\n",
      "443: FEAT_A: 01ca3a3c FEAT_B: Leaf Leader_4080 - Correlation: 0.9999999999999999\n",
      "444: FEAT_A: bd612267 FEAT_B: Chest Sorter (Assessment)_4070 - Correlation: 0.9999999999999999\n",
      "445: FEAT_A: df4940d3 FEAT_B: 67439901 - Correlation: 0.999935162643595\n",
      "446: FEAT_A: df4940d3 FEAT_B: Bottle Filler (Activity)_3110 - Correlation: 0.9999999999999999\n",
      "447: FEAT_A: df4940d3 FEAT_B: Bottle Filler (Activity)_3010 - Correlation: 0.999935162643595\n",
      "448: FEAT_A: 9ce586dd FEAT_B: Chest Sorter (Assessment)_4035 - Correlation: 1.0\n",
      "449: FEAT_A: 00c73085 FEAT_B: Dino Dive_2030 - Correlation: 0.9999999999999999\n",
      "450: FEAT_A: 77c76bc5 FEAT_B: Cauldron Filler (Assessment)_4090 - Correlation: 1.0\n",
      "451: FEAT_A: c6971acf FEAT_B: Dino Drink_2060 - Correlation: 0.9999999999999999\n",
      "452: FEAT_A: 71fe8f75 FEAT_B: 0a08139c - Correlation: 0.9999850342981554\n",
      "453: FEAT_A: 71fe8f75 FEAT_B: Bug Measurer (Activity)_3010 - Correlation: 0.9999850342981554\n",
      "454: FEAT_A: 71fe8f75 FEAT_B: Bug Measurer (Activity)_3110 - Correlation: 1.0\n",
      "455: FEAT_A: 65abac75 FEAT_B: Air Show_4010 - Correlation: 1.0\n",
      "456: FEAT_A: bfc77bd6 FEAT_B: Chest Sorter (Assessment)_4080 - Correlation: 0.9999999999999998\n",
      "457: FEAT_A: 99ea62f3 FEAT_B: Bubble Bath_2083 - Correlation: 1.0\n",
      "458: FEAT_A: 5348fd84 FEAT_B: Cauldron Filler (Assessment)_4040 - Correlation: 1.0\n",
      "459: FEAT_A: 3a4be871 FEAT_B: Flower Waterer (Activity)_4080 - Correlation: 1.0\n",
      "460: FEAT_A: a76029ee FEAT_B: Bird Measurer (Assessment)_4040 - Correlation: 0.9999999999999999\n",
      "461: FEAT_A: 884228c8 FEAT_B: Fireworks (Activity)_4070 - Correlation: 1.0\n",
      "462: FEAT_A: abc5811c FEAT_B: Happy Camel_4010 - Correlation: 1.0\n",
      "463: FEAT_A: 47efca07 FEAT_B: Bottle Filler (Activity)_4090 - Correlation: 1.0\n",
      "464: FEAT_A: 9e6b7fb5 FEAT_B: Chow Time_4095 - Correlation: 0.9999999999999998\n",
      "465: FEAT_A: d9c005dd FEAT_B: Happy Camel_2000 - Correlation: 1.0\n",
      "466: FEAT_A: 30614231 FEAT_B: 37ee8496 - Correlation: 0.9967763987631819\n",
      "467: FEAT_A: 30614231 FEAT_B: Cauldron Filler (Assessment)_4030 - Correlation: 0.9967763987631819\n",
      "468: FEAT_A: 30614231 FEAT_B: Cauldron Filler (Assessment)_4020 - Correlation: 1.0\n",
      "469: FEAT_A: f50fc6c1 FEAT_B: Watering Hole (Activity)_4021 - Correlation: 1.0\n",
      "470: FEAT_A: 2dc29e21 FEAT_B: All Star Sorting_4020 - Correlation: 1.0\n",
      "471: FEAT_A: 3d0b9317 FEAT_B: Chest Sorter (Assessment)_4040 - Correlation: 1.0\n",
      "472: FEAT_A: 14de4c5d FEAT_B: Air Show_4100 - Correlation: 1.0\n",
      "473: FEAT_A: d2659ab4 FEAT_B: Air Show_2075 - Correlation: 1.0\n",
      "474: FEAT_A: 3ee399c3 FEAT_B: Cauldron Filler (Assessment)_4070 - Correlation: 1.0\n",
      "475: FEAT_A: c0415e5c FEAT_B: Dino Dive_4020 - Correlation: 1.0\n",
      "476: FEAT_A: 6f4bd64e FEAT_B: Air Show_4090 - Correlation: 0.9999999999999998\n",
      "477: FEAT_A: 363c86c9 FEAT_B: Bug Measurer (Activity)_4035 - Correlation: 1.0\n",
      "478: FEAT_A: 51311d7a FEAT_B: Dino Drink_2000 - Correlation: 1.0\n",
      "479: FEAT_A: 3bb91ced FEAT_B: Happy Camel_2081 - Correlation: 1.0\n",
      "480: FEAT_A: d02b7a8e FEAT_B: All Star Sorting_4035 - Correlation: 1.0\n",
      "481: FEAT_A: 565a3990 FEAT_B: Bug Measurer (Activity)_4070 - Correlation: 1.0\n",
      "482: FEAT_A: 461eace6 FEAT_B: Egg Dropper (Activity)_4020 - Correlation: 1.0\n",
      "483: FEAT_A: 7fd1ac25 FEAT_B: Egg Dropper (Activity)_4080 - Correlation: 1.0\n",
      "484: FEAT_A: 4c2ec19f FEAT_B: Egg Dropper (Activity)_4025 - Correlation: 1.0\n",
      "485: FEAT_A: 1af8be29 FEAT_B: 3bf1cf26 - Correlation: 0.9998900847287077\n",
      "486: FEAT_A: 1af8be29 FEAT_B: Happy Camel_3120 - Correlation: 0.9998900847287077\n",
      "487: FEAT_A: 1af8be29 FEAT_B: Happy Camel_3020 - Correlation: 0.9999999999999999\n",
      "488: FEAT_A: 1325467d FEAT_B: Sandcastle Builder (Activity)_4070 - Correlation: 1.0\n",
      "489: FEAT_A: 7d5c30a2 FEAT_B: Dino Dive_2060 - Correlation: 1.0\n",
      "490: FEAT_A: c189aaf2 FEAT_B: Happy Camel_2083 - Correlation: 0.9999999999999999\n",
      "491: FEAT_A: 13f56524 FEAT_B: Mushroom Sorter (Assessment)_4080 - Correlation: 1.0\n",
      "492: FEAT_A: a8efe47b FEAT_B: Chest Sorter (Assessment)_4030 - Correlation: 1.0\n",
      "493: FEAT_A: 3393b68b FEAT_B: Bird Measurer (Assessment)_2010 - Correlation: 1.0\n",
      "494: FEAT_A: 2ec694de FEAT_B: Bug Measurer (Activity)_4080 - Correlation: 0.9999999999999998\n",
      "495: FEAT_A: 93edfe2e FEAT_B: Crystals Rule_4090 - Correlation: 1.0\n",
      "496: FEAT_A: fbaf3456 FEAT_B: Mushroom Sorter (Assessment)_4030 - Correlation: 0.9999999999999999\n",
      "497: FEAT_A: 587b5989 FEAT_B: All Star Sorting_4070 - Correlation: 1.0\n",
      "498: FEAT_A: 7dfe6d8a FEAT_B: Leaf Leader_4070 - Correlation: 0.9999999999999998\n",
      "499: FEAT_A: 85d1b0de FEAT_B: Chicken Balancer (Activity)_4090 - Correlation: 1.0\n",
      "500: FEAT_A: f93fc684 FEAT_B: Chow Time_4010 - Correlation: 0.9999999999999999\n",
      "501: FEAT_A: 4d6737eb FEAT_B: Dino Drink_2070 - Correlation: 1.0\n",
      "502: FEAT_A: b7dc8128 FEAT_B: 4b5efe37 - Correlation: 0.9980151285383981\n",
      "503: FEAT_A: b7dc8128 FEAT_B: All Star Sorting_2000 - Correlation: 1.0\n",
      "504: FEAT_A: b7dc8128 FEAT_B: All Star Sorting_4010 - Correlation: 0.9980151285383981\n",
      "505: FEAT_A: 1b54d27f FEAT_B: Watering Hole (Activity)_2010 - Correlation: 1.0\n",
      "506: FEAT_A: 90d848e0 FEAT_B: Cauldron Filler (Assessment)_2000 - Correlation: 1.0\n",
      "507: FEAT_A: 06372577 FEAT_B: Air Show_2060 - Correlation: 1.0\n",
      "508: FEAT_A: cb1178ad FEAT_B: Chest Sorter (Assessment)_4090 - Correlation: 1.0\n",
      "509: FEAT_A: 30df3273 FEAT_B: Sandcastle Builder (Activity)_4080 - Correlation: 1.0\n",
      "510: FEAT_A: 8d748b58 FEAT_B: Bug Measurer (Activity)_4090 - Correlation: 0.9999999999999998\n",
      "511: FEAT_A: 15ba1109 FEAT_B: Air Show_2000 - Correlation: 1.0\n",
      "512: FEAT_A: a5e9da97 FEAT_B: Pan Balance_4100 - Correlation: 1.0\n",
      "513: FEAT_A: 92687c59 FEAT_B: Scrub-A-Dub_4090 - Correlation: 1.0\n",
      "514: FEAT_A: 4bb2f698 FEAT_B: Chicken Balancer (Activity)_4070 - Correlation: 0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "to_remove = remove_correlated_features(reduce_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 426 features in LGBM\n",
      "Training with 425 features in NN and LR\n"
     ]
    }
   ],
   "source": [
    "features = [col for col in features if col not in to_remove]\n",
    "features = [col for col in features if col not in ['Heavy, Heavier, Heaviest_2000', 'Heavy, Heavier, Heaviest']]\n",
    "features.append('installation_id')\n",
    "print('Training with {} features in LGBM'.format(len(features)))\n",
    "\n",
    "lr_features = [col for col in lr_features if col not in to_remove]\n",
    "lr_features = [col for col in lr_features if col not in ['Heavy, Heavier, Heaviest_2000', 'Heavy, Heavier, Heaviest', \"session_title\"]]\n",
    "nn_features = [col for col in nn_features if col not in to_remove]\n",
    "nn_features = [col for col in nn_features if col not in ['Heavy, Heavier, Heaviest_2000', 'Heavy, Heavier, Heaviest', \"session_title\"]]\n",
    "lr_features.append('installation_id')\n",
    "nn_features.append('installation_id')\n",
    "print('Training with {} features in NN and LR'.format(len(lr_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_Cart Balancer (Assessment)\n",
      "misses\n",
      "6aeafed4\n",
      "ab4ec3a4\n",
      "a8cc6fec\n",
      "5dc079d8\n",
      "003cd2ee\n",
      "119b5b02\n",
      "ecc6157f\n",
      "29a42aea\n",
      "0ce40006\n",
      "dcb1663e\n",
      "17ca3959\n",
      "611485c5\n",
      "eb2c19cd\n",
      "e4d32835\n",
      "01ca3a3c\n",
      "4074bac2\n",
      "bfc77bd6\n",
      "7fd1ac25\n",
      "13f56524\n",
      "2ec694de\n",
      "1b54d27f\n",
      "Crystals Rule_2010\n",
      "Air Show_4080\n",
      "Pan Balance_2010\n",
      "Sandcastle Builder (Activity)_2010\n",
      "Bottle Filler (Activity)_2010\n",
      "Scrub-A-Dub_4080\n"
     ]
    }
   ],
   "source": [
    "to_exclude, ajusted_test = exclude(reduce_train, reduce_test, features)\n",
    "features = [col for col in features if col not in to_exclude]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modelling and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "used validation data:  721\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870295\tvalid_1's rmse: 1.02686\n",
      "[200]\ttraining's rmse: 0.791164\tvalid_1's rmse: 1.02195\n",
      "Early stopping, best iteration is:\n",
      "[185]\ttraining's rmse: 0.801493\tvalid_1's rmse: 1.02076\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.871566\tvalid_1's rmse: 1.03133\n",
      "[200]\ttraining's rmse: 0.793145\tvalid_1's rmse: 1.03554\n",
      "Early stopping, best iteration is:\n",
      "[125]\ttraining's rmse: 0.848612\tvalid_1's rmse: 1.03045\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870242\tvalid_1's rmse: 1.02202\n",
      "[200]\ttraining's rmse: 0.790982\tvalid_1's rmse: 1.02794\n",
      "Early stopping, best iteration is:\n",
      "[115]\ttraining's rmse: 0.856993\tvalid_1's rmse: 1.02061\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.864278\tvalid_1's rmse: 1.03917\n",
      "[200]\ttraining's rmse: 0.786535\tvalid_1's rmse: 1.03945\n",
      "Early stopping, best iteration is:\n",
      "[143]\ttraining's rmse: 0.827372\tvalid_1's rmse: 1.03786\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.860438\tvalid_1's rmse: 1.04336\n",
      "[200]\ttraining's rmse: 0.780821\tvalid_1's rmse: 1.04382\n",
      "Early stopping, best iteration is:\n",
      "[137]\ttraining's rmse: 0.827814\tvalid_1's rmse: 1.03977\n",
      "Our oof rmse score is: 1.0299324946521067\n",
      "Our oof cohen kappa score is: 0.5697128485571106\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870295\tvalid_1's rmse: 1.03293\n",
      "[200]\ttraining's rmse: 0.791164\tvalid_1's rmse: 1.02858\n",
      "Early stopping, best iteration is:\n",
      "[174]\ttraining's rmse: 0.808937\tvalid_1's rmse: 1.02767\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.871566\tvalid_1's rmse: 1.03706\n",
      "Early stopping, best iteration is:\n",
      "[97]\ttraining's rmse: 0.87426\tvalid_1's rmse: 1.03679\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870242\tvalid_1's rmse: 1.01029\n",
      "[200]\ttraining's rmse: 0.790982\tvalid_1's rmse: 1.01755\n",
      "Early stopping, best iteration is:\n",
      "[102]\ttraining's rmse: 0.868338\tvalid_1's rmse: 1.00991\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.864278\tvalid_1's rmse: 1.06983\n",
      "Early stopping, best iteration is:\n",
      "[61]\ttraining's rmse: 0.906542\tvalid_1's rmse: 1.06828\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.860438\tvalid_1's rmse: 1.05606\n",
      "[200]\ttraining's rmse: 0.780821\tvalid_1's rmse: 1.05863\n",
      "Early stopping, best iteration is:\n",
      "[114]\ttraining's rmse: 0.847663\tvalid_1's rmse: 1.05476\n",
      "Our oof rmse score is: 1.039702514758471\n",
      "Our oof cohen kappa score is: 0.5360891853409234\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870295\tvalid_1's rmse: 1.0332\n",
      "[200]\ttraining's rmse: 0.791164\tvalid_1's rmse: 1.03079\n",
      "[300]\ttraining's rmse: 0.728963\tvalid_1's rmse: 1.03386\n",
      "Early stopping, best iteration is:\n",
      "[246]\ttraining's rmse: 0.760975\tvalid_1's rmse: 1.02907\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.871566\tvalid_1's rmse: 1.03005\n",
      "[200]\ttraining's rmse: 0.793145\tvalid_1's rmse: 1.0337\n",
      "Early stopping, best iteration is:\n",
      "[133]\ttraining's rmse: 0.841532\tvalid_1's rmse: 1.02848\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870242\tvalid_1's rmse: 0.999314\n",
      "[200]\ttraining's rmse: 0.790982\tvalid_1's rmse: 0.998481\n",
      "[300]\ttraining's rmse: 0.731498\tvalid_1's rmse: 0.998073\n",
      "Early stopping, best iteration is:\n",
      "[225]\ttraining's rmse: 0.774502\tvalid_1's rmse: 0.996454\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.864278\tvalid_1's rmse: 1.09737\n",
      "[200]\ttraining's rmse: 0.786535\tvalid_1's rmse: 1.09826\n",
      "Early stopping, best iteration is:\n",
      "[143]\ttraining's rmse: 0.827372\tvalid_1's rmse: 1.0946\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.860438\tvalid_1's rmse: 1.04459\n",
      "[200]\ttraining's rmse: 0.780821\tvalid_1's rmse: 1.04522\n",
      "Early stopping, best iteration is:\n",
      "[135]\ttraining's rmse: 0.829498\tvalid_1's rmse: 1.04258\n",
      "Our oof rmse score is: 1.0387554059100716\n",
      "Our oof cohen kappa score is: 0.5451528334594444\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870295\tvalid_1's rmse: 1.10649\n",
      "[200]\ttraining's rmse: 0.791164\tvalid_1's rmse: 1.10317\n",
      "Early stopping, best iteration is:\n",
      "[171]\ttraining's rmse: 0.811099\tvalid_1's rmse: 1.10165\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.871566\tvalid_1's rmse: 1.0105\n",
      "[200]\ttraining's rmse: 0.793145\tvalid_1's rmse: 1.009\n",
      "Early stopping, best iteration is:\n",
      "[128]\ttraining's rmse: 0.845921\tvalid_1's rmse: 1.00726\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870242\tvalid_1's rmse: 1.01542\n",
      "Early stopping, best iteration is:\n",
      "[91]\ttraining's rmse: 0.878759\tvalid_1's rmse: 1.01531\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.864278\tvalid_1's rmse: 1.05632\n",
      "[200]\ttraining's rmse: 0.786535\tvalid_1's rmse: 1.06395\n",
      "Early stopping, best iteration is:\n",
      "[105]\ttraining's rmse: 0.859403\tvalid_1's rmse: 1.05501\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.860438\tvalid_1's rmse: 1.04151\n",
      "[200]\ttraining's rmse: 0.780821\tvalid_1's rmse: 1.04181\n",
      "Early stopping, best iteration is:\n",
      "[137]\ttraining's rmse: 0.827814\tvalid_1's rmse: 1.03915\n",
      "Our oof rmse score is: 1.0441957741836532\n",
      "Our oof cohen kappa score is: 0.5426376486105913\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870295\tvalid_1's rmse: 1.04602\n",
      "[200]\ttraining's rmse: 0.791164\tvalid_1's rmse: 1.04201\n",
      "Early stopping, best iteration is:\n",
      "[171]\ttraining's rmse: 0.811099\tvalid_1's rmse: 1.04124\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.871566\tvalid_1's rmse: 1.03421\n",
      "[200]\ttraining's rmse: 0.793145\tvalid_1's rmse: 1.03787\n",
      "Early stopping, best iteration is:\n",
      "[110]\ttraining's rmse: 0.862409\tvalid_1's rmse: 1.03306\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870242\tvalid_1's rmse: 0.996685\n",
      "[200]\ttraining's rmse: 0.790982\tvalid_1's rmse: 1.00523\n",
      "Early stopping, best iteration is:\n",
      "[101]\ttraining's rmse: 0.869179\tvalid_1's rmse: 0.996442\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.864278\tvalid_1's rmse: 1.08701\n",
      "[200]\ttraining's rmse: 0.786535\tvalid_1's rmse: 1.0921\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttraining's rmse: 0.86143\tvalid_1's rmse: 1.0867\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.860438\tvalid_1's rmse: 1.04243\n",
      "[200]\ttraining's rmse: 0.780821\tvalid_1's rmse: 1.04666\n",
      "Early stopping, best iteration is:\n",
      "[110]\ttraining's rmse: 0.850993\tvalid_1's rmse: 1.04149\n",
      "Our oof rmse score is: 1.0401999872260657\n",
      "Our oof cohen kappa score is: 0.5511702085002961\n",
      "Our mean rmse score is:  1.0385572353460737\n",
      "Our mean cohen kappa score is:  0.5489525448936732\n"
     ]
    }
   ],
   "source": [
    "# train 5 times because the evaluation and training data change with the randomness\n",
    "y_pred_1, oof_rmse_score_1, oof_cohen_score_1 = run_lgb(reduce_train, ajusted_test, features)\n",
    "y_pred_2, oof_rmse_score_2, oof_cohen_score_2 = run_lgb(reduce_train, ajusted_test, features)\n",
    "y_pred_3, oof_rmse_score_3, oof_cohen_score_3 = run_lgb(reduce_train, ajusted_test, features)\n",
    "y_pred_4, oof_rmse_score_4, oof_cohen_score_4 = run_lgb(reduce_train, ajusted_test, features)\n",
    "y_pred_5, oof_rmse_score_5, oof_cohen_score_5 = run_lgb(reduce_train, ajusted_test, features)\n",
    "mean_rmse = (oof_rmse_score_1 + oof_rmse_score_2 + oof_rmse_score_3 + oof_rmse_score_4 + oof_rmse_score_5) / 5\n",
    "mean_cohen_kappa = (oof_cohen_score_1 + oof_cohen_score_2 + oof_cohen_score_3 + oof_cohen_score_4 + oof_cohen_score_5) / 5\n",
    "print('Our mean rmse score is: ', mean_rmse)\n",
    "print('Our mean cohen kappa score is: ', mean_cohen_kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression and neutal network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_std, test_std = standardize_data(reduce_train, ajusted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "used validation data:  721\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Our oof rmse score is: 1.323370624263243\n",
      "Our oof cohen kappa score is: 0.3621713527205296\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Our oof rmse score is: 1.286606344347764\n",
      "Our oof cohen kappa score is: 0.37888036094989275\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Our oof rmse score is: 1.3199924706619068\n",
      "Our oof cohen kappa score is: 0.3594178222380865\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Our oof rmse score is: 1.322448800818068\n",
      "Our oof cohen kappa score is: 0.36811973441767176\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Our oof rmse score is: 1.3403434573073227\n",
      "Our oof cohen kappa score is: 0.34780192346428773\n",
      "Our mean rmse score is:  1.318552339479661\n",
      "Our mean cohen kappa score is:  0.3632782387580937\n"
     ]
    }
   ],
   "source": [
    "for i in train_std.columns:\n",
    "    if \"session_title\" in str(i):\n",
    "        lr_features.append(i)\n",
    "y_pred_1_lr, oof_rmse_score_1_lr, oof_cohen_score_1_lr = run_lr(train_std, test_std, nn_features)\n",
    "y_pred_2_lr, oof_rmse_score_2_lr, oof_cohen_score_2_lr = run_lr(train_std, test_std, nn_features)\n",
    "y_pred_3_lr, oof_rmse_score_3_lr, oof_cohen_score_3_lr = run_lr(train_std, test_std, nn_features)\n",
    "y_pred_4_lr, oof_rmse_score_4_lr, oof_cohen_score_4_lr = run_lr(train_std, test_std, nn_features)\n",
    "y_pred_5_lr, oof_rmse_score_5_lr, oof_cohen_score_5_lr = run_lr(train_std, test_std, nn_features)\n",
    "mean_rmse_lr = (oof_rmse_score_1_lr + oof_rmse_score_2_lr + oof_rmse_score_3_lr + oof_rmse_score_4_lr + oof_rmse_score_5_lr) / 5\n",
    "mean_cohen_kappa_lr = (oof_cohen_score_1_lr + oof_cohen_score_2_lr + oof_cohen_score_3_lr + oof_cohen_score_4_lr + oof_cohen_score_5_lr) / 5\n",
    "print('Our mean rmse score is: ', mean_rmse_lr)\n",
    "print('Our mean cohen kappa score is: ', mean_cohen_kappa_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "used validation data:  721\n",
      "Train on 14152 samples, validate on 721 samples\n",
      "Epoch 1/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.3277\n",
      "Epoch 00001: val_loss improved from inf to 1.32660, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 228us/sample - loss: 1.3238 - val_loss: 1.3266\n",
      "Epoch 2/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.1523\n",
      "Epoch 00002: val_loss improved from 1.32660 to 1.26210, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 143us/sample - loss: 1.1507 - val_loss: 1.2621\n",
      "Epoch 3/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.1096\n",
      "Epoch 00003: val_loss improved from 1.26210 to 1.24879, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 148us/sample - loss: 1.1134 - val_loss: 1.2488\n",
      "Epoch 4/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0896\n",
      "Epoch 00004: val_loss did not improve from 1.24879\n",
      "14152/14152 [==============================] - 2s 147us/sample - loss: 1.0897 - val_loss: 1.2861\n",
      "Epoch 5/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0835\n",
      "Epoch 00005: val_loss improved from 1.24879 to 1.23899, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 149us/sample - loss: 1.0816 - val_loss: 1.2390\n",
      "Epoch 6/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0695\n",
      "Epoch 00006: val_loss did not improve from 1.23899\n",
      "14152/14152 [==============================] - 2s 146us/sample - loss: 1.0692 - val_loss: 1.2902\n",
      "Epoch 7/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0500\n",
      "Epoch 00007: val_loss did not improve from 1.23899\n",
      "14152/14152 [==============================] - 2s 145us/sample - loss: 1.0470 - val_loss: 1.2572\n",
      "Epoch 8/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0389\n",
      "Epoch 00008: val_loss did not improve from 1.23899\n",
      "14152/14152 [==============================] - 2s 143us/sample - loss: 1.0388 - val_loss: 1.2437\n",
      "Epoch 9/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0390\n",
      "Epoch 00009: val_loss did not improve from 1.23899\n",
      "14152/14152 [==============================] - 2s 141us/sample - loss: 1.0383 - val_loss: 1.2566\n",
      "Epoch 10/100\n",
      "13760/14152 [============================>.] - ETA: 0s - loss: 1.0260\n",
      "Epoch 00010: val_loss did not improve from 1.23899\n",
      "14152/14152 [==============================] - 2s 142us/sample - loss: 1.0247 - val_loss: 1.2602\n",
      "Epoch 11/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0162\n",
      "Epoch 00011: val_loss improved from 1.23899 to 1.18783, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 145us/sample - loss: 1.0168 - val_loss: 1.1878\n",
      "Epoch 12/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0230\n",
      "Epoch 00012: val_loss did not improve from 1.18783\n",
      "14152/14152 [==============================] - 2s 147us/sample - loss: 1.0228 - val_loss: 1.2839\n",
      "Epoch 13/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0087\n",
      "Epoch 00013: val_loss did not improve from 1.18783\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 1.0076 - val_loss: 1.2257\n",
      "Epoch 14/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9976\n",
      "Epoch 00014: val_loss did not improve from 1.18783\n",
      "14152/14152 [==============================] - 2s 150us/sample - loss: 0.9982 - val_loss: 1.2129\n",
      "Epoch 15/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9972\n",
      "Epoch 00015: val_loss did not improve from 1.18783\n",
      "14152/14152 [==============================] - 2s 143us/sample - loss: 0.9958 - val_loss: 1.2217\n",
      "Epoch 16/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0014\n",
      "Epoch 00016: val_loss did not improve from 1.18783\n",
      "14152/14152 [==============================] - 2s 142us/sample - loss: 1.0013 - val_loss: 1.2277\n",
      "Epoch 17/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0050\n",
      "Epoch 00017: val_loss did not improve from 1.18783\n",
      "14152/14152 [==============================] - 2s 147us/sample - loss: 1.0052 - val_loss: 1.2315\n",
      "Epoch 18/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9875\n",
      "Epoch 00018: val_loss did not improve from 1.18783\n",
      "14152/14152 [==============================] - 2s 151us/sample - loss: 0.9876 - val_loss: 1.2902\n",
      "Epoch 19/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9891\n",
      "Epoch 00019: val_loss did not improve from 1.18783\n",
      "14152/14152 [==============================] - 2s 144us/sample - loss: 0.9902 - val_loss: 1.2063\n",
      "Epoch 20/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9882\n",
      "Epoch 00020: val_loss did not improve from 1.18783\n",
      "14152/14152 [==============================] - 2s 145us/sample - loss: 0.9891 - val_loss: 1.2057\n",
      "Epoch 21/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9734\n",
      "Epoch 00021: val_loss did not improve from 1.18783\n",
      "14152/14152 [==============================] - 2s 144us/sample - loss: 0.9739 - val_loss: 1.2016\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Train on 14152 samples, validate on 722 samples\n",
      "Epoch 1/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.3629\n",
      "Epoch 00001: val_loss improved from inf to 1.16980, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 207us/sample - loss: 1.3619 - val_loss: 1.1698\n",
      "Epoch 2/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.1462\n",
      "Epoch 00002: val_loss did not improve from 1.16980\n",
      "14152/14152 [==============================] - 2s 145us/sample - loss: 1.1475 - val_loss: 1.1839\n",
      "Epoch 3/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.1008\n",
      "Epoch 00003: val_loss improved from 1.16980 to 1.13204, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 144us/sample - loss: 1.1005 - val_loss: 1.1320\n",
      "Epoch 4/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0702\n",
      "Epoch 00004: val_loss did not improve from 1.13204\n",
      "14152/14152 [==============================] - 2s 143us/sample - loss: 1.0718 - val_loss: 1.1684\n",
      "Epoch 5/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0619\n",
      "Epoch 00005: val_loss improved from 1.13204 to 1.12039, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 144us/sample - loss: 1.0632 - val_loss: 1.1204\n",
      "Epoch 6/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0552\n",
      "Epoch 00006: val_loss did not improve from 1.12039\n",
      "14152/14152 [==============================] - 2s 143us/sample - loss: 1.0551 - val_loss: 1.1384\n",
      "Epoch 7/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0369\n",
      "Epoch 00007: val_loss improved from 1.12039 to 1.11194, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 141us/sample - loss: 1.0379 - val_loss: 1.1119\n",
      "Epoch 8/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0257\n",
      "Epoch 00008: val_loss improved from 1.11194 to 1.10916, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 142us/sample - loss: 1.0246 - val_loss: 1.1092\n",
      "Epoch 9/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0413\n",
      "Epoch 00009: val_loss improved from 1.10916 to 1.08469, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 142us/sample - loss: 1.0403 - val_loss: 1.0847\n",
      "Epoch 10/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0174\n",
      "Epoch 00010: val_loss did not improve from 1.08469\n",
      "14152/14152 [==============================] - 2s 141us/sample - loss: 1.0178 - val_loss: 1.1104\n",
      "Epoch 11/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0111\n",
      "Epoch 00011: val_loss did not improve from 1.08469\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.0110 - val_loss: 1.1105\n",
      "Epoch 12/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0101\n",
      "Epoch 00012: val_loss did not improve from 1.08469\n",
      "14152/14152 [==============================] - 2s 141us/sample - loss: 1.0093 - val_loss: 1.1288\n",
      "Epoch 13/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0125\n",
      "Epoch 00013: val_loss improved from 1.08469 to 1.07253, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 145us/sample - loss: 1.0110 - val_loss: 1.0725\n",
      "Epoch 14/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9974\n",
      "Epoch 00014: val_loss did not improve from 1.07253\n",
      "14152/14152 [==============================] - 2s 146us/sample - loss: 0.9973 - val_loss: 1.1103\n",
      "Epoch 15/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9918\n",
      "Epoch 00015: val_loss did not improve from 1.07253\n",
      "14152/14152 [==============================] - 2s 145us/sample - loss: 0.9922 - val_loss: 1.0957\n",
      "Epoch 16/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9867\n",
      "Epoch 00016: val_loss did not improve from 1.07253\n",
      "14152/14152 [==============================] - 2s 145us/sample - loss: 0.9890 - val_loss: 1.1065\n",
      "Epoch 17/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9785\n",
      "Epoch 00017: val_loss did not improve from 1.07253\n",
      "14152/14152 [==============================] - 2s 146us/sample - loss: 0.9797 - val_loss: 1.1090\n",
      "Epoch 18/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9847\n",
      "Epoch 00018: val_loss did not improve from 1.07253\n",
      "14152/14152 [==============================] - 2s 144us/sample - loss: 0.9827 - val_loss: 1.1159\n",
      "Epoch 19/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9729\n",
      "Epoch 00019: val_loss improved from 1.07253 to 1.06203, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 147us/sample - loss: 0.9730 - val_loss: 1.0620\n",
      "Epoch 20/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9701\n",
      "Epoch 00020: val_loss did not improve from 1.06203\n",
      "14152/14152 [==============================] - 2s 150us/sample - loss: 0.9699 - val_loss: 1.0950\n",
      "Epoch 21/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9712\n",
      "Epoch 00021: val_loss did not improve from 1.06203\n",
      "14152/14152 [==============================] - 2s 151us/sample - loss: 0.9711 - val_loss: 1.1265\n",
      "Epoch 22/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9744\n",
      "Epoch 00022: val_loss did not improve from 1.06203\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 0.9737 - val_loss: 1.1088\n",
      "Epoch 23/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9598\n",
      "Epoch 00023: val_loss did not improve from 1.06203\n",
      "14152/14152 [==============================] - 2s 151us/sample - loss: 0.9613 - val_loss: 1.1061\n",
      "Epoch 24/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9685\n",
      "Epoch 00024: val_loss did not improve from 1.06203\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 0.9668 - val_loss: 1.1289\n",
      "Epoch 25/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9545\n",
      "Epoch 00025: val_loss did not improve from 1.06203\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 0.9557 - val_loss: 1.1221\n",
      "Epoch 26/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9525\n",
      "Epoch 00026: val_loss did not improve from 1.06203\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 0.9522 - val_loss: 1.1205\n",
      "Epoch 27/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9367\n",
      "Epoch 00027: val_loss did not improve from 1.06203\n",
      "14152/14152 [==============================] - 2s 151us/sample - loss: 0.9373 - val_loss: 1.1214\n",
      "Epoch 28/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9468\n",
      "Epoch 00028: val_loss did not improve from 1.06203\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 0.9468 - val_loss: 1.0907\n",
      "Epoch 29/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9434\n",
      "Epoch 00029: val_loss did not improve from 1.06203\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 0.9435 - val_loss: 1.1292\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Train on 14152 samples, validate on 723 samples\n",
      "Epoch 1/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.3191\n",
      "Epoch 00001: val_loss improved from inf to 1.13273, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 4s 249us/sample - loss: 1.3191 - val_loss: 1.1327\n",
      "Epoch 2/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1505\n",
      "Epoch 00002: val_loss did not improve from 1.13273\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 1.1503 - val_loss: 1.1405\n",
      "Epoch 3/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.1176\n",
      "Epoch 00003: val_loss did not improve from 1.13273\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.1150 - val_loss: 1.1473\n",
      "Epoch 4/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.1060\n",
      "Epoch 00004: val_loss improved from 1.13273 to 1.12090, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.1062 - val_loss: 1.1209\n",
      "Epoch 5/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0747\n",
      "Epoch 00005: val_loss did not improve from 1.12090\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.0751 - val_loss: 1.1364\n",
      "Epoch 6/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0575\n",
      "Epoch 00006: val_loss improved from 1.12090 to 1.11849, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.0584 - val_loss: 1.1185\n",
      "Epoch 7/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0557\n",
      "Epoch 00007: val_loss did not improve from 1.11849\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 1.0553 - val_loss: 1.1414\n",
      "Epoch 8/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0526\n",
      "Epoch 00008: val_loss improved from 1.11849 to 1.10874, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.0533 - val_loss: 1.1087\n",
      "Epoch 9/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0473\n",
      "Epoch 00009: val_loss did not improve from 1.10874\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 1.0480 - val_loss: 1.1260\n",
      "Epoch 10/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0404\n",
      "Epoch 00010: val_loss did not improve from 1.10874\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 1.0397 - val_loss: 1.1361\n",
      "Epoch 11/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0365\n",
      "Epoch 00011: val_loss improved from 1.10874 to 1.09973, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.0360 - val_loss: 1.0997\n",
      "Epoch 12/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0310\n",
      "Epoch 00012: val_loss improved from 1.09973 to 1.08103, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 1.0295 - val_loss: 1.0810\n",
      "Epoch 13/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0236\n",
      "Epoch 00013: val_loss did not improve from 1.08103\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 1.0227 - val_loss: 1.1116\n",
      "Epoch 14/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0163\n",
      "Epoch 00014: val_loss did not improve from 1.08103\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 1.0162 - val_loss: 1.1194\n",
      "Epoch 15/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0241\n",
      "Epoch 00015: val_loss improved from 1.08103 to 1.07117, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 1.0237 - val_loss: 1.0712\n",
      "Epoch 16/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0174\n",
      "Epoch 00016: val_loss did not improve from 1.07117\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 1.0169 - val_loss: 1.0825\n",
      "Epoch 17/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0147\n",
      "Epoch 00017: val_loss did not improve from 1.07117\n",
      "14152/14152 [==============================] - 3s 181us/sample - loss: 1.0145 - val_loss: 1.0853\n",
      "Epoch 18/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0141\n",
      "Epoch 00018: val_loss did not improve from 1.07117\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0145 - val_loss: 1.0937\n",
      "Epoch 19/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0083\n",
      "Epoch 00019: val_loss did not improve from 1.07117\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0079 - val_loss: 1.0857\n",
      "Epoch 20/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0018\n",
      "Epoch 00020: val_loss did not improve from 1.07117\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0007 - val_loss: 1.0805\n",
      "Epoch 21/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0052\n",
      "Epoch 00021: val_loss did not improve from 1.07117\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 1.0058 - val_loss: 1.0971\n",
      "Epoch 22/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0061\n",
      "Epoch 00022: val_loss did not improve from 1.07117\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 1.0056 - val_loss: 1.1175\n",
      "Epoch 23/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9885\n",
      "Epoch 00023: val_loss did not improve from 1.07117\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 0.9894 - val_loss: 1.0859\n",
      "Epoch 24/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9861\n",
      "Epoch 00024: val_loss did not improve from 1.07117\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 0.9861 - val_loss: 1.1090\n",
      "Epoch 25/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9913\n",
      "Epoch 00025: val_loss did not improve from 1.07117\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 0.9914 - val_loss: 1.0910\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Train on 14152 samples, validate on 724 samples\n",
      "Epoch 1/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.3476\n",
      "Epoch 00001: val_loss improved from inf to 1.27221, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 220us/sample - loss: 1.3453 - val_loss: 1.2722\n",
      "Epoch 2/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.1406\n",
      "Epoch 00002: val_loss improved from 1.27221 to 1.24911, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 178us/sample - loss: 1.1392 - val_loss: 1.2491\n",
      "Epoch 3/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0983\n",
      "Epoch 00003: val_loss did not improve from 1.24911\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 1.0982 - val_loss: 1.2540\n",
      "Epoch 4/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0835\n",
      "Epoch 00004: val_loss did not improve from 1.24911\n",
      "14152/14152 [==============================] - 3s 184us/sample - loss: 1.0813 - val_loss: 1.3499\n",
      "Epoch 5/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0695\n",
      "Epoch 00005: val_loss improved from 1.24911 to 1.23113, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 1.0709 - val_loss: 1.2311\n",
      "Epoch 6/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0410\n",
      "Epoch 00006: val_loss did not improve from 1.23113\n",
      "14152/14152 [==============================] - 2s 171us/sample - loss: 1.0410 - val_loss: 1.2472\n",
      "Epoch 7/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0450\n",
      "Epoch 00007: val_loss improved from 1.23113 to 1.20039, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 1.0446 - val_loss: 1.2004\n",
      "Epoch 8/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0310\n",
      "Epoch 00008: val_loss did not improve from 1.20039\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.0321 - val_loss: 1.2502\n",
      "Epoch 9/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0231\n",
      "Epoch 00009: val_loss did not improve from 1.20039\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0216 - val_loss: 1.2710\n",
      "Epoch 10/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0203\n",
      "Epoch 00010: val_loss improved from 1.20039 to 1.15395, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0201 - val_loss: 1.1539\n",
      "Epoch 11/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0162\n",
      "Epoch 00011: val_loss did not improve from 1.15395\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 1.0152 - val_loss: 1.1756\n",
      "Epoch 12/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9950\n",
      "Epoch 00012: val_loss did not improve from 1.15395\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 0.9957 - val_loss: 1.1864\n",
      "Epoch 13/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9963\n",
      "Epoch 00013: val_loss did not improve from 1.15395\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 0.9960 - val_loss: 1.2274\n",
      "Epoch 14/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9925\n",
      "Epoch 00014: val_loss did not improve from 1.15395\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 0.9928 - val_loss: 1.1788\n",
      "Epoch 15/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9917\n",
      "Epoch 00015: val_loss did not improve from 1.15395\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 0.9898 - val_loss: 1.2238\n",
      "Epoch 16/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9820\n",
      "Epoch 00016: val_loss did not improve from 1.15395\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 0.9816 - val_loss: 1.2560\n",
      "Epoch 17/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9874\n",
      "Epoch 00017: val_loss did not improve from 1.15395\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 0.9875 - val_loss: 1.1604\n",
      "Epoch 18/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9777\n",
      "Epoch 00018: val_loss did not improve from 1.15395\n",
      "14152/14152 [==============================] - 2s 150us/sample - loss: 0.9780 - val_loss: 1.1676\n",
      "Epoch 19/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9732\n",
      "Epoch 00019: val_loss did not improve from 1.15395\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 0.9757 - val_loss: 1.1669\n",
      "Epoch 20/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9670\n",
      "Epoch 00020: val_loss did not improve from 1.15395\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 0.9678 - val_loss: 1.2096\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Train on 14152 samples, validate on 724 samples\n",
      "Epoch 1/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.3487\n",
      "Epoch 00001: val_loss improved from inf to 1.17558, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 223us/sample - loss: 1.3470 - val_loss: 1.1756\n",
      "Epoch 2/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.1185\n",
      "Epoch 00002: val_loss did not improve from 1.17558\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 1.1177 - val_loss: 1.1923\n",
      "Epoch 3/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0912\n",
      "Epoch 00003: val_loss did not improve from 1.17558\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0937 - val_loss: 1.2172\n",
      "Epoch 4/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0618\n",
      "Epoch 00004: val_loss did not improve from 1.17558\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 1.0611 - val_loss: 1.1835\n",
      "Epoch 5/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0421\n",
      "Epoch 00005: val_loss did not improve from 1.17558\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0416 - val_loss: 1.2743\n",
      "Epoch 6/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0288\n",
      "Epoch 00006: val_loss did not improve from 1.17558\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 1.0283 - val_loss: 1.2310\n",
      "Epoch 7/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0226\n",
      "Epoch 00007: val_loss did not improve from 1.17558\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0234 - val_loss: 1.2029\n",
      "Epoch 8/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0224\n",
      "Epoch 00008: val_loss did not improve from 1.17558\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0234 - val_loss: 1.1967\n",
      "Epoch 9/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0166\n",
      "Epoch 00009: val_loss improved from 1.17558 to 1.16133, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 1.0163 - val_loss: 1.1613\n",
      "Epoch 10/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9973\n",
      "Epoch 00010: val_loss did not improve from 1.16133\n",
      "14152/14152 [==============================] - 2s 153us/sample - loss: 0.9998 - val_loss: 1.1674\n",
      "Epoch 11/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9989\n",
      "Epoch 00011: val_loss did not improve from 1.16133\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 0.9994 - val_loss: 1.1703\n",
      "Epoch 12/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9926\n",
      "Epoch 00012: val_loss did not improve from 1.16133\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 0.9919 - val_loss: 1.2705\n",
      "Epoch 13/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0071\n",
      "Epoch 00013: val_loss did not improve from 1.16133\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 1.0063 - val_loss: 1.2147\n",
      "Epoch 14/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9871\n",
      "Epoch 00014: val_loss did not improve from 1.16133\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 0.9872 - val_loss: 1.2000\n",
      "Epoch 15/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9773\n",
      "Epoch 00015: val_loss did not improve from 1.16133\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 0.9779 - val_loss: 1.1691\n",
      "Epoch 16/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9742\n",
      "Epoch 00016: val_loss did not improve from 1.16133\n",
      "14152/14152 [==============================] - 2s 173us/sample - loss: 0.9740 - val_loss: 1.2149\n",
      "Epoch 17/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9774\n",
      "Epoch 00017: val_loss did not improve from 1.16133\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 0.9779 - val_loss: 1.1950\n",
      "Epoch 18/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9692\n",
      "Epoch 00018: val_loss did not improve from 1.16133\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 0.9689 - val_loss: 1.1754\n",
      "Epoch 19/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9798\n",
      "Epoch 00019: val_loss did not improve from 1.16133\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 0.9798 - val_loss: 1.2093\n",
      "Our oof rmse score is: 1.0617257057083984\n",
      "Our oof cohen kappa score is: 0.5234011299154179\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Train on 14152 samples, validate on 721 samples\n",
      "Epoch 1/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.3138\n",
      "Epoch 00001: val_loss improved from inf to 1.25998, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 224us/sample - loss: 1.3125 - val_loss: 1.2600\n",
      "Epoch 2/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.1474\n",
      "Epoch 00002: val_loss improved from 1.25998 to 1.21321, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 1.1474 - val_loss: 1.2132\n",
      "Epoch 3/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.1211\n",
      "Epoch 00003: val_loss did not improve from 1.21321\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.1208 - val_loss: 1.2205\n",
      "Epoch 4/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0798\n",
      "Epoch 00004: val_loss did not improve from 1.21321\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0805 - val_loss: 1.2151\n",
      "Epoch 5/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0765\n",
      "Epoch 00005: val_loss improved from 1.21321 to 1.20426, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0767 - val_loss: 1.2043\n",
      "Epoch 6/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0617\n",
      "Epoch 00006: val_loss did not improve from 1.20426\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0589 - val_loss: 1.2581\n",
      "Epoch 7/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0584\n",
      "Epoch 00007: val_loss improved from 1.20426 to 1.19157, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.0594 - val_loss: 1.1916\n",
      "Epoch 8/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0492\n",
      "Epoch 00008: val_loss did not improve from 1.19157\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0469 - val_loss: 1.1976\n",
      "Epoch 9/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0247\n",
      "Epoch 00009: val_loss did not improve from 1.19157\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0245 - val_loss: 1.2228\n",
      "Epoch 10/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0319\n",
      "Epoch 00010: val_loss did not improve from 1.19157\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0327 - val_loss: 1.2101\n",
      "Epoch 11/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0182\n",
      "Epoch 00011: val_loss did not improve from 1.19157\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.0169 - val_loss: 1.2206\n",
      "Epoch 12/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0164\n",
      "Epoch 00012: val_loss did not improve from 1.19157\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0177 - val_loss: 1.2356\n",
      "Epoch 13/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0188\n",
      "Epoch 00013: val_loss improved from 1.19157 to 1.17267, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0192 - val_loss: 1.1727\n",
      "Epoch 14/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0006\n",
      "Epoch 00014: val_loss did not improve from 1.17267\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 0.9995 - val_loss: 1.1755\n",
      "Epoch 15/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9975\n",
      "Epoch 00015: val_loss did not improve from 1.17267\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 0.9948 - val_loss: 1.2215\n",
      "Epoch 16/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9935\n",
      "Epoch 00016: val_loss did not improve from 1.17267\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 0.9929 - val_loss: 1.1766\n",
      "Epoch 17/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9944\n",
      "Epoch 00017: val_loss did not improve from 1.17267\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 0.9931 - val_loss: 1.2011\n",
      "Epoch 18/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0003\n",
      "Epoch 00018: val_loss did not improve from 1.17267\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 0.9986 - val_loss: 1.2156\n",
      "Epoch 19/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9868\n",
      "Epoch 00019: val_loss did not improve from 1.17267\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 0.9861 - val_loss: 1.2422\n",
      "Epoch 20/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9809\n",
      "Epoch 00020: val_loss did not improve from 1.17267\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 0.9800 - val_loss: 1.1956\n",
      "Epoch 21/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9743\n",
      "Epoch 00021: val_loss did not improve from 1.17267\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 0.9737 - val_loss: 1.1838\n",
      "Epoch 22/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9672\n",
      "Epoch 00022: val_loss did not improve from 1.17267\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 0.9684 - val_loss: 1.2721\n",
      "Epoch 23/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9698\n",
      "Epoch 00023: val_loss did not improve from 1.17267\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 0.9697 - val_loss: 1.1783\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Train on 14152 samples, validate on 722 samples\n",
      "Epoch 1/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.3328\n",
      "Epoch 00001: val_loss improved from inf to 1.19310, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 224us/sample - loss: 1.3281 - val_loss: 1.1931\n",
      "Epoch 2/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.1480\n",
      "Epoch 00002: val_loss improved from 1.19310 to 1.13524, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.1472 - val_loss: 1.1352\n",
      "Epoch 3/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.1134\n",
      "Epoch 00003: val_loss improved from 1.13524 to 1.12120, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.1139 - val_loss: 1.1212\n",
      "Epoch 4/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0653\n",
      "Epoch 00004: val_loss improved from 1.12120 to 1.11218, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0658 - val_loss: 1.1122\n",
      "Epoch 5/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0546\n",
      "Epoch 00005: val_loss did not improve from 1.11218\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0537 - val_loss: 1.1432\n",
      "Epoch 6/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0352\n",
      "Epoch 00006: val_loss improved from 1.11218 to 1.08634, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0342 - val_loss: 1.0863\n",
      "Epoch 7/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0397\n",
      "Epoch 00007: val_loss did not improve from 1.08634\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 1.0389 - val_loss: 1.1048\n",
      "Epoch 8/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0311\n",
      "Epoch 00008: val_loss did not improve from 1.08634\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0340 - val_loss: 1.1519\n",
      "Epoch 9/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0275\n",
      "Epoch 00009: val_loss did not improve from 1.08634\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0236 - val_loss: 1.1215\n",
      "Epoch 10/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0076\n",
      "Epoch 00010: val_loss did not improve from 1.08634\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.0086 - val_loss: 1.1273\n",
      "Epoch 11/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0157\n",
      "Epoch 00011: val_loss improved from 1.08634 to 1.07887, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0160 - val_loss: 1.0789\n",
      "Epoch 12/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0022\n",
      "Epoch 00012: val_loss did not improve from 1.07887\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.0025 - val_loss: 1.0964\n",
      "Epoch 13/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9948\n",
      "Epoch 00013: val_loss did not improve from 1.07887\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 0.9959 - val_loss: 1.1467\n",
      "Epoch 14/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9907\n",
      "Epoch 00014: val_loss did not improve from 1.07887\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 0.9914 - val_loss: 1.0848\n",
      "Epoch 15/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9842\n",
      "Epoch 00015: val_loss did not improve from 1.07887\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 0.9844 - val_loss: 1.0838\n",
      "Epoch 16/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9913\n",
      "Epoch 00016: val_loss did not improve from 1.07887\n",
      "14152/14152 [==============================] - 2s 171us/sample - loss: 0.9894 - val_loss: 1.1165\n",
      "Epoch 17/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9767\n",
      "Epoch 00017: val_loss did not improve from 1.07887\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 0.9780 - val_loss: 1.1599\n",
      "Epoch 18/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9834\n",
      "Epoch 00018: val_loss did not improve from 1.07887\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 0.9834 - val_loss: 1.1176\n",
      "Epoch 19/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9671\n",
      "Epoch 00019: val_loss did not improve from 1.07887\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 0.9685 - val_loss: 1.1061\n",
      "Epoch 20/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9642\n",
      "Epoch 00020: val_loss did not improve from 1.07887\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 0.9654 - val_loss: 1.1705\n",
      "Epoch 21/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9563\n",
      "Epoch 00021: val_loss did not improve from 1.07887\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 0.9559 - val_loss: 1.0891\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Train on 14152 samples, validate on 723 samples\n",
      "Epoch 1/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.4040\n",
      "Epoch 00001: val_loss improved from inf to 1.15640, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 222us/sample - loss: 1.4033 - val_loss: 1.1564\n",
      "Epoch 2/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.1509\n",
      "Epoch 00002: val_loss improved from 1.15640 to 1.13595, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.1516 - val_loss: 1.1360\n",
      "Epoch 3/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1172\n",
      "Epoch 00003: val_loss did not improve from 1.13595\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.1174 - val_loss: 1.1488\n",
      "Epoch 4/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0875\n",
      "Epoch 00004: val_loss did not improve from 1.13595\n",
      "14152/14152 [==============================] - 2s 153us/sample - loss: 1.0887 - val_loss: 1.1523\n",
      "Epoch 5/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0811\n",
      "Epoch 00005: val_loss improved from 1.13595 to 1.11425, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 1.0791 - val_loss: 1.1143\n",
      "Epoch 6/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0630\n",
      "Epoch 00006: val_loss improved from 1.11425 to 1.11130, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0644 - val_loss: 1.1113\n",
      "Epoch 7/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0549\n",
      "Epoch 00007: val_loss did not improve from 1.11130\n",
      "14152/14152 [==============================] - 2s 153us/sample - loss: 1.0525 - val_loss: 1.1321\n",
      "Epoch 8/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0506\n",
      "Epoch 00008: val_loss improved from 1.11130 to 1.09825, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 1.0494 - val_loss: 1.0982\n",
      "Epoch 9/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0359\n",
      "Epoch 00009: val_loss did not improve from 1.09825\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 1.0347 - val_loss: 1.1313\n",
      "Epoch 10/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0351\n",
      "Epoch 00010: val_loss did not improve from 1.09825\n",
      "14152/14152 [==============================] - 2s 153us/sample - loss: 1.0342 - val_loss: 1.1091\n",
      "Epoch 11/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0293\n",
      "Epoch 00011: val_loss did not improve from 1.09825\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 1.0311 - val_loss: 1.1001\n",
      "Epoch 12/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0212\n",
      "Epoch 00012: val_loss improved from 1.09825 to 1.09775, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0231 - val_loss: 1.0977\n",
      "Epoch 13/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0239\n",
      "Epoch 00013: val_loss did not improve from 1.09775\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 1.0239 - val_loss: 1.1500\n",
      "Epoch 14/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0122\n",
      "Epoch 00014: val_loss did not improve from 1.09775\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 1.0113 - val_loss: 1.1248\n",
      "Epoch 15/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9989\n",
      "Epoch 00015: val_loss did not improve from 1.09775\n",
      "14152/14152 [==============================] - 2s 153us/sample - loss: 0.9988 - val_loss: 1.1250\n",
      "Epoch 16/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0106\n",
      "Epoch 00016: val_loss did not improve from 1.09775\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 1.0097 - val_loss: 1.1115\n",
      "Epoch 17/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9880\n",
      "Epoch 00017: val_loss did not improve from 1.09775\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 0.9880 - val_loss: 1.1060\n",
      "Epoch 18/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9873\n",
      "Epoch 00018: val_loss did not improve from 1.09775\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 0.9886 - val_loss: 1.1494\n",
      "Epoch 19/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9900\n",
      "Epoch 00019: val_loss improved from 1.09775 to 1.09451, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 0.9929 - val_loss: 1.0945\n",
      "Epoch 20/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9867\n",
      "Epoch 00020: val_loss did not improve from 1.09451\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 0.9872 - val_loss: 1.1364\n",
      "Epoch 21/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9722\n",
      "Epoch 00021: val_loss improved from 1.09451 to 1.08939, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 0.9721 - val_loss: 1.0894\n",
      "Epoch 22/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9785\n",
      "Epoch 00022: val_loss did not improve from 1.08939\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 0.9801 - val_loss: 1.1028\n",
      "Epoch 23/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9661\n",
      "Epoch 00023: val_loss did not improve from 1.08939\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 0.9690 - val_loss: 1.0951\n",
      "Epoch 24/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9596\n",
      "Epoch 00024: val_loss did not improve from 1.08939\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 0.9609 - val_loss: 1.1010\n",
      "Epoch 25/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9670\n",
      "Epoch 00025: val_loss did not improve from 1.08939\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 0.9676 - val_loss: 1.0942\n",
      "Epoch 26/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9705\n",
      "Epoch 00026: val_loss did not improve from 1.08939\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 0.9701 - val_loss: 1.1121\n",
      "Epoch 27/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9540\n",
      "Epoch 00027: val_loss did not improve from 1.08939\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 0.9569 - val_loss: 1.1387\n",
      "Epoch 28/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9634\n",
      "Epoch 00028: val_loss did not improve from 1.08939\n",
      "14152/14152 [==============================] - 2s 153us/sample - loss: 0.9649 - val_loss: 1.1345\n",
      "Epoch 29/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9612\n",
      "Epoch 00029: val_loss did not improve from 1.08939\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 0.9593 - val_loss: 1.1011\n",
      "Epoch 30/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9544\n",
      "Epoch 00030: val_loss did not improve from 1.08939\n",
      "14152/14152 [==============================] - 2s 153us/sample - loss: 0.9539 - val_loss: 1.1200\n",
      "Epoch 31/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9497\n",
      "Epoch 00031: val_loss did not improve from 1.08939\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 0.9506 - val_loss: 1.1043\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Train on 14152 samples, validate on 724 samples\n",
      "Epoch 1/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.2842\n",
      "Epoch 00001: val_loss improved from inf to 1.26964, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 219us/sample - loss: 1.2792 - val_loss: 1.2696\n",
      "Epoch 2/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.1377\n",
      "Epoch 00002: val_loss did not improve from 1.26964\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 1.1351 - val_loss: 1.5164\n",
      "Epoch 3/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0875\n",
      "Epoch 00003: val_loss improved from 1.26964 to 1.22581, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.0904 - val_loss: 1.2258\n",
      "Epoch 4/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0755\n",
      "Epoch 00004: val_loss did not improve from 1.22581\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 1.0762 - val_loss: 1.2734\n",
      "Epoch 5/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0607\n",
      "Epoch 00005: val_loss did not improve from 1.22581\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 1.0610 - val_loss: 1.2392\n",
      "Epoch 6/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0521\n",
      "Epoch 00006: val_loss improved from 1.22581 to 1.22405, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 1.0534 - val_loss: 1.2241\n",
      "Epoch 7/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0316\n",
      "Epoch 00007: val_loss improved from 1.22405 to 1.22309, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0316 - val_loss: 1.2231\n",
      "Epoch 8/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0255\n",
      "Epoch 00008: val_loss did not improve from 1.22309\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 1.0258 - val_loss: 1.2417\n",
      "Epoch 9/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0190\n",
      "Epoch 00009: val_loss did not improve from 1.22309\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 1.0185 - val_loss: 1.2599\n",
      "Epoch 10/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0203\n",
      "Epoch 00010: val_loss improved from 1.22309 to 1.21858, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0193 - val_loss: 1.2186\n",
      "Epoch 11/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0095\n",
      "Epoch 00011: val_loss did not improve from 1.21858\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0087 - val_loss: 1.2320\n",
      "Epoch 12/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0110\n",
      "Epoch 00012: val_loss did not improve from 1.21858\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 1.0077 - val_loss: 1.2713\n",
      "Epoch 13/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0043\n",
      "Epoch 00013: val_loss did not improve from 1.21858\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 1.0055 - val_loss: 1.2580\n",
      "Epoch 14/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9983\n",
      "Epoch 00014: val_loss did not improve from 1.21858\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 0.9977 - val_loss: 1.2872\n",
      "Epoch 15/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9928\n",
      "Epoch 00015: val_loss did not improve from 1.21858\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 0.9931 - val_loss: 1.2437\n",
      "Epoch 16/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9812\n",
      "Epoch 00016: val_loss did not improve from 1.21858\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 0.9819 - val_loss: 1.2285\n",
      "Epoch 17/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9828\n",
      "Epoch 00017: val_loss did not improve from 1.21858\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 0.9821 - val_loss: 1.2313\n",
      "Epoch 18/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9727\n",
      "Epoch 00018: val_loss did not improve from 1.21858\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 0.9719 - val_loss: 1.2783\n",
      "Epoch 19/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9771\n",
      "Epoch 00019: val_loss did not improve from 1.21858\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 0.9765 - val_loss: 1.2595\n",
      "Epoch 20/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9742\n",
      "Epoch 00020: val_loss did not improve from 1.21858\n",
      "14152/14152 [==============================] - 3s 179us/sample - loss: 0.9740 - val_loss: 1.2634\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Train on 14152 samples, validate on 724 samples\n",
      "Epoch 1/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.3030\n",
      "Epoch 00001: val_loss improved from inf to 1.21213, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 4s 267us/sample - loss: 1.3001 - val_loss: 1.2121\n",
      "Epoch 2/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.1229\n",
      "Epoch 00002: val_loss improved from 1.21213 to 1.18902, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 169us/sample - loss: 1.1246 - val_loss: 1.1890\n",
      "Epoch 3/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0941\n",
      "Epoch 00003: val_loss improved from 1.18902 to 1.18349, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.0944 - val_loss: 1.1835\n",
      "Epoch 4/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0694\n",
      "Epoch 00004: val_loss improved from 1.18349 to 1.16843, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0692 - val_loss: 1.1684\n",
      "Epoch 5/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0538\n",
      "Epoch 00005: val_loss did not improve from 1.16843\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 1.0539 - val_loss: 1.1981\n",
      "Epoch 6/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0411\n",
      "Epoch 00006: val_loss did not improve from 1.16843\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 1.0428 - val_loss: 1.2439\n",
      "Epoch 7/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0405\n",
      "Epoch 00007: val_loss did not improve from 1.16843\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 1.0379 - val_loss: 1.1966\n",
      "Epoch 8/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0276\n",
      "Epoch 00008: val_loss improved from 1.16843 to 1.16454, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.0262 - val_loss: 1.1645\n",
      "Epoch 9/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0164\n",
      "Epoch 00009: val_loss did not improve from 1.16454\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0147 - val_loss: 1.1717\n",
      "Epoch 10/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0135\n",
      "Epoch 00010: val_loss improved from 1.16454 to 1.15859, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0145 - val_loss: 1.1586\n",
      "Epoch 11/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0089\n",
      "Epoch 00011: val_loss did not improve from 1.15859\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 1.0085 - val_loss: 1.1590\n",
      "Epoch 12/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0048\n",
      "Epoch 00012: val_loss did not improve from 1.15859\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.0044 - val_loss: 1.1717\n",
      "Epoch 13/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9965\n",
      "Epoch 00013: val_loss did not improve from 1.15859\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 0.9960 - val_loss: 1.2453\n",
      "Epoch 14/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9922\n",
      "Epoch 00014: val_loss did not improve from 1.15859\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 0.9925 - val_loss: 1.1980\n",
      "Epoch 15/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9910\n",
      "Epoch 00015: val_loss did not improve from 1.15859\n",
      "14152/14152 [==============================] - 2s 149us/sample - loss: 0.9902 - val_loss: 1.1982\n",
      "Epoch 16/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9834\n",
      "Epoch 00016: val_loss did not improve from 1.15859\n",
      "14152/14152 [==============================] - 2s 150us/sample - loss: 0.9834 - val_loss: 1.1650\n",
      "Epoch 17/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9899\n",
      "Epoch 00017: val_loss did not improve from 1.15859\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 0.9891 - val_loss: 1.2126\n",
      "Epoch 18/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9708\n",
      "Epoch 00018: val_loss did not improve from 1.15859\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 0.9714 - val_loss: 1.2041\n",
      "Epoch 19/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9686\n",
      "Epoch 00019: val_loss did not improve from 1.15859\n",
      "14152/14152 [==============================] - 2s 151us/sample - loss: 0.9679 - val_loss: 1.1828\n",
      "Epoch 20/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9754\n",
      "Epoch 00020: val_loss did not improve from 1.15859\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 0.9750 - val_loss: 1.1953\n",
      "Our oof rmse score is: 1.0694141315594092\n",
      "Our oof cohen kappa score is: 0.5119375478054554\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Train on 14152 samples, validate on 721 samples\n",
      "Epoch 1/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.3738\n",
      "Epoch 00001: val_loss improved from inf to 1.18382, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 217us/sample - loss: 1.3736 - val_loss: 1.1838\n",
      "Epoch 2/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.1619\n",
      "Epoch 00002: val_loss improved from 1.18382 to 1.16863, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 150us/sample - loss: 1.1606 - val_loss: 1.1686\n",
      "Epoch 3/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.1093\n",
      "Epoch 00003: val_loss improved from 1.16863 to 1.14523, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 149us/sample - loss: 1.1112 - val_loss: 1.1452\n",
      "Epoch 4/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0841\n",
      "Epoch 00004: val_loss did not improve from 1.14523\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 1.0847 - val_loss: 1.1839\n",
      "Epoch 5/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0669\n",
      "Epoch 00005: val_loss improved from 1.14523 to 1.14246, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 153us/sample - loss: 1.0680 - val_loss: 1.1425\n",
      "Epoch 6/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0556\n",
      "Epoch 00006: val_loss did not improve from 1.14246\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 1.0561 - val_loss: 1.1574\n",
      "Epoch 7/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0490\n",
      "Epoch 00007: val_loss did not improve from 1.14246\n",
      "14152/14152 [==============================] - 2s 153us/sample - loss: 1.0492 - val_loss: 1.1789\n",
      "Epoch 8/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0384\n",
      "Epoch 00008: val_loss improved from 1.14246 to 1.13390, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 1.0382 - val_loss: 1.1339\n",
      "Epoch 9/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0319\n",
      "Epoch 00009: val_loss did not improve from 1.13390\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.0326 - val_loss: 1.1556\n",
      "Epoch 10/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0334\n",
      "Epoch 00010: val_loss did not improve from 1.13390\n",
      "14152/14152 [==============================] - 2s 172us/sample - loss: 1.0308 - val_loss: 1.1514\n",
      "Epoch 11/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0235\n",
      "Epoch 00011: val_loss did not improve from 1.13390\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0224 - val_loss: 1.1598\n",
      "Epoch 12/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0274\n",
      "Epoch 00012: val_loss did not improve from 1.13390\n",
      "14152/14152 [==============================] - 2s 153us/sample - loss: 1.0271 - val_loss: 1.1996\n",
      "Epoch 13/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0198\n",
      "Epoch 00013: val_loss did not improve from 1.13390\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 1.0175 - val_loss: 1.1594\n",
      "Epoch 14/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0196\n",
      "Epoch 00014: val_loss did not improve from 1.13390\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0185 - val_loss: 1.1536\n",
      "Epoch 15/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9950\n",
      "Epoch 00015: val_loss did not improve from 1.13390\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 0.9954 - val_loss: 1.1874\n",
      "Epoch 16/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0003\n",
      "Epoch 00016: val_loss did not improve from 1.13390\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 0.9990 - val_loss: 1.1435\n",
      "Epoch 17/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9940\n",
      "Epoch 00017: val_loss did not improve from 1.13390\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 0.9915 - val_loss: 1.1804\n",
      "Epoch 18/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9858\n",
      "Epoch 00018: val_loss did not improve from 1.13390\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 0.9850 - val_loss: 1.1869\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Train on 14152 samples, validate on 722 samples\n",
      "Epoch 1/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.3305\n",
      "Epoch 00001: val_loss improved from inf to 1.21735, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 221us/sample - loss: 1.3294 - val_loss: 1.2173\n",
      "Epoch 2/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.1281\n",
      "Epoch 00002: val_loss improved from 1.21735 to 1.09640, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 1.1289 - val_loss: 1.0964\n",
      "Epoch 3/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0943\n",
      "Epoch 00003: val_loss did not improve from 1.09640\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 1.0940 - val_loss: 1.1025\n",
      "Epoch 4/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0812\n",
      "Epoch 00004: val_loss did not improve from 1.09640\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 1.0814 - val_loss: 1.1098\n",
      "Epoch 5/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0511\n",
      "Epoch 00005: val_loss did not improve from 1.09640\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 1.0523 - val_loss: 1.1171\n",
      "Epoch 6/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0473\n",
      "Epoch 00006: val_loss did not improve from 1.09640\n",
      "14152/14152 [==============================] - 2s 153us/sample - loss: 1.0469 - val_loss: 1.1106\n",
      "Epoch 7/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0367\n",
      "Epoch 00007: val_loss did not improve from 1.09640\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 1.0377 - val_loss: 1.1115\n",
      "Epoch 8/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0309\n",
      "Epoch 00008: val_loss improved from 1.09640 to 1.08539, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 153us/sample - loss: 1.0298 - val_loss: 1.0854\n",
      "Epoch 9/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0152\n",
      "Epoch 00009: val_loss did not improve from 1.08539\n",
      "14152/14152 [==============================] - 2s 151us/sample - loss: 1.0144 - val_loss: 1.0988\n",
      "Epoch 10/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0192\n",
      "Epoch 00010: val_loss did not improve from 1.08539\n",
      "14152/14152 [==============================] - 2s 149us/sample - loss: 1.0171 - val_loss: 1.0899\n",
      "Epoch 11/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0055\n",
      "Epoch 00011: val_loss did not improve from 1.08539\n",
      "14152/14152 [==============================] - 2s 149us/sample - loss: 1.0036 - val_loss: 1.1102\n",
      "Epoch 12/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9981\n",
      "Epoch 00012: val_loss did not improve from 1.08539\n",
      "14152/14152 [==============================] - 2s 148us/sample - loss: 0.9992 - val_loss: 1.0891\n",
      "Epoch 13/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9988\n",
      "Epoch 00013: val_loss did not improve from 1.08539\n",
      "14152/14152 [==============================] - 2s 145us/sample - loss: 0.9989 - val_loss: 1.1139\n",
      "Epoch 14/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9961\n",
      "Epoch 00014: val_loss did not improve from 1.08539\n",
      "14152/14152 [==============================] - 2s 146us/sample - loss: 0.9965 - val_loss: 1.0870\n",
      "Epoch 15/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9979\n",
      "Epoch 00015: val_loss improved from 1.08539 to 1.06179, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 143us/sample - loss: 0.9978 - val_loss: 1.0618\n",
      "Epoch 16/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9865\n",
      "Epoch 00016: val_loss did not improve from 1.06179\n",
      "14152/14152 [==============================] - 2s 145us/sample - loss: 0.9880 - val_loss: 1.0938\n",
      "Epoch 17/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9815\n",
      "Epoch 00017: val_loss did not improve from 1.06179\n",
      "14152/14152 [==============================] - 2s 143us/sample - loss: 0.9815 - val_loss: 1.1077\n",
      "Epoch 18/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9759\n",
      "Epoch 00018: val_loss improved from 1.06179 to 1.05497, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 143us/sample - loss: 0.9763 - val_loss: 1.0550\n",
      "Epoch 19/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9765\n",
      "Epoch 00019: val_loss did not improve from 1.05497\n",
      "14152/14152 [==============================] - 2s 143us/sample - loss: 0.9768 - val_loss: 1.0588\n",
      "Epoch 20/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9729\n",
      "Epoch 00020: val_loss improved from 1.05497 to 1.04944, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 145us/sample - loss: 0.9725 - val_loss: 1.0494\n",
      "Epoch 21/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9920\n",
      "Epoch 00021: val_loss did not improve from 1.04944\n",
      "14152/14152 [==============================] - 2s 142us/sample - loss: 0.9913 - val_loss: 1.0885\n",
      "Epoch 22/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9728\n",
      "Epoch 00022: val_loss did not improve from 1.04944\n",
      "14152/14152 [==============================] - 2s 143us/sample - loss: 0.9711 - val_loss: 1.0640\n",
      "Epoch 23/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9675\n",
      "Epoch 00023: val_loss did not improve from 1.04944\n",
      "14152/14152 [==============================] - 2s 142us/sample - loss: 0.9662 - val_loss: 1.1279\n",
      "Epoch 24/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9542\n",
      "Epoch 00024: val_loss did not improve from 1.04944\n",
      "14152/14152 [==============================] - 2s 142us/sample - loss: 0.9543 - val_loss: 1.0751\n",
      "Epoch 25/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9571\n",
      "Epoch 00025: val_loss did not improve from 1.04944\n",
      "14152/14152 [==============================] - 2s 143us/sample - loss: 0.9577 - val_loss: 1.0529\n",
      "Epoch 26/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9576\n",
      "Epoch 00026: val_loss did not improve from 1.04944\n",
      "14152/14152 [==============================] - 2s 147us/sample - loss: 0.9568 - val_loss: 1.1090\n",
      "Epoch 27/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9540\n",
      "Epoch 00027: val_loss did not improve from 1.04944\n",
      "14152/14152 [==============================] - 2s 150us/sample - loss: 0.9536 - val_loss: 1.0762\n",
      "Epoch 28/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9678\n",
      "Epoch 00028: val_loss did not improve from 1.04944\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 0.9666 - val_loss: 1.0599\n",
      "Epoch 29/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9642\n",
      "Epoch 00029: val_loss did not improve from 1.04944\n",
      "14152/14152 [==============================] - 2s 150us/sample - loss: 0.9649 - val_loss: 1.0678\n",
      "Epoch 30/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9616\n",
      "Epoch 00030: val_loss did not improve from 1.04944\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 0.9610 - val_loss: 1.0590\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Train on 14152 samples, validate on 723 samples\n",
      "Epoch 1/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.3195\n",
      "Epoch 00001: val_loss improved from inf to 1.08629, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 220us/sample - loss: 1.3141 - val_loss: 1.0863\n",
      "Epoch 2/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1463\n",
      "Epoch 00002: val_loss did not improve from 1.08629\n",
      "14152/14152 [==============================] - 2s 148us/sample - loss: 1.1449 - val_loss: 1.0924\n",
      "Epoch 3/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.1102\n",
      "Epoch 00003: val_loss did not improve from 1.08629\n",
      "14152/14152 [==============================] - 2s 148us/sample - loss: 1.1132 - val_loss: 1.1591\n",
      "Epoch 4/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0879\n",
      "Epoch 00004: val_loss improved from 1.08629 to 1.07684, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 151us/sample - loss: 1.0887 - val_loss: 1.0768\n",
      "Epoch 5/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0707\n",
      "Epoch 00005: val_loss improved from 1.07684 to 1.07461, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 1.0695 - val_loss: 1.0746\n",
      "Epoch 6/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0706\n",
      "Epoch 00006: val_loss improved from 1.07461 to 1.06774, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 149us/sample - loss: 1.0687 - val_loss: 1.0677\n",
      "Epoch 7/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0567\n",
      "Epoch 00007: val_loss did not improve from 1.06774\n",
      "14152/14152 [==============================] - 2s 149us/sample - loss: 1.0571 - val_loss: 1.1304\n",
      "Epoch 8/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0446\n",
      "Epoch 00008: val_loss did not improve from 1.06774\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 1.0447 - val_loss: 1.1116\n",
      "Epoch 9/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0288\n",
      "Epoch 00009: val_loss improved from 1.06774 to 1.05847, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 1.0297 - val_loss: 1.0585\n",
      "Epoch 10/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0306\n",
      "Epoch 00010: val_loss did not improve from 1.05847\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0326 - val_loss: 1.0659\n",
      "Epoch 11/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0224\n",
      "Epoch 00011: val_loss did not improve from 1.05847\n",
      "14152/14152 [==============================] - 2s 151us/sample - loss: 1.0238 - val_loss: 1.0772\n",
      "Epoch 12/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0284\n",
      "Epoch 00012: val_loss did not improve from 1.05847\n",
      "14152/14152 [==============================] - 2s 148us/sample - loss: 1.0279 - val_loss: 1.0673\n",
      "Epoch 13/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0165\n",
      "Epoch 00013: val_loss improved from 1.05847 to 1.02988, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 149us/sample - loss: 1.0155 - val_loss: 1.0299\n",
      "Epoch 14/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0016\n",
      "Epoch 00014: val_loss did not improve from 1.02988\n",
      "14152/14152 [==============================] - 2s 148us/sample - loss: 1.0018 - val_loss: 1.0454\n",
      "Epoch 15/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9995\n",
      "Epoch 00015: val_loss did not improve from 1.02988\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 1.0003 - val_loss: 1.0620\n",
      "Epoch 16/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0088\n",
      "Epoch 00016: val_loss did not improve from 1.02988\n",
      "14152/14152 [==============================] - 2s 149us/sample - loss: 1.0085 - val_loss: 1.0451\n",
      "Epoch 17/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9964\n",
      "Epoch 00017: val_loss did not improve from 1.02988\n",
      "14152/14152 [==============================] - 2s 147us/sample - loss: 0.9950 - val_loss: 1.0455\n",
      "Epoch 18/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9958\n",
      "Epoch 00018: val_loss did not improve from 1.02988\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 0.9972 - val_loss: 1.0908\n",
      "Epoch 19/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9892\n",
      "Epoch 00019: val_loss did not improve from 1.02988\n",
      "14152/14152 [==============================] - 2s 149us/sample - loss: 0.9890 - val_loss: 1.0534\n",
      "Epoch 20/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9878\n",
      "Epoch 00020: val_loss did not improve from 1.02988\n",
      "14152/14152 [==============================] - 2s 149us/sample - loss: 0.9849 - val_loss: 1.1040\n",
      "Epoch 21/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9728\n",
      "Epoch 00021: val_loss did not improve from 1.02988\n",
      "14152/14152 [==============================] - 2s 147us/sample - loss: 0.9744 - val_loss: 1.0651\n",
      "Epoch 22/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9699\n",
      "Epoch 00022: val_loss did not improve from 1.02988\n",
      "14152/14152 [==============================] - 2s 149us/sample - loss: 0.9664 - val_loss: 1.0477\n",
      "Epoch 23/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9766\n",
      "Epoch 00023: val_loss did not improve from 1.02988\n",
      "14152/14152 [==============================] - 2s 151us/sample - loss: 0.9776 - val_loss: 1.0634\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Train on 14152 samples, validate on 724 samples\n",
      "Epoch 1/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.4258\n",
      "Epoch 00001: val_loss improved from inf to 1.31868, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 243us/sample - loss: 1.4257 - val_loss: 1.3187\n",
      "Epoch 2/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.1653\n",
      "Epoch 00002: val_loss improved from 1.31868 to 1.27959, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 1.1642 - val_loss: 1.2796\n",
      "Epoch 3/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1130\n",
      "Epoch 00003: val_loss improved from 1.27959 to 1.24662, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 151us/sample - loss: 1.1140 - val_loss: 1.2466\n",
      "Epoch 4/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0933\n",
      "Epoch 00004: val_loss improved from 1.24662 to 1.22211, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 1.0919 - val_loss: 1.2221\n",
      "Epoch 5/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0789\n",
      "Epoch 00005: val_loss did not improve from 1.22211\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 1.0771 - val_loss: 1.2495\n",
      "Epoch 6/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0565\n",
      "Epoch 00006: val_loss did not improve from 1.22211\n",
      "14152/14152 [==============================] - 2s 150us/sample - loss: 1.0564 - val_loss: 1.2248\n",
      "Epoch 7/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0426\n",
      "Epoch 00007: val_loss did not improve from 1.22211\n",
      "14152/14152 [==============================] - 2s 151us/sample - loss: 1.0423 - val_loss: 1.3124\n",
      "Epoch 8/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0362\n",
      "Epoch 00008: val_loss did not improve from 1.22211\n",
      "14152/14152 [==============================] - 2s 149us/sample - loss: 1.0353 - val_loss: 1.2535\n",
      "Epoch 9/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0247\n",
      "Epoch 00009: val_loss improved from 1.22211 to 1.18553, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 1.0262 - val_loss: 1.1855\n",
      "Epoch 10/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0274\n",
      "Epoch 00010: val_loss did not improve from 1.18553\n",
      "14152/14152 [==============================] - 2s 151us/sample - loss: 1.0275 - val_loss: 1.2190\n",
      "Epoch 11/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0168\n",
      "Epoch 00011: val_loss did not improve from 1.18553\n",
      "14152/14152 [==============================] - 2s 151us/sample - loss: 1.0139 - val_loss: 1.2158\n",
      "Epoch 12/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0093\n",
      "Epoch 00012: val_loss did not improve from 1.18553\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 1.0095 - val_loss: 1.2340\n",
      "Epoch 13/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0068\n",
      "Epoch 00013: val_loss did not improve from 1.18553\n",
      "14152/14152 [==============================] - 2s 151us/sample - loss: 1.0073 - val_loss: 1.2434\n",
      "Epoch 14/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0086\n",
      "Epoch 00014: val_loss did not improve from 1.18553\n",
      "14152/14152 [==============================] - 2s 150us/sample - loss: 1.0092 - val_loss: 1.2456\n",
      "Epoch 15/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9872\n",
      "Epoch 00015: val_loss did not improve from 1.18553\n",
      "14152/14152 [==============================] - 2s 149us/sample - loss: 0.9876 - val_loss: 1.2580\n",
      "Epoch 16/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9807\n",
      "Epoch 00016: val_loss did not improve from 1.18553\n",
      "14152/14152 [==============================] - 2s 146us/sample - loss: 0.9816 - val_loss: 1.1948\n",
      "Epoch 17/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9843\n",
      "Epoch 00017: val_loss did not improve from 1.18553\n",
      "14152/14152 [==============================] - 2s 147us/sample - loss: 0.9836 - val_loss: 1.2768\n",
      "Epoch 18/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9763\n",
      "Epoch 00018: val_loss did not improve from 1.18553\n",
      "14152/14152 [==============================] - 2s 150us/sample - loss: 0.9756 - val_loss: 1.2637\n",
      "Epoch 19/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9779\n",
      "Epoch 00019: val_loss did not improve from 1.18553\n",
      "14152/14152 [==============================] - 2s 148us/sample - loss: 0.9762 - val_loss: 1.2324\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Train on 14152 samples, validate on 724 samples\n",
      "Epoch 1/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.2952\n",
      "Epoch 00001: val_loss improved from inf to 1.28155, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 215us/sample - loss: 1.2940 - val_loss: 1.2815\n",
      "Epoch 2/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1298\n",
      "Epoch 00002: val_loss improved from 1.28155 to 1.23716, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 1.1294 - val_loss: 1.2372\n",
      "Epoch 3/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.1031\n",
      "Epoch 00003: val_loss improved from 1.23716 to 1.19028, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 153us/sample - loss: 1.1016 - val_loss: 1.1903\n",
      "Epoch 4/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0587\n",
      "Epoch 00004: val_loss did not improve from 1.19028\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 1.0572 - val_loss: 1.2229\n",
      "Epoch 5/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0501\n",
      "Epoch 00005: val_loss did not improve from 1.19028\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 1.0500 - val_loss: 1.2357\n",
      "Epoch 6/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0356\n",
      "Epoch 00006: val_loss did not improve from 1.19028\n",
      "14152/14152 [==============================] - 2s 175us/sample - loss: 1.0388 - val_loss: 1.2186\n",
      "Epoch 7/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0385\n",
      "Epoch 00007: val_loss did not improve from 1.19028\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 1.0388 - val_loss: 1.2493\n",
      "Epoch 8/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0321\n",
      "Epoch 00008: val_loss did not improve from 1.19028\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 1.0329 - val_loss: 1.2300\n",
      "Epoch 9/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0243\n",
      "Epoch 00009: val_loss did not improve from 1.19028\n",
      "14152/14152 [==============================] - 2s 145us/sample - loss: 1.0225 - val_loss: 1.1964\n",
      "Epoch 10/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0162\n",
      "Epoch 00010: val_loss did not improve from 1.19028\n",
      "14152/14152 [==============================] - 2s 147us/sample - loss: 1.0147 - val_loss: 1.2149\n",
      "Epoch 11/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0073\n",
      "Epoch 00011: val_loss did not improve from 1.19028\n",
      "14152/14152 [==============================] - 2s 149us/sample - loss: 1.0118 - val_loss: 1.2329\n",
      "Epoch 12/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0093\n",
      "Epoch 00012: val_loss did not improve from 1.19028\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 1.0081 - val_loss: 1.1995\n",
      "Epoch 13/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0010\n",
      "Epoch 00013: val_loss did not improve from 1.19028\n",
      "14152/14152 [==============================] - 2s 147us/sample - loss: 1.0005 - val_loss: 1.2321\n",
      "Our oof rmse score is: 1.0572868426037023\n",
      "Our oof cohen kappa score is: 0.5387848331790854\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Train on 14152 samples, validate on 721 samples\n",
      "Epoch 1/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.3695\n",
      "Epoch 00001: val_loss improved from inf to 1.19248, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 211us/sample - loss: 1.3677 - val_loss: 1.1925\n",
      "Epoch 2/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.1467\n",
      "Epoch 00002: val_loss improved from 1.19248 to 1.17743, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 1.1455 - val_loss: 1.1774\n",
      "Epoch 3/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.1189\n",
      "Epoch 00003: val_loss improved from 1.17743 to 1.16871, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 1.1190 - val_loss: 1.1687\n",
      "Epoch 4/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0994\n",
      "Epoch 00004: val_loss did not improve from 1.16871\n",
      "14152/14152 [==============================] - 2s 147us/sample - loss: 1.1010 - val_loss: 1.1744\n",
      "Epoch 5/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0724\n",
      "Epoch 00005: val_loss did not improve from 1.16871\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 1.0717 - val_loss: 1.1978\n",
      "Epoch 6/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0629\n",
      "Epoch 00006: val_loss did not improve from 1.16871\n",
      "14152/14152 [==============================] - 2s 151us/sample - loss: 1.0630 - val_loss: 1.1760\n",
      "Epoch 7/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0474\n",
      "Epoch 00007: val_loss improved from 1.16871 to 1.16088, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 153us/sample - loss: 1.0457 - val_loss: 1.1609\n",
      "Epoch 8/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0516\n",
      "Epoch 00008: val_loss did not improve from 1.16088\n",
      "14152/14152 [==============================] - 2s 148us/sample - loss: 1.0474 - val_loss: 1.1911\n",
      "Epoch 9/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0410\n",
      "Epoch 00009: val_loss did not improve from 1.16088\n",
      "14152/14152 [==============================] - 2s 146us/sample - loss: 1.0407 - val_loss: 1.2373\n",
      "Epoch 10/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0275\n",
      "Epoch 00010: val_loss did not improve from 1.16088\n",
      "14152/14152 [==============================] - 2s 150us/sample - loss: 1.0272 - val_loss: 1.2144\n",
      "Epoch 11/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0256\n",
      "Epoch 00011: val_loss did not improve from 1.16088\n",
      "14152/14152 [==============================] - 2s 168us/sample - loss: 1.0246 - val_loss: 1.1775\n",
      "Epoch 12/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0244\n",
      "Epoch 00012: val_loss did not improve from 1.16088\n",
      "14152/14152 [==============================] - 2s 175us/sample - loss: 1.0243 - val_loss: 1.1967\n",
      "Epoch 13/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0095\n",
      "Epoch 00013: val_loss improved from 1.16088 to 1.15497, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 174us/sample - loss: 1.0109 - val_loss: 1.1550\n",
      "Epoch 14/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0096\n",
      "Epoch 00014: val_loss did not improve from 1.15497\n",
      "14152/14152 [==============================] - 2s 173us/sample - loss: 1.0099 - val_loss: 1.1645\n",
      "Epoch 15/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9952\n",
      "Epoch 00015: val_loss did not improve from 1.15497\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 0.9928 - val_loss: 1.1939\n",
      "Epoch 16/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9986\n",
      "Epoch 00016: val_loss did not improve from 1.15497\n",
      "14152/14152 [==============================] - 2s 149us/sample - loss: 0.9979 - val_loss: 1.1840\n",
      "Epoch 17/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9925\n",
      "Epoch 00017: val_loss did not improve from 1.15497\n",
      "14152/14152 [==============================] - 2s 148us/sample - loss: 0.9915 - val_loss: 1.1996\n",
      "Epoch 18/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9793\n",
      "Epoch 00018: val_loss did not improve from 1.15497\n",
      "14152/14152 [==============================] - 2s 150us/sample - loss: 0.9790 - val_loss: 1.1602\n",
      "Epoch 19/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9928\n",
      "Epoch 00019: val_loss did not improve from 1.15497\n",
      "14152/14152 [==============================] - 2s 146us/sample - loss: 0.9928 - val_loss: 1.1793\n",
      "Epoch 20/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9770\n",
      "Epoch 00020: val_loss did not improve from 1.15497\n",
      "14152/14152 [==============================] - 2s 151us/sample - loss: 0.9770 - val_loss: 1.2184\n",
      "Epoch 21/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9749\n",
      "Epoch 00021: val_loss did not improve from 1.15497\n",
      "14152/14152 [==============================] - 2s 146us/sample - loss: 0.9731 - val_loss: 1.1984\n",
      "Epoch 22/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9814\n",
      "Epoch 00022: val_loss did not improve from 1.15497\n",
      "14152/14152 [==============================] - 2s 148us/sample - loss: 0.9828 - val_loss: 1.1800\n",
      "Epoch 23/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9791\n",
      "Epoch 00023: val_loss did not improve from 1.15497\n",
      "14152/14152 [==============================] - 2s 149us/sample - loss: 0.9764 - val_loss: 1.2625\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Train on 14152 samples, validate on 722 samples\n",
      "Epoch 1/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.3105\n",
      "Epoch 00001: val_loss improved from inf to 1.17888, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 215us/sample - loss: 1.3107 - val_loss: 1.1789\n",
      "Epoch 2/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.1348\n",
      "Epoch 00002: val_loss improved from 1.17888 to 1.15052, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 147us/sample - loss: 1.1347 - val_loss: 1.1505\n",
      "Epoch 3/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1028\n",
      "Epoch 00003: val_loss improved from 1.15052 to 1.12784, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 148us/sample - loss: 1.1061 - val_loss: 1.1278\n",
      "Epoch 4/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0745\n",
      "Epoch 00004: val_loss did not improve from 1.12784\n",
      "14152/14152 [==============================] - 2s 144us/sample - loss: 1.0764 - val_loss: 1.1535\n",
      "Epoch 5/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0579\n",
      "Epoch 00005: val_loss did not improve from 1.12784\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 1.0586 - val_loss: 1.1522\n",
      "Epoch 6/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0469\n",
      "Epoch 00006: val_loss did not improve from 1.12784\n",
      "14152/14152 [==============================] - 2s 149us/sample - loss: 1.0483 - val_loss: 1.1521\n",
      "Epoch 7/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0391\n",
      "Epoch 00007: val_loss did not improve from 1.12784\n",
      "14152/14152 [==============================] - 2s 147us/sample - loss: 1.0388 - val_loss: 1.1701\n",
      "Epoch 8/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0362\n",
      "Epoch 00008: val_loss improved from 1.12784 to 1.12775, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 148us/sample - loss: 1.0355 - val_loss: 1.1277\n",
      "Epoch 9/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0242\n",
      "Epoch 00009: val_loss did not improve from 1.12775\n",
      "14152/14152 [==============================] - 2s 147us/sample - loss: 1.0215 - val_loss: 1.1459\n",
      "Epoch 10/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0207\n",
      "Epoch 00010: val_loss did not improve from 1.12775\n",
      "14152/14152 [==============================] - 2s 153us/sample - loss: 1.0205 - val_loss: 1.1370\n",
      "Epoch 11/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0150\n",
      "Epoch 00011: val_loss did not improve from 1.12775\n",
      "14152/14152 [==============================] - 2s 151us/sample - loss: 1.0140 - val_loss: 1.1295\n",
      "Epoch 12/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0077\n",
      "Epoch 00012: val_loss did not improve from 1.12775\n",
      "14152/14152 [==============================] - 2s 153us/sample - loss: 1.0086 - val_loss: 1.1344\n",
      "Epoch 13/100\n",
      "   32/14152 [..............................] - ETA: 2s - loss: 1.1412"
     ]
    }
   ],
   "source": [
    "for i in train_std.columns:\n",
    "    if \"session_title\" in str(i):\n",
    "        nn_features.append(i)\n",
    "y_pred_1_nn, oof_rmse_score_1_nn, oof_cohen_score_1_nn = run_nn(train_std, test_std, nn_features)\n",
    "y_pred_2_nn, oof_rmse_score_2_nn, oof_cohen_score_2_nn = run_nn(train_std, test_std, nn_features)\n",
    "y_pred_3_nn, oof_rmse_score_3_nn, oof_cohen_score_3_nn = run_nn(train_std, test_std, nn_features)\n",
    "y_pred_4_nn, oof_rmse_score_4_nn, oof_cohen_score_4_nn = run_nn(train_std, test_std, nn_features)\n",
    "y_pred_5_nn, oof_rmse_score_5_nn, oof_cohen_score_5_nn = run_nn(train_std, test_std, nn_features)\n",
    "mean_rmse_nn = (oof_rmse_score_1_nn + oof_rmse_score_2_nn + oof_rmse_score_3_nn + oof_rmse_score_4_nn + oof_rmse_score_5_nn) / 5\n",
    "mean_cohen_kappa_nn = (oof_cohen_score_1_nn + oof_cohen_score_2_nn + oof_cohen_score_3_nn + oof_cohen_score_4_nn + oof_cohen_score_5_nn) / 5\n",
    "print('Our mean rmse score is: ', mean_rmse_nn)\n",
    "print('Our mean cohen kappa score is: ', mean_cohen_kappa_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    0.495\n",
      "0    0.239\n",
      "1    0.136\n",
      "2    0.130\n",
      "Name: accuracy_group, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "y_final_lgb = (y_pred_1 + y_pred_2 + y_pred_3 + y_pred_4 + y_pred_5) / 5\n",
    "y_final_lr = (y_pred_1_lr + y_pred_2_lr + y_pred_3_lr + y_pred_4_lr + y_pred_5_lr) / 5\n",
    "y_final_nn = (y_pred_1_nn + y_pred_2_nn + y_pred_3_nn + y_pred_4_nn + y_pred_5_nn) / 5\n",
    "y_final = y_final_lgb * 0.6 + y_final_nn * 0.3 + y_final_lr * 0.1\n",
    "y_final = eval_qwk_lgb_regr(y_final, reduce_train)\n",
    "predict(sample_submission, y_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0410a24260cc4109952f20d4bb62237f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6b863ddbb7414d42abdd6a5eaac24ba7",
        "IPY_MODEL_7910c758c0944a5ca52b6b7f06f3d982"
       ],
       "layout": "IPY_MODEL_3bf335077cd944e18689404de6f07f95"
      }
     },
     "236941409cc84758abc2ae21829ab4b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3bf335077cd944e18689404de6f07f95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "43671947342c4b098ef442b60d90ad3d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "47ca820dc8ed47da8a792bd11ed832de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "4b99f0e0830b454597345611488b4294": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4fafcc84fef04f8d9a462cb4b5a87379": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_efae5627dac046df9aee094568ff0476",
       "max": 17000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e8662155d8fc4ecc9e5c414e685774f2",
       "value": 17000
      }
     },
     "504994fe40dc44a38a3e3d63e5acbde1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4fafcc84fef04f8d9a462cb4b5a87379",
        "IPY_MODEL_60adc20847744c34abe29ce0c4647aea"
       ],
       "layout": "IPY_MODEL_236941409cc84758abc2ae21829ab4b5"
      }
     },
     "57b23460d7ff48298bfe6412feedb302": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "60adc20847744c34abe29ce0c4647aea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7eb53ed371cb4b43b4ac3ef808b98037",
       "placeholder": "",
       "style": "IPY_MODEL_66a290e3a75c44119bb611900527fc89",
       "value": " 17000/17000 [12:34&lt;00:00, 22.54it/s]"
      }
     },
     "66a290e3a75c44119bb611900527fc89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6b863ddbb7414d42abdd6a5eaac24ba7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c2c754e6689447b2a8d6f4362f1483c4",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_47ca820dc8ed47da8a792bd11ed832de",
       "value": 1000
      }
     },
     "720e3e8cfc06410ea75333600cdbc56b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7910c758c0944a5ca52b6b7f06f3d982": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_720e3e8cfc06410ea75333600cdbc56b",
       "placeholder": "",
       "style": "IPY_MODEL_4b99f0e0830b454597345611488b4294",
       "value": " 1000/1000 [12:30&lt;00:00,  1.33it/s]"
      }
     },
     "79436f5ce40a4a0fa12bd960c01a4e62": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7eb53ed371cb4b43b4ac3ef808b98037": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9289056b2e14466081aa09972b6544e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c2dfd19463cc4bbbac0bc160124728c1",
        "IPY_MODEL_eb805e47c4894d5f86851b5d2dfa9b98"
       ],
       "layout": "IPY_MODEL_966da1bc35cf42f89fde9448c39f628b"
      }
     },
     "966da1bc35cf42f89fde9448c39f628b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b03f983c820b4411a712a2b693e90173": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c2c754e6689447b2a8d6f4362f1483c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c2dfd19463cc4bbbac0bc160124728c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_43671947342c4b098ef442b60d90ad3d",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_57b23460d7ff48298bfe6412feedb302",
       "value": 1000
      }
     },
     "e8662155d8fc4ecc9e5c414e685774f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "eb805e47c4894d5f86851b5d2dfa9b98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_79436f5ce40a4a0fa12bd960c01a4e62",
       "placeholder": "",
       "style": "IPY_MODEL_b03f983c820b4411a712a2b693e90173",
       "value": " 1000/1000 [01:54&lt;00:00,  8.71it/s]"
      }
     },
     "efae5627dac046df9aee094568ff0476": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
