{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- improve linear model and nn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "import json\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    print('Reading train.csv file....')\n",
    "    train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\n",
    "\n",
    "    print('Reading test.csv file....')\n",
    "    test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n",
    "\n",
    "    print('Reading train_labels.csv file....')\n",
    "    train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\n",
    "\n",
    "    print('Reading specs.csv file....')\n",
    "    specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n",
    "\n",
    "    print('Reading sample_submission.csv file....')\n",
    "    sample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n",
    "    return train, test, train_labels, specs, sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_title(train, test, train_labels):\n",
    "    # encode title\n",
    "    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n",
    "    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n",
    "    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n",
    "    # make a list with all the unique 'titles' from the train and test set\n",
    "    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n",
    "    # make a list with all the unique 'event_code' from the train and test set\n",
    "    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n",
    "    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n",
    "    # make a list with all the unique worlds from the train and test set\n",
    "    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n",
    "    # create a dictionary numerating the titles\n",
    "    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n",
    "    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n",
    "    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n",
    "    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n",
    "    # replace the text titles with the number titles from the dict\n",
    "    train['title'] = train['title'].map(activities_map)\n",
    "    test['title'] = test['title'].map(activities_map)\n",
    "    train['world'] = train['world'].map(activities_world)\n",
    "    test['world'] = test['world'].map(activities_world)\n",
    "    train_labels['title'] = train_labels['title'].map(activities_map)\n",
    "    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n",
    "    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n",
    "    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n",
    "    # convert text into datetime\n",
    "    train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "    test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "    train[\"misses\"] = train[\"event_data\"].apply(lambda x: json.loads(x)[\"misses\"] if \"\\\"misses\\\"\" in x else np.nan)\n",
    "    test[\"misses\"] = test[\"event_data\"].apply(lambda x: json.loads(x)[\"misses\"] if \"\\\"misses\\\"\" in x else np.nan)\n",
    "    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code, activities_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(user_sample, test_set=False):\n",
    "    '''\n",
    "    The user_sample is a DataFrame from train or test where the only one \n",
    "    installation_id is filtered\n",
    "    And the test_set parameter is related with the labels processing, that is only requered\n",
    "    if test_set=False\n",
    "    '''\n",
    "    # Constants and parameters declaration\n",
    "    last_activity = 0\n",
    "    \n",
    "    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "    \n",
    "    # new features: time spent in each activity\n",
    "    last_session_time_sec = 0\n",
    "    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n",
    "    all_assessments = []\n",
    "    accumulated_accuracy_group = 0\n",
    "    accumulated_accuracy = 0\n",
    "    accumulated_correct_attempts = 0 \n",
    "    accumulated_uncorrect_attempts = 0\n",
    "    accumulated_actions = 0\n",
    "    counter = 0\n",
    "    time_first_activity = float(user_sample['timestamp'].values[0])\n",
    "    durations = []\n",
    "    durations_game = []\n",
    "    durations_activity = []\n",
    "    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n",
    "    last_game_time_title = {'lgt_' + title: 0 for title in assess_titles}\n",
    "    ac_game_time_title = {'agt_' + title: 0 for title in assess_titles}\n",
    "    ac_true_attempts_title = {'ata_' + title: 0 for title in assess_titles}\n",
    "    ac_false_attempts_title = {'afa_' + title: 0 for title in assess_titles}\n",
    "    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n",
    "    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n",
    "    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n",
    "    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n",
    "    session_count = 0\n",
    "    miss = 0\n",
    "    crys_game_true = 0; crys_game_false = 0\n",
    "    tree_game_true = 0; tree_game_false = 0\n",
    "    magma_game_true = 0; magma_game_false = 0\n",
    "    crys_game_acc = []; tree_game_acc = []; magma_game_acc = []\n",
    "    crys_act_true = 0; crys_act_false = 0\n",
    "    tree_act_true = 0; tree_act_false = 0\n",
    "    magma_act_true = 0; magma_act_false = 0\n",
    "    crys_act_acc = []; tree_act_acc = []; magma_act_acc = []\n",
    "    \n",
    "    # itarates through each session of one instalation_id\n",
    "    for i, session in user_sample.groupby('game_session', sort=False):\n",
    "        # i = game_session_id\n",
    "        # session is a DataFrame that contain only one game_session\n",
    "        \n",
    "        # get some sessions information\n",
    "        session_type = session['type'].iloc[0]\n",
    "        session_title = session['title'].iloc[0]\n",
    "        session_title_text = activities_labels[session_title]  \n",
    "        session_world = session[\"world\"].iloc[0]\n",
    "            \n",
    "        # for each assessment, and only this kind off session, the features below are processed\n",
    "        # and a register are generated      \n",
    "        \n",
    "        if (session_type == 'Assessment') & (test_set or len(session)>1):\n",
    "            # search for event_code 4100, that represents the assessments trial\n",
    "            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n",
    "            # then, check the numbers of wins and the number of losses\n",
    "            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n",
    "            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n",
    "            # copy a dict to use as feature template, it's initialized with some itens: \n",
    "            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "            features = user_activities_count.copy()\n",
    "            features.update(last_accuracy_title.copy())\n",
    "            features.update(event_code_count.copy())\n",
    "            features.update(title_count.copy())\n",
    "            features.update(event_id_count.copy())\n",
    "            features.update(title_event_code_count.copy())\n",
    "            features.update(last_game_time_title.copy())\n",
    "            features.update(ac_game_time_title.copy())\n",
    "            features.update(ac_true_attempts_title.copy())\n",
    "            features.update(ac_false_attempts_title.copy())\n",
    "            features['installation_session_count'] = session_count\n",
    "            \n",
    "            variety_features = [('var_event_code', event_code_count), \n",
    "                                ('var_event_id', event_id_count), \n",
    "                                ('var_title', title_count), \n",
    "                                ('var_title_event_code', title_event_code_count)]\n",
    "            \n",
    "            for name, dict_counts in variety_features:\n",
    "                arr = np.array(list(dict_counts.values()))\n",
    "                features[name] = np.count_nonzero(arr)\n",
    "                \n",
    "            # get installation_id for aggregated features\n",
    "            features['installation_id'] = session['installation_id'].iloc[-1]\n",
    "            # add title as feature, remembering that title represents the name of the game\n",
    "            features['session_title'] = session['title'].iloc[0]\n",
    "            # the 4 lines below add the feature of the history of the trials of this player\n",
    "            # this is based on the all time attempts so far, at the moment of this assessment\n",
    "            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n",
    "            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n",
    "            accumulated_correct_attempts += true_attempts \n",
    "            accumulated_uncorrect_attempts += false_attempts\n",
    "            \n",
    "            # ----------------------------------------------\n",
    "            ac_true_attempts_title['ata_' + session_title_text] += true_attempts\n",
    "            ac_false_attempts_title['afa_' + session_title_text] += false_attempts\n",
    "            \n",
    "            \n",
    "            last_game_time_title['lgt_' + session_title_text] = session['game_time'].iloc[-1]\n",
    "            ac_game_time_title['agt_' + session_title_text] += session['game_time'].iloc[-1]\n",
    "            # ----------------------------------------------\n",
    "            \n",
    "            # the time spent in the app so far\n",
    "            if durations == []:\n",
    "                features['duration_mean'] = 0\n",
    "                features['duration_std'] = 0\n",
    "                features['last_duration'] = 0\n",
    "                features['duration_max'] = 0\n",
    "            else:\n",
    "                features['duration_mean'] = np.mean(durations)\n",
    "                features['duration_std'] = np.std(durations)\n",
    "                features['last_duration'] = durations[-1]\n",
    "                features['duration_max'] = np.max(durations)\n",
    "            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            \n",
    "            if durations_game == []:\n",
    "                features['duration_game_mean'] = 0\n",
    "                features['duration_game_std'] = 0\n",
    "                features['game_last_duration'] = 0\n",
    "                features['game_max_duration'] = 0\n",
    "            else:\n",
    "                features['duration_game_mean'] = np.mean(durations_game)\n",
    "                features['duration_game_std'] = np.std(durations_game)\n",
    "                features['game_last_duration'] = durations_game[-1]\n",
    "                features['game_max_duration'] = np.max(durations_game)\n",
    "                \n",
    "            if durations_activity == []:\n",
    "                features['duration_activity_mean'] = 0\n",
    "                features['duration_activity_std'] = 0\n",
    "                features['game_activity_duration'] = 0\n",
    "                features['game_activity_max'] = 0\n",
    "            else:\n",
    "                features['duration_activity_mean'] = np.mean(durations_activity)\n",
    "                features['duration_activity_std'] = np.std(durations_activity)\n",
    "                features['game_activity_duration'] = durations_activity[-1]\n",
    "                features['game_activity_max'] = np.max(durations_activity)\n",
    "                \n",
    "            features[\"misses\"] = miss\n",
    "            if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                features[\"game_true\"] = crys_game_true\n",
    "                features[\"game_false\"] = crys_game_false\n",
    "                features['game_accuracy'] = crys_game_true / (crys_game_true + crys_game_false) if (crys_game_true + crys_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(crys_game_acc) if len(crys_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = crys_game_acc[-1] if len(crys_game_acc) >=1 else 0\n",
    "                features[\"act_true\"] = crys_act_true\n",
    "                features[\"act_false\"] = crys_act_false\n",
    "                features['act_accuracy'] = crys_act_true / (crys_act_true + crys_act_false) if (crys_act_true + crys_act_false) != 0 else 0\n",
    "                features[\"act_accuracy_std\"] = np.std(crys_act_acc) if len(crys_act_acc) >=1 else 0\n",
    "                features[\"last_act_acc\"] = crys_act_acc[-1] if len(crys_act_acc) >=1 else 0\n",
    "            elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                features[\"game_true\"] = tree_game_true\n",
    "                features[\"game_false\"] = tree_game_false\n",
    "                features['game_accuracy'] = tree_game_true / (tree_game_true + tree_game_false) if (tree_game_true + tree_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(tree_game_acc) if len(tree_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = tree_game_acc[-1] if len(tree_game_acc) >=1 else 0\n",
    "                features[\"act_true\"] = tree_act_true\n",
    "                features[\"act_false\"] = tree_act_false\n",
    "                features['act_accuracy'] = tree_act_true / (tree_act_true + tree_act_false) if (tree_act_true + tree_act_false) != 0 else 0\n",
    "                features[\"act_accuracy_std\"] = np.std(tree_act_acc) if len(tree_act_acc) >=1 else 0\n",
    "                features[\"last_act_acc\"] = tree_act_acc[-1] if len(tree_act_acc) >=1 else 0\n",
    "            elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                features[\"game_true\"] = magma_game_true\n",
    "                features[\"game_false\"] = magma_game_false\n",
    "                features['game_accuracy'] = magma_game_true / (magma_game_true + magma_game_false) if (magma_game_true + magma_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(magma_game_acc) if len(magma_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = magma_game_acc[-1] if len(magma_game_acc) >=1 else 0\n",
    "                features[\"act_true\"] = magma_act_true\n",
    "                features[\"act_false\"] = magma_act_false\n",
    "                features['act_accuracy'] = magma_act_true / (magma_act_true + magma_act_false) if (magma_act_true + magma_act_false) != 0 else 0\n",
    "                features[\"act_accuracy_std\"] = np.std(magma_act_acc) if len(magma_act_acc) >=1 else 0\n",
    "                features[\"last_act_acc\"] = magma_act_acc[-1] if len(magma_act_acc) >=1 else 0\n",
    "            \n",
    "            # the accuracy is the all time wins divided by the all time attempts\n",
    "            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n",
    "            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n",
    "            accumulated_accuracy += accuracy\n",
    "            last_accuracy_title['acc_' + session_title_text] = accuracy\n",
    "            # a feature of the current accuracy categorized\n",
    "            # it is a counter of how many times this player was in each accuracy group\n",
    "            if accuracy == 0:\n",
    "                features['accuracy_group'] = 0\n",
    "            elif accuracy == 1:\n",
    "                features['accuracy_group'] = 3\n",
    "            elif accuracy == 0.5:\n",
    "                features['accuracy_group'] = 2\n",
    "            else:\n",
    "                features['accuracy_group'] = 1\n",
    "            features.update(accuracy_groups)\n",
    "            accuracy_groups[features['accuracy_group']] += 1\n",
    "            # mean of the all accuracy groups of this player\n",
    "            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n",
    "            accumulated_accuracy_group += features['accuracy_group']\n",
    "            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n",
    "            features['accumulated_actions'] = accumulated_actions\n",
    "            \n",
    "            # there are some conditions to allow this features to be inserted in the datasets\n",
    "            # if it's a test set, all sessions belong to the final dataset\n",
    "            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n",
    "            # that means, must exist an event_code 4100 or 4110\n",
    "            if test_set:\n",
    "                all_assessments.append(features)\n",
    "            elif true_attempts+false_attempts > 0:\n",
    "                all_assessments.append(features)\n",
    "                \n",
    "            counter += 1\n",
    "            \n",
    "        if session_type == 'Game':\n",
    "            durations_game.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            true = session['event_data'].str.contains('true').sum()\n",
    "            false = session['event_data'].str.contains('false').sum() \n",
    "            durations_game.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                crys_game_true += true\n",
    "                crys_game_false += false\n",
    "                crys_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "            elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                tree_game_true += true\n",
    "                tree_game_false += false\n",
    "                tree_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "            elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                magma_game_true += true\n",
    "                magma_game_false += false\n",
    "                magma_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        if session_type == 'Activity':\n",
    "            durations_activity.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            true = session['event_data'].str.contains('true').sum()\n",
    "            false = session['event_data'].str.contains('false').sum() \n",
    "            durations_activity.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                crys_act_true += true\n",
    "                crys_act_false += false\n",
    "                crys_act_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "            elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                tree_act_true += true\n",
    "                tree_act_false += false\n",
    "                tree_act_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "            elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                magma_act_true += true\n",
    "                magma_act_false += false\n",
    "                magma_act_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "            else:\n",
    "                pass    \n",
    "        \n",
    "        session_count += 1\n",
    "        # this piece counts how many actions was made in each event_code so far\n",
    "        def update_counters(counter: dict, col: str):\n",
    "                num_of_session_count = Counter(session[col])\n",
    "                for k in num_of_session_count.keys():\n",
    "                    x = k\n",
    "                    if col == 'title':\n",
    "                        x = activities_labels[k]\n",
    "                    counter[x] += num_of_session_count[k]\n",
    "                return counter\n",
    "            \n",
    "        event_code_count = update_counters(event_code_count, \"event_code\")\n",
    "        event_id_count = update_counters(event_id_count, \"event_id\")\n",
    "        title_count = update_counters(title_count, 'title')\n",
    "        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n",
    "\n",
    "        # counts how many actions the player has done so far, used in the feature of the same name\n",
    "        accumulated_actions += len(session)\n",
    "        if last_activity != session_type:\n",
    "            user_activities_count[session_type] += 1\n",
    "            last_activitiy = session_type \n",
    "                        \n",
    "    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n",
    "    if test_set:\n",
    "        return all_assessments[-1]\n",
    "    # in the train_set, all assessments goes to the dataset\n",
    "    return all_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test(train, test):\n",
    "    compiled_train = []\n",
    "    compiled_test = []\n",
    "    compiled_test_his = []\n",
    "    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total = 17000):\n",
    "        compiled_train += get_data(user_sample)\n",
    "    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n",
    "        test_data = get_data(user_sample, test_set = True)\n",
    "        compiled_test.append(test_data)\n",
    "    for i, (ins_id, user_sample) in tqdm(enumerate(test.groupby('installation_id', sort = False)), total = 1000):\n",
    "        compiled_test_his += get_data(user_sample)\n",
    "    reduce_train = pd.DataFrame(compiled_train)\n",
    "    reduce_test = pd.DataFrame(compiled_test)\n",
    "    reduce_test_his = pd.DataFrame(compiled_test_his)\n",
    "    \n",
    "    return reduce_train, reduce_test, reduce_test_his"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thank to Bruno\n",
    "def eval_qwk_lgb_regr(y_pred, train_t):\n",
    "    \"\"\"\n",
    "    Fast cappa eval function for lgb.\n",
    "    \"\"\"\n",
    "    dist = Counter(train_t['accuracy_group'])\n",
    "    for k in dist:\n",
    "        dist[k] /= len(train_t)\n",
    "    \n",
    "    acum = 0\n",
    "    bound = {}\n",
    "    for i in range(3):\n",
    "        acum += dist[i]\n",
    "        bound[i] = np.percentile(y_pred, acum * 100)\n",
    "\n",
    "    def classify(x):\n",
    "        if x <= bound[0]:\n",
    "            return 0\n",
    "        elif x <= bound[1]:\n",
    "            return 1\n",
    "        elif x <= bound[2]:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    y_pred = np.array(list(map(classify, y_pred)))\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def predict(sample_submission, y_pred):\n",
    "    sample_submission['accuracy_group'] = y_pred\n",
    "    sample_submission['accuracy_group'] = sample_submission['accuracy_group'].astype(int)\n",
    "    sample_submission.to_csv('submission.csv', index = False)\n",
    "    print(sample_submission['accuracy_group'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_assessment(reduce_train):\n",
    "    used_idx = []\n",
    "    for iid in set(reduce_train['installation_id']):\n",
    "        list_ = list(reduce_train[reduce_train['installation_id']==iid].index)\n",
    "        cur = random.choices(list_, k = 1)[0]\n",
    "        used_idx.append(cur)\n",
    "    reduce_train_t = reduce_train.loc[used_idx]\n",
    "    print(\"used validation data: \", len(used_idx))\n",
    "    return reduce_train_t, used_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each validation fold extract one random observation for each installation id to simulate the test set\n",
    "def run_lgb(reduce_train, reduce_test, features):\n",
    "    # features found in initial bayesian optimization\n",
    "    params = {'boosting_type': 'gbdt', \n",
    "              'metric': 'rmse', \n",
    "              'objective': 'regression', \n",
    "              'eval_metric': 'cappa', \n",
    "              'n_jobs': -1, \n",
    "              'seed': 42, \n",
    "              'num_leaves': 26, \n",
    "              'learning_rate': 0.077439684887749, \n",
    "              'max_depth': 33, \n",
    "              'lambda_l1': 3.27791989030057, \n",
    "              'lambda_l2': 1.3047627805931334, \n",
    "              'bagging_fraction': 0.896924978584253, \n",
    "              'bagging_freq': 1, \n",
    "              'colsample_bytree': 0.8710772167017853}\n",
    "\n",
    "    kf = GroupKFold(n_splits = 5)\n",
    "    target = 'accuracy_group'\n",
    "    oof_pred = np.zeros(len(reduce_train))\n",
    "    y_pred = np.zeros(len(reduce_test))\n",
    "    ind = []\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train, groups = reduce_train['installation_id'])):\n",
    "        print('Fold:', fold + 1)\n",
    "        x_train, x_val = reduce_train[features].iloc[tr_ind], reduce_train[features].iloc[val_ind]\n",
    "        y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n",
    "        x_train.drop('installation_id', inplace = True, axis = 1)\n",
    "        train_set = lgb.Dataset(x_train, y_train, categorical_feature = ['session_title'])\n",
    "\n",
    "\n",
    "        x_val, idx_val = get_random_assessment(x_val)\n",
    "        ind.extend(idx_val)\n",
    "        x_val.drop('installation_id', inplace = True, axis = 1)\n",
    "        y_val = y_val.loc[idx_val]\n",
    "        val_set = lgb.Dataset(x_val, y_val, categorical_feature = ['session_title'])\n",
    "\n",
    "        model = lgb.train(params, train_set, num_boost_round = 100000, early_stopping_rounds = 100, \n",
    "                         valid_sets = [train_set, val_set], verbose_eval = 100)\n",
    "\n",
    "        oof_pred[idx_val] = model.predict(x_val)\n",
    "        y_pred += model.predict(reduce_test[[x for x in features if x not in ['installation_id']]]) / kf.n_splits\n",
    "    oof_rmse_score = np.sqrt(mean_squared_error(reduce_train[target][ind], oof_pred[ind]))\n",
    "    oof_cohen_score = cohen_kappa_score(reduce_train[target][ind], eval_qwk_lgb_regr(oof_pred[ind], reduce_train), weights = 'quadratic')\n",
    "    print('Our oof rmse score is:', oof_rmse_score)\n",
    "    print('Our oof cohen kappa score is:', oof_cohen_score)\n",
    "    return y_pred, oof_rmse_score, oof_cohen_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each validation fold extract one random observation for each installation id to simulate the test set\n",
    "def run_lr(reduce_train, reduce_test, features):\n",
    "    kf = GroupKFold(n_splits = 5)\n",
    "    target = 'accuracy_group'\n",
    "    oof_pred = np.zeros(len(reduce_train))\n",
    "    y_pred = np.zeros(len(reduce_test))\n",
    "    ind = []\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train, groups = reduce_train['installation_id'])):\n",
    "        print('Fold:', fold + 1)\n",
    "        x_train, x_val = reduce_train[features].iloc[tr_ind], reduce_train[features].iloc[val_ind]\n",
    "        y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n",
    "        x_train.drop('installation_id', inplace = True, axis = 1)\n",
    "\n",
    "        x_val, idx_val = get_random_assessment(x_val)\n",
    "        ind.extend(idx_val)\n",
    "        x_val.drop('installation_id', inplace = True, axis = 1)\n",
    "        y_val = y_val.loc[idx_val]\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(x_train, y_train)  \n",
    "\n",
    "        oof_pred[idx_val] = model.predict(x_val)\n",
    "        y_pred += model.predict(reduce_test[[x for x in features if x not in ['installation_id']]]) / kf.n_splits\n",
    "    oof_rmse_score = np.sqrt(mean_squared_error(reduce_train[target][ind], oof_pred[ind]))\n",
    "    oof_cohen_score = cohen_kappa_score(reduce_train[target][ind], eval_qwk_lgb_regr(oof_pred[ind], reduce_train), weights = 'quadratic')\n",
    "    print('Our oof rmse score is:', oof_rmse_score)\n",
    "    print('Our oof cohen kappa score is:', oof_cohen_score)\n",
    "    return y_pred, oof_rmse_score, oof_cohen_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def run_nn(reduce_train, reduce_test, features):\n",
    "    kf = GroupKFold(n_splits = 5)\n",
    "    target = 'accuracy_group'\n",
    "    oof_pred = np.zeros(len(reduce_train))\n",
    "    y_pred = np.zeros(len(reduce_test))\n",
    "    ind = []\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train, groups = reduce_train['installation_id'])):\n",
    "        print('Fold:', fold + 1)\n",
    "        x_train, x_val = reduce_train[features].iloc[tr_ind], reduce_train[features].iloc[val_ind]\n",
    "        y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n",
    "        x_train.drop('installation_id', inplace = True, axis = 1)\n",
    "\n",
    "        x_val, idx_val = get_random_assessment(x_val)\n",
    "        ind.extend(idx_val)\n",
    "        x_val.drop('installation_id', inplace = True, axis = 1)\n",
    "        y_val = y_val.loc[idx_val]\n",
    "        \n",
    "        verbosity = 100\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(x_train.shape[1],)),\n",
    "            tf.keras.layers.Dense(200, activation='relu'), #, kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(100, activation='tanh'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(25, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1, activation='relu')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mse')\n",
    "        #print(model.summary())\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('./nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "        \n",
    "        model.fit(x_train, \n",
    "                y_train, \n",
    "                validation_data=(x_val, y_val),\n",
    "                epochs=100,\n",
    "                 callbacks=[save_best, early_stop])\n",
    "        model.load_weights('./nn_model.w8')\n",
    "        \n",
    "        oof_pred[idx_val] = model.predict(x_val).reshape(x_val.shape[0],)\n",
    "        y_pred += model.predict(reduce_test[[x for x in features if x not in ['installation_id']]]).reshape(reduce_test.shape[0],) / kf.n_splits\n",
    "    oof_rmse_score = np.sqrt(mean_squared_error(reduce_train[target][ind], oof_pred[ind]))\n",
    "    oof_cohen_score = cohen_kappa_score(reduce_train[target][ind], eval_qwk_lgb_regr(oof_pred[ind], reduce_train), weights = 'quadratic')\n",
    "    print('Our oof rmse score is:', oof_rmse_score)\n",
    "    print('Our oof cohen kappa score is:', oof_cohen_score)\n",
    "    return y_pred, oof_rmse_score, oof_cohen_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(reduce_train, reduce_test):\n",
    "    features = [i for i in reduce_train.columns if i not in [\"installation_id\", \"accuracy_group\"]]\n",
    "    categoricals = ['session_title']\n",
    "    features = features.copy()\n",
    "    new_train = reduce_train.copy()\n",
    "    new_test = reduce_test.copy()\n",
    "    if len(categoricals) > 0:\n",
    "        for cat in categoricals:\n",
    "            enc = OneHotEncoder()\n",
    "            train_cats = enc.fit_transform(new_train[[cat]])\n",
    "            test_cats = enc.transform(new_test[[cat]])\n",
    "            cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.active_features_]\n",
    "            features += cat_cols\n",
    "            train_cats = pd.DataFrame(train_cats.toarray(), columns=cat_cols)\n",
    "            test_cats = pd.DataFrame(test_cats.toarray(), columns=cat_cols)\n",
    "            new_train = pd.concat([new_train, train_cats], axis=1)\n",
    "            new_test = pd.concat([new_test, test_cats], axis=1)\n",
    "        scalar = MinMaxScaler()\n",
    "        new_train[features] = scalar.fit_transform(new_train[features])\n",
    "        new_test[features] = scalar.transform(new_test[features])\n",
    "    new_train = new_train.drop([\"session_title\"], axis=1)\n",
    "    new_test = new_test.drop([\"session_title\"], axis=1)\n",
    "    return new_train, new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_correlated_features(reduce_train):\n",
    "    counter = 0\n",
    "    to_remove = []\n",
    "    for feat_a in features:\n",
    "        for feat_b in features:\n",
    "            if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n",
    "                c = np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1]\n",
    "                if c > 0.995:\n",
    "                    counter += 1\n",
    "                    to_remove.append(feat_b)\n",
    "                    print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))\n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to exclude columns from the train and test set if the mean is different, also adjust test column by a factor to simulate the same distribution\n",
    "def exclude(reduce_train, reduce_test, features):\n",
    "    to_exclude = [] \n",
    "    ajusted_test = reduce_test.copy()\n",
    "    for feature in features:\n",
    "        if feature not in ['accuracy_group', 'installation_id', 'session_title']:\n",
    "            data = reduce_train[feature]\n",
    "            train_mean = data.mean()\n",
    "            data = ajusted_test[feature] \n",
    "            test_mean = data.mean()\n",
    "            try:\n",
    "                ajust_factor = train_mean / test_mean\n",
    "                if ajust_factor > 10 or ajust_factor < 0.1:# or error > 0.01:\n",
    "                    to_exclude.append(feature)\n",
    "                    print(feature)\n",
    "                else:\n",
    "                    ajusted_test[feature] *= ajust_factor\n",
    "            except:\n",
    "                to_exclude.append(feature)\n",
    "                print(feature)\n",
    "    return to_exclude, ajusted_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train.csv file....\n",
      "Reading test.csv file....\n",
      "Reading train_labels.csv file....\n",
      "Reading specs.csv file....\n",
      "Reading sample_submission.csv file....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e79a889db214fabb1ad6bbd00aa8cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=17000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62412ed55ed457aa8381489bad5b350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0764b23abfa4e4bbd6d406ec0cfde35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train, test, train_labels, specs, sample_submission = read_data()\n",
    "train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code, activities_world = encode_title(train, test, train_labels)\n",
    "reduce_train, reduce_test, reduce_his_test = get_train_and_test(train, test)\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted from feature elimination first round script\n",
    "old_features = list(reduce_train.columns[0:95]) + list(reduce_train.columns[882:])\n",
    "el_features = ['accuracy_group', 'accuracy', 'installation_id']\n",
    "old_features = [col for col in old_features if col not in el_features]\n",
    "event_id_features = list_of_event_id #list(reduce_train.columns[95:479])\n",
    "title_event_code_cross = all_title_event_code #list(reduce_train.columns[479:882])\n",
    "features = old_features + event_id_features + title_event_code_cross\n",
    "lr_features = features.copy()\n",
    "nn_features = features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: FEAT_A: Clip FEAT_B: 27253bdc - Correlation: 0.9999999999999999\n",
      "2: FEAT_A: 2050 FEAT_B: 2040 - Correlation: 0.9965259434878118\n",
      "3: FEAT_A: 2050 FEAT_B: 26fd2d99 - Correlation: 0.9965084543995759\n",
      "4: FEAT_A: 2050 FEAT_B: 37c53127 - Correlation: 1.0\n",
      "5: FEAT_A: 2050 FEAT_B: dcaede90 - Correlation: 0.9965259434878118\n",
      "6: FEAT_A: 2050 FEAT_B: 08fd73f3 - Correlation: 0.9966123918733654\n",
      "7: FEAT_A: 2050 FEAT_B: 73757a5e - Correlation: 0.9998050146713992\n",
      "8: FEAT_A: 2050 FEAT_B: 2b9272f4 - Correlation: 0.9999839030068793\n",
      "9: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_3121 - Correlation: 0.9999839030068793\n",
      "10: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_2030 - Correlation: 0.9966123918733654\n",
      "11: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_2040 - Correlation: 0.9965259434878118\n",
      "12: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_3021 - Correlation: 0.9998050146713992\n",
      "13: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_2050 - Correlation: 1.0\n",
      "14: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_2020 - Correlation: 0.9965084543995759\n",
      "15: FEAT_A: 4230 FEAT_B: 4235 - Correlation: 0.9999995197498746\n",
      "16: FEAT_A: 4230 FEAT_B: ad148f58 - Correlation: 0.9999999999999998\n",
      "17: FEAT_A: 4230 FEAT_B: 85de926c - Correlation: 0.9999995197498746\n",
      "18: FEAT_A: 4230 FEAT_B: Bubble Bath_4235 - Correlation: 0.9999995197498746\n",
      "19: FEAT_A: 4230 FEAT_B: Bubble Bath_4230 - Correlation: 0.9999999999999998\n",
      "20: FEAT_A: 5000 FEAT_B: 5010 - Correlation: 0.9991849213605333\n",
      "21: FEAT_A: 5000 FEAT_B: 71e712d8 - Correlation: 0.9991849213605333\n",
      "22: FEAT_A: 5000 FEAT_B: a6d66e51 - Correlation: 1.0\n",
      "23: FEAT_A: 5000 FEAT_B: Watering Hole (Activity)_5000 - Correlation: 1.0\n",
      "24: FEAT_A: 5000 FEAT_B: Watering Hole (Activity)_5010 - Correlation: 0.9991849213605333\n",
      "25: FEAT_A: 3110 FEAT_B: 3010 - Correlation: 0.9999293402893735\n",
      "26: FEAT_A: 3120 FEAT_B: 3020 - Correlation: 0.9998761417908972\n",
      "27: FEAT_A: 3121 FEAT_B: 3021 - Correlation: 0.9999098200487934\n",
      "28: FEAT_A: 4031 FEAT_B: 1996c610 - Correlation: 1.0\n",
      "29: FEAT_A: 4031 FEAT_B: Dino Drink_4031 - Correlation: 1.0\n",
      "30: FEAT_A: 2000 FEAT_B: installation_session_count - Correlation: 1.0\n",
      "31: FEAT_A: 4050 FEAT_B: a1192f43 - Correlation: 0.9999999999999999\n",
      "32: FEAT_A: 4050 FEAT_B: Crystals Rule_4050 - Correlation: 0.9999999999999999\n",
      "33: FEAT_A: 2020 FEAT_B: 2030 - Correlation: 0.9959933262816534\n",
      "34: FEAT_A: 4220 FEAT_B: 1340b8d7 - Correlation: 1.0\n",
      "35: FEAT_A: 4220 FEAT_B: Bubble Bath_4220 - Correlation: 1.0\n",
      "36: FEAT_A: Tree Top City - Level 3 FEAT_B: Tree Top City - Level 3_2000 - Correlation: 1.0\n",
      "37: FEAT_A: Crystal Caves - Level 1 FEAT_B: Crystal Caves - Level 1_2000 - Correlation: 1.0\n",
      "38: FEAT_A: Tree Top City - Level 2 FEAT_B: Tree Top City - Level 2_2000 - Correlation: 1.0\n",
      "39: FEAT_A: 12 Monkeys FEAT_B: 12 Monkeys_2000 - Correlation: 1.0\n",
      "40: FEAT_A: Tree Top City - Level 1 FEAT_B: Tree Top City - Level 1_2000 - Correlation: 0.9999999999999999\n",
      "41: FEAT_A: Welcome to Lost Lagoon! FEAT_B: Welcome to Lost Lagoon!_2000 - Correlation: 1.0\n",
      "42: FEAT_A: Heavy, Heavier, Heaviest FEAT_B: Heavy, Heavier, Heaviest_2000 - Correlation: 1.0\n",
      "43: FEAT_A: Costume Box FEAT_B: Costume Box_2000 - Correlation: 1.0\n",
      "44: FEAT_A: Bottle Filler (Activity) FEAT_B: bb3e370b - Correlation: 0.9950043311420306\n",
      "45: FEAT_A: Bottle Filler (Activity) FEAT_B: Bottle Filler (Activity)_4030 - Correlation: 0.9950043311420306\n",
      "46: FEAT_A: Pirate's Tale FEAT_B: Pirate's Tale_2000 - Correlation: 0.9999999999999999\n",
      "47: FEAT_A: Honey Cake FEAT_B: Honey Cake_2000 - Correlation: 1.0\n",
      "48: FEAT_A: Magma Peak - Level 1 FEAT_B: Magma Peak - Level 1_2000 - Correlation: 1.0\n",
      "49: FEAT_A: Lifting Heavy Things FEAT_B: Lifting Heavy Things_2000 - Correlation: 0.9999999999999999\n",
      "50: FEAT_A: Magma Peak - Level 2 FEAT_B: Magma Peak - Level 2_2000 - Correlation: 1.0\n",
      "51: FEAT_A: Slop Problem FEAT_B: Slop Problem_2000 - Correlation: 1.0\n",
      "52: FEAT_A: Ordering Spheres FEAT_B: Ordering Spheres_2000 - Correlation: 1.0\n",
      "53: FEAT_A: Rulers FEAT_B: Rulers_2000 - Correlation: 1.0\n",
      "54: FEAT_A: Crystal Caves - Level 3 FEAT_B: Crystal Caves - Level 3_2000 - Correlation: 1.0\n",
      "55: FEAT_A: Treasure Map FEAT_B: Treasure Map_2000 - Correlation: 1.0\n",
      "56: FEAT_A: Crystal Caves - Level 2 FEAT_B: Crystal Caves - Level 2_2000 - Correlation: 1.0\n",
      "57: FEAT_A: Balancing Act FEAT_B: Balancing Act_2000 - Correlation: 1.0\n",
      "58: FEAT_A: lgt_Mushroom Sorter (Assessment) FEAT_B: agt_Mushroom Sorter (Assessment) - Correlation: 0.9954655260051172\n",
      "59: FEAT_A: ata_Chest Sorter (Assessment) FEAT_B: e4f1efe6 - Correlation: 0.9981999245282789\n",
      "60: FEAT_A: ata_Chest Sorter (Assessment) FEAT_B: 3afb49e6 - Correlation: 0.9996140232445475\n",
      "61: FEAT_A: ata_Chest Sorter (Assessment) FEAT_B: Chest Sorter (Assessment)_3121 - Correlation: 0.9981999245282789\n",
      "62: FEAT_A: ata_Chest Sorter (Assessment) FEAT_B: Chest Sorter (Assessment)_3021 - Correlation: 0.9996140232445475\n",
      "63: FEAT_A: ata_Cart Balancer (Assessment) FEAT_B: a8876db3 - Correlation: 0.9999170128095376\n",
      "64: FEAT_A: ata_Cart Balancer (Assessment) FEAT_B: Cart Balancer (Assessment)_3021 - Correlation: 0.9999170128095376\n",
      "65: FEAT_A: ata_Bird Measurer (Assessment) FEAT_B: 7525289a - Correlation: 0.9981555049446889\n",
      "66: FEAT_A: ata_Bird Measurer (Assessment) FEAT_B: 45d01abe - Correlation: 1.0\n",
      "67: FEAT_A: ata_Bird Measurer (Assessment) FEAT_B: Bird Measurer (Assessment)_3121 - Correlation: 0.9981555049446889\n",
      "68: FEAT_A: ata_Bird Measurer (Assessment) FEAT_B: Bird Measurer (Assessment)_3021 - Correlation: 1.0\n",
      "69: FEAT_A: ata_Mushroom Sorter (Assessment) FEAT_B: 6c930e6e - Correlation: 0.9985426312249887\n",
      "70: FEAT_A: ata_Mushroom Sorter (Assessment) FEAT_B: Mushroom Sorter (Assessment)_2030 - Correlation: 0.9985426312249887\n",
      "71: FEAT_A: afa_Chest Sorter (Assessment) FEAT_B: df4fe8b6 - Correlation: 0.9972434688333254\n",
      "72: FEAT_A: afa_Chest Sorter (Assessment) FEAT_B: ea296733 - Correlation: 0.9999880332296727\n",
      "73: FEAT_A: afa_Chest Sorter (Assessment) FEAT_B: Chest Sorter (Assessment)_3020 - Correlation: 0.9999880332296727\n",
      "74: FEAT_A: afa_Chest Sorter (Assessment) FEAT_B: Chest Sorter (Assessment)_3120 - Correlation: 0.9972434688333254\n",
      "75: FEAT_A: afa_Bird Measurer (Assessment) FEAT_B: ad2fc29c - Correlation: 0.9991605930121138\n",
      "76: FEAT_A: afa_Bird Measurer (Assessment) FEAT_B: 17113b36 - Correlation: 0.998391883995427\n",
      "77: FEAT_A: afa_Bird Measurer (Assessment) FEAT_B: e37a2b78 - Correlation: 0.9983241920747599\n",
      "78: FEAT_A: afa_Bird Measurer (Assessment) FEAT_B: Bird Measurer (Assessment)_3120 - Correlation: 0.9983241920747599\n",
      "79: FEAT_A: afa_Bird Measurer (Assessment) FEAT_B: Bird Measurer (Assessment)_4110 - Correlation: 0.998391883995427\n",
      "80: FEAT_A: afa_Bird Measurer (Assessment) FEAT_B: Bird Measurer (Assessment)_3020 - Correlation: 0.9991605930121138\n",
      "81: FEAT_A: afa_Mushroom Sorter (Assessment) FEAT_B: 88d4a5be - Correlation: 0.9989293679278667\n",
      "82: FEAT_A: afa_Mushroom Sorter (Assessment) FEAT_B: 160654fd - Correlation: 0.9999996751514757\n",
      "83: FEAT_A: afa_Mushroom Sorter (Assessment) FEAT_B: Mushroom Sorter (Assessment)_3020 - Correlation: 0.9999996751514757\n",
      "84: FEAT_A: afa_Mushroom Sorter (Assessment) FEAT_B: Mushroom Sorter (Assessment)_3120 - Correlation: 0.9989293679278667\n",
      "85: FEAT_A: var_event_id FEAT_B: var_title_event_code - Correlation: 0.9995254184300616\n",
      "86: FEAT_A: b7530680 FEAT_B: e9c52111 - Correlation: 0.998817689964623\n",
      "87: FEAT_A: b7530680 FEAT_B: Bottle Filler (Activity)_2020 - Correlation: 1.0\n",
      "88: FEAT_A: b7530680 FEAT_B: Bottle Filler (Activity)_2030 - Correlation: 0.998817689964623\n",
      "89: FEAT_A: 2a444e03 FEAT_B: Pan Balance_4030 - Correlation: 1.0\n",
      "90: FEAT_A: 30614231 FEAT_B: 37ee8496 - Correlation: 0.9967763987631819\n",
      "91: FEAT_A: 30614231 FEAT_B: Cauldron Filler (Assessment)_4020 - Correlation: 1.0\n",
      "92: FEAT_A: 30614231 FEAT_B: Cauldron Filler (Assessment)_4030 - Correlation: 0.9967763987631819\n",
      "93: FEAT_A: 90d848e0 FEAT_B: Cauldron Filler (Assessment)_2000 - Correlation: 1.0\n",
      "94: FEAT_A: e5734469 FEAT_B: 89aace00 - Correlation: 0.9998406115110344\n",
      "95: FEAT_A: e5734469 FEAT_B: Dino Drink_3120 - Correlation: 0.9998406115110344\n",
      "96: FEAT_A: e5734469 FEAT_B: Dino Drink_3020 - Correlation: 1.0\n",
      "97: FEAT_A: 3ddc79c3 FEAT_B: 7cf1bc53 - Correlation: 0.99823323877902\n",
      "98: FEAT_A: 3ddc79c3 FEAT_B: 3323d7e9 - Correlation: 0.9998606654761331\n",
      "99: FEAT_A: 3ddc79c3 FEAT_B: e720d930 - Correlation: 0.9998920962508026\n",
      "100: FEAT_A: 3ddc79c3 FEAT_B: Crystals Rule_2030 - Correlation: 0.9998606654761331\n",
      "101: FEAT_A: 3ddc79c3 FEAT_B: Crystals Rule_3021 - Correlation: 0.9999999999999998\n",
      "102: FEAT_A: 3ddc79c3 FEAT_B: Crystals Rule_2020 - Correlation: 0.99823323877902\n",
      "103: FEAT_A: 3ddc79c3 FEAT_B: Crystals Rule_3121 - Correlation: 0.9998920962508026\n",
      "104: FEAT_A: 5c3d2b2f FEAT_B: Scrub-A-Dub_4020 - Correlation: 1.0\n",
      "105: FEAT_A: 9e6b7fb5 FEAT_B: Chow Time_4095 - Correlation: 0.9999999999999998\n",
      "106: FEAT_A: 67aa2ada FEAT_B: Leaf Leader_4090 - Correlation: 1.0\n",
      "107: FEAT_A: 31973d56 FEAT_B: 5de79a6a - Correlation: 0.9973945339840646\n",
      "108: FEAT_A: 31973d56 FEAT_B: Cart Balancer (Assessment)_3020 - Correlation: 0.9973945339840646\n",
      "109: FEAT_A: 31973d56 FEAT_B: Cart Balancer (Assessment)_3120 - Correlation: 1.0\n",
      "110: FEAT_A: e79f3763 FEAT_B: Bug Measurer (Activity)_4030 - Correlation: 1.0\n",
      "111: FEAT_A: d06f75b5 FEAT_B: 3bb91dda - Correlation: 0.997087740932082\n",
      "112: FEAT_A: d06f75b5 FEAT_B: 895865f3 - Correlation: 0.9991496470458585\n",
      "113: FEAT_A: d06f75b5 FEAT_B: c54cf6c5 - Correlation: 0.99723835793018\n",
      "114: FEAT_A: d06f75b5 FEAT_B: Bubble Bath_2030 - Correlation: 0.9991496470458585\n",
      "115: FEAT_A: d06f75b5 FEAT_B: Bubble Bath_2025 - Correlation: 0.99723835793018\n",
      "116: FEAT_A: d06f75b5 FEAT_B: Bubble Bath_2035 - Correlation: 1.0\n",
      "117: FEAT_A: d06f75b5 FEAT_B: Bubble Bath_4020 - Correlation: 0.997087740932082\n",
      "118: FEAT_A: db02c830 FEAT_B: 3bfd1a65 - Correlation: 0.9999982205265873\n",
      "119: FEAT_A: db02c830 FEAT_B: Mushroom Sorter (Assessment)_2000 - Correlation: 0.9999982205265873\n",
      "120: FEAT_A: db02c830 FEAT_B: Mushroom Sorter (Assessment)_2025 - Correlation: 0.9999999999999999\n",
      "121: FEAT_A: 250513af FEAT_B: cf7638f3 - Correlation: 0.999630364081927\n",
      "122: FEAT_A: 250513af FEAT_B: a592d54e - Correlation: 0.9973020736885158\n",
      "123: FEAT_A: 250513af FEAT_B: 1c178d24 - Correlation: 0.9999803429610506\n",
      "124: FEAT_A: 250513af FEAT_B: Pan Balance_3121 - Correlation: 0.999630364081927\n",
      "125: FEAT_A: 250513af FEAT_B: Pan Balance_3021 - Correlation: 1.0\n",
      "126: FEAT_A: 250513af FEAT_B: Pan Balance_2030 - Correlation: 0.9999803429610506\n",
      "127: FEAT_A: 250513af FEAT_B: Pan Balance_2020 - Correlation: 0.9973020736885158\n",
      "128: FEAT_A: 532a2afb FEAT_B: Cauldron Filler (Assessment)_2020 - Correlation: 1.0\n",
      "129: FEAT_A: 05ad839b FEAT_B: Happy Camel_4090 - Correlation: 1.0\n",
      "130: FEAT_A: 16667cc5 FEAT_B: Chicken Balancer (Activity)_4080 - Correlation: 0.9999999999999999\n",
      "131: FEAT_A: 49ed92e9 FEAT_B: bd701df8 - Correlation: 0.9993109138888533\n",
      "132: FEAT_A: 49ed92e9 FEAT_B: Watering Hole (Activity)_3010 - Correlation: 1.0\n",
      "133: FEAT_A: 49ed92e9 FEAT_B: Watering Hole (Activity)_3110 - Correlation: 0.9993109138888533\n",
      "134: FEAT_A: 4c2ec19f FEAT_B: Egg Dropper (Activity)_4025 - Correlation: 1.0\n",
      "135: FEAT_A: cb6010f8 FEAT_B: 47026d5f - Correlation: 0.9996901555713447\n",
      "136: FEAT_A: cb6010f8 FEAT_B: 56817e2b - Correlation: 0.999047573942978\n",
      "137: FEAT_A: cb6010f8 FEAT_B: Chow Time_3121 - Correlation: 0.9999999999999999\n",
      "138: FEAT_A: cb6010f8 FEAT_B: Chow Time_2030 - Correlation: 0.999047573942978\n",
      "139: FEAT_A: cb6010f8 FEAT_B: Chow Time_3021 - Correlation: 0.9996901555713447\n",
      "140: FEAT_A: 3bb91ced FEAT_B: Happy Camel_2081 - Correlation: 1.0\n",
      "141: FEAT_A: 6aeafed4 FEAT_B: Bubble Bath_4090 - Correlation: 1.0\n",
      "142: FEAT_A: 7d5c30a2 FEAT_B: Dino Dive_2060 - Correlation: 1.0\n",
      "143: FEAT_A: 736f9581 FEAT_B: 9b23e8ee - Correlation: 0.9999999999999999\n",
      "144: FEAT_A: 736f9581 FEAT_B: Egg Dropper (Activity)_2000 - Correlation: 0.9999999999999999\n",
      "145: FEAT_A: 736f9581 FEAT_B: Egg Dropper (Activity)_2020 - Correlation: 0.9999999999999999\n",
      "146: FEAT_A: 3a4be871 FEAT_B: Flower Waterer (Activity)_4080 - Correlation: 1.0\n",
      "147: FEAT_A: 7da34a02 FEAT_B: Mushroom Sorter (Assessment)_4070 - Correlation: 0.9999999999999998\n",
      "148: FEAT_A: 5e3ea25a FEAT_B: Crystals Rule_4070 - Correlation: 1.0\n",
      "149: FEAT_A: 83c6c409 FEAT_B: 3dfd4aa4 - Correlation: 0.9999970893851744\n",
      "150: FEAT_A: 83c6c409 FEAT_B: 28ed704e - Correlation: 0.9996646794414074\n",
      "151: FEAT_A: 83c6c409 FEAT_B: c74f40cd - Correlation: 0.9963987221516345\n",
      "152: FEAT_A: 83c6c409 FEAT_B: 9d29771f - Correlation: 0.9963745894369906\n",
      "153: FEAT_A: 83c6c409 FEAT_B: Mushroom Sorter (Assessment)_2035 - Correlation: 1.0\n",
      "154: FEAT_A: 83c6c409 FEAT_B: Mushroom Sorter (Assessment)_3121 - Correlation: 0.9963987221516345\n",
      "155: FEAT_A: 83c6c409 FEAT_B: Mushroom Sorter (Assessment)_2020 - Correlation: 0.9999970893851744\n",
      "156: FEAT_A: 83c6c409 FEAT_B: Mushroom Sorter (Assessment)_4025 - Correlation: 0.9996646794414074\n",
      "157: FEAT_A: 83c6c409 FEAT_B: Mushroom Sorter (Assessment)_3021 - Correlation: 0.9963745894369906\n",
      "158: FEAT_A: 14de4c5d FEAT_B: Air Show_4100 - Correlation: 1.0\n",
      "159: FEAT_A: 7961e599 FEAT_B: 709b1251 - Correlation: 0.9950004881933644\n",
      "160: FEAT_A: 7961e599 FEAT_B: Dino Dive_3121 - Correlation: 0.9950004881933644\n",
      "161: FEAT_A: 7961e599 FEAT_B: Dino Dive_2020 - Correlation: 0.9999999999999999\n",
      "162: FEAT_A: 7ad3efc6 FEAT_B: 65a38bf7 - Correlation: 0.999978626569043\n",
      "163: FEAT_A: 7ad3efc6 FEAT_B: Cart Balancer (Assessment)_2000 - Correlation: 1.0\n",
      "164: FEAT_A: 7ad3efc6 FEAT_B: Cart Balancer (Assessment)_2020 - Correlation: 0.999978626569043\n",
      "165: FEAT_A: 5f0eb72c FEAT_B: Mushroom Sorter (Assessment)_4020 - Correlation: 0.9999999999999998\n",
      "166: FEAT_A: 8b757ab8 FEAT_B: 44cb4907 - Correlation: 0.999835058794711\n",
      "167: FEAT_A: 8b757ab8 FEAT_B: Crystals Rule_3020 - Correlation: 0.999835058794711\n",
      "168: FEAT_A: 8b757ab8 FEAT_B: Crystals Rule_3120 - Correlation: 1.0\n",
      "169: FEAT_A: e7e44842 FEAT_B: Watering Hole (Activity)_4090 - Correlation: 0.9999999999999999\n",
      "170: FEAT_A: ea321fb1 FEAT_B: 84b0e0c8 - Correlation: 0.9993007600205108\n",
      "171: FEAT_A: ea321fb1 FEAT_B: Chicken Balancer (Activity)_3010 - Correlation: 1.0\n",
      "172: FEAT_A: ea321fb1 FEAT_B: Chicken Balancer (Activity)_3110 - Correlation: 0.9993007600205108\n",
      "173: FEAT_A: 3ee399c3 FEAT_B: Cauldron Filler (Assessment)_4070 - Correlation: 1.0\n",
      "174: FEAT_A: beb0a7b9 FEAT_B: b88f38da - Correlation: 0.9999125179829754\n",
      "175: FEAT_A: beb0a7b9 FEAT_B: Fireworks (Activity)_3110 - Correlation: 0.9999125179829754\n",
      "176: FEAT_A: beb0a7b9 FEAT_B: Fireworks (Activity)_3010 - Correlation: 1.0\n",
      "177: FEAT_A: 47efca07 FEAT_B: Bottle Filler (Activity)_4090 - Correlation: 1.0\n",
      "178: FEAT_A: 99ea62f3 FEAT_B: Bubble Bath_2083 - Correlation: 1.0\n",
      "179: FEAT_A: 5348fd84 FEAT_B: Cauldron Filler (Assessment)_4040 - Correlation: 1.0\n",
      "180: FEAT_A: d02b7a8e FEAT_B: All Star Sorting_4035 - Correlation: 1.0\n",
      "181: FEAT_A: 5e812b27 FEAT_B: Sandcastle Builder (Activity)_4030 - Correlation: 1.0\n",
      "182: FEAT_A: ecc6157f FEAT_B: Cart Balancer (Assessment)_4080 - Correlation: 0.9999999999999999\n",
      "183: FEAT_A: f806dc10 FEAT_B: Dino Drink_2020 - Correlation: 1.0\n",
      "184: FEAT_A: a0faea5d FEAT_B: Bubble Bath_4070 - Correlation: 1.0\n",
      "185: FEAT_A: 9554a50b FEAT_B: Cauldron Filler (Assessment)_4080 - Correlation: 0.9999999999999999\n",
      "186: FEAT_A: 8ac7cce4 FEAT_B: Leaf Leader_2000 - Correlation: 1.0\n",
      "187: FEAT_A: 7423acbc FEAT_B: e04fb33d - Correlation: 0.999628997408126\n",
      "188: FEAT_A: 7423acbc FEAT_B: Air Show_3020 - Correlation: 1.0\n",
      "189: FEAT_A: 7423acbc FEAT_B: Air Show_3120 - Correlation: 0.999628997408126\n",
      "190: FEAT_A: 15a43e5b FEAT_B: Bottle Filler (Activity)_4070 - Correlation: 1.0\n",
      "191: FEAT_A: 611485c5 FEAT_B: Fireworks (Activity)_4080 - Correlation: 1.0\n",
      "192: FEAT_A: 37db1c2f FEAT_B: Happy Camel_4045 - Correlation: 1.0\n",
      "193: FEAT_A: c1cac9a2 FEAT_B: Scrub-A-Dub_2081 - Correlation: 0.9999999999999998\n",
      "194: FEAT_A: e64e2cfd FEAT_B: Watering Hole (Activity)_2000 - Correlation: 0.9999999999999998\n",
      "195: FEAT_A: 4d911100 FEAT_B: 77ead60d - Correlation: 0.9998475927724766\n",
      "196: FEAT_A: 4d911100 FEAT_B: 16dffff1 - Correlation: 0.9986046680098603\n",
      "197: FEAT_A: 4d911100 FEAT_B: Dino Drink_3021 - Correlation: 0.9998475927724766\n",
      "198: FEAT_A: 4d911100 FEAT_B: Dino Drink_2030 - Correlation: 0.9986046680098603\n",
      "199: FEAT_A: 4d911100 FEAT_B: Dino Drink_3121 - Correlation: 1.0\n",
      "200: FEAT_A: 51102b85 FEAT_B: Bird Measurer (Assessment)_4030 - Correlation: 1.0\n",
      "201: FEAT_A: 565a3990 FEAT_B: Bug Measurer (Activity)_4070 - Correlation: 1.0\n",
      "202: FEAT_A: 55115cbd FEAT_B: 6f4adc4b - Correlation: 0.9997831892615398\n",
      "203: FEAT_A: 55115cbd FEAT_B: Bubble Bath_3021 - Correlation: 0.9997831892615398\n",
      "204: FEAT_A: 55115cbd FEAT_B: Bubble Bath_3121 - Correlation: 0.9999999999999999\n",
      "205: FEAT_A: 8d748b58 FEAT_B: Bug Measurer (Activity)_4090 - Correlation: 0.9999999999999998\n",
      "206: FEAT_A: 0330ab6a FEAT_B: 2230fab4 - Correlation: 0.9998673365188063\n",
      "207: FEAT_A: 0330ab6a FEAT_B: Chow Time_3020 - Correlation: 1.0\n",
      "208: FEAT_A: 0330ab6a FEAT_B: Chow Time_3120 - Correlation: 0.9998673365188063\n",
      "209: FEAT_A: 731c0cbe FEAT_B: Bird Measurer (Assessment)_4090 - Correlation: 1.0\n",
      "210: FEAT_A: 4e5fc6f5 FEAT_B: Cart Balancer (Assessment)_4090 - Correlation: 1.0\n",
      "211: FEAT_A: 6f445b57 FEAT_B: Chow Time_4080 - Correlation: 1.0\n",
      "212: FEAT_A: 9ed8f6da FEAT_B: Dino Drink_2075 - Correlation: 0.9999999999999999\n",
      "213: FEAT_A: 6bf9e3e1 FEAT_B: Happy Camel_4040 - Correlation: 0.9999999999999999\n",
      "214: FEAT_A: a1bbe385 FEAT_B: f28c589a - Correlation: 0.999953679734225\n",
      "215: FEAT_A: a1bbe385 FEAT_B: Air Show_3110 - Correlation: 1.0\n",
      "216: FEAT_A: a1bbe385 FEAT_B: Air Show_3010 - Correlation: 0.999953679734225\n",
      "217: FEAT_A: a8efe47b FEAT_B: Chest Sorter (Assessment)_4030 - Correlation: 1.0\n",
      "218: FEAT_A: 923afab1 FEAT_B: 2dcad279 - Correlation: 0.9998567985670082\n",
      "219: FEAT_A: 923afab1 FEAT_B: Cauldron Filler (Assessment)_3010 - Correlation: 1.0\n",
      "220: FEAT_A: 923afab1 FEAT_B: Cauldron Filler (Assessment)_3110 - Correlation: 0.9998567985670082\n",
      "221: FEAT_A: c952eb01 FEAT_B: Watering Hole (Activity)_4070 - Correlation: 1.0\n",
      "222: FEAT_A: 3d63345e FEAT_B: Cart Balancer (Assessment)_4035 - Correlation: 1.0\n",
      "223: FEAT_A: 99abe2bb FEAT_B: Bubble Bath_2080 - Correlation: 1.0\n",
      "224: FEAT_A: c7fe2a55 FEAT_B: 36fa3ebe - Correlation: 0.9998350234117662\n",
      "225: FEAT_A: c7fe2a55 FEAT_B: a8a78786 - Correlation: 0.9981452039350779\n",
      "226: FEAT_A: c7fe2a55 FEAT_B: Happy Camel_3021 - Correlation: 1.0\n",
      "227: FEAT_A: c7fe2a55 FEAT_B: Happy Camel_2030 - Correlation: 0.9998350234117662\n",
      "228: FEAT_A: c7fe2a55 FEAT_B: Happy Camel_3121 - Correlation: 0.9981452039350779\n",
      "229: FEAT_A: d122731b FEAT_B: Cart Balancer (Assessment)_4100 - Correlation: 0.9999999999999998\n",
      "230: FEAT_A: 93edfe2e FEAT_B: Crystals Rule_4090 - Correlation: 1.0\n",
      "231: FEAT_A: fcfdffb6 FEAT_B: Flower Waterer (Activity)_4022 - Correlation: 1.0\n",
      "232: FEAT_A: e4d32835 FEAT_B: Pan Balance_4080 - Correlation: 0.9999999999999998\n",
      "233: FEAT_A: 756e5507 FEAT_B: Chicken Balancer (Activity)_2000 - Correlation: 1.0\n",
      "234: FEAT_A: 6c517a88 FEAT_B: Dino Drink_4070 - Correlation: 1.0\n",
      "235: FEAT_A: 56cd3b43 FEAT_B: bbfe0445 - Correlation: 0.9996926215355526\n",
      "236: FEAT_A: 56cd3b43 FEAT_B: Flower Waterer (Activity)_3010 - Correlation: 1.0\n",
      "237: FEAT_A: 56cd3b43 FEAT_B: Flower Waterer (Activity)_3110 - Correlation: 0.9996926215355526\n",
      "238: FEAT_A: 598f4598 FEAT_B: Flower Waterer (Activity)_4025 - Correlation: 0.9999999999999998\n",
      "239: FEAT_A: 3b2048ee FEAT_B: Leaf Leader_4095 - Correlation: 1.0\n",
      "240: FEAT_A: 6d90d394 FEAT_B: Scrub-A-Dub_2000 - Correlation: 0.9999999999999999\n",
      "241: FEAT_A: 86ba578b FEAT_B: Leaf Leader_2070 - Correlation: 1.0\n",
      "242: FEAT_A: 1af8be29 FEAT_B: 3bf1cf26 - Correlation: 0.9998900847287077\n",
      "243: FEAT_A: 1af8be29 FEAT_B: Happy Camel_3020 - Correlation: 0.9999999999999999\n",
      "244: FEAT_A: 1af8be29 FEAT_B: Happy Camel_3120 - Correlation: 0.9998900847287077\n",
      "245: FEAT_A: e080a381 FEAT_B: Pan Balance_4090 - Correlation: 1.0\n",
      "246: FEAT_A: d2e9262e FEAT_B: 2fb91ec1 - Correlation: 0.9991434495208743\n",
      "247: FEAT_A: d2e9262e FEAT_B: Watering Hole (Activity)_4025 - Correlation: 0.9991434495208743\n",
      "248: FEAT_A: d2e9262e FEAT_B: Watering Hole (Activity)_4020 - Correlation: 0.9999999999999998\n",
      "249: FEAT_A: 0ce40006 FEAT_B: Happy Camel_4080 - Correlation: 1.0\n",
      "250: FEAT_A: 5c2f29ca FEAT_B: Cart Balancer (Assessment)_4020 - Correlation: 1.0\n",
      "251: FEAT_A: 93b353f2 FEAT_B: Chest Sorter (Assessment)_4100 - Correlation: 0.9999999999999998\n",
      "252: FEAT_A: 56bcd38d FEAT_B: Chicken Balancer (Activity)_4030 - Correlation: 0.9999999999999998\n",
      "253: FEAT_A: acf5c23f FEAT_B: Cart Balancer (Assessment)_4070 - Correlation: 1.0\n",
      "254: FEAT_A: 1325467d FEAT_B: Sandcastle Builder (Activity)_4070 - Correlation: 1.0\n",
      "255: FEAT_A: 7ab78247 FEAT_B: b80e5e84 - Correlation: 0.9998336590281087\n",
      "256: FEAT_A: 7ab78247 FEAT_B: Egg Dropper (Activity)_3110 - Correlation: 0.9998336590281087\n",
      "257: FEAT_A: 7ab78247 FEAT_B: Egg Dropper (Activity)_3010 - Correlation: 1.0\n",
      "258: FEAT_A: bfc77bd6 FEAT_B: Chest Sorter (Assessment)_4080 - Correlation: 0.9999999999999998\n",
      "259: FEAT_A: 119b5b02 FEAT_B: Dino Dive_4080 - Correlation: 1.0\n",
      "260: FEAT_A: abc5811c FEAT_B: Happy Camel_4010 - Correlation: 1.0\n",
      "261: FEAT_A: 29f54413 FEAT_B: Leaf Leader_2060 - Correlation: 0.9999999999999998\n",
      "262: FEAT_A: 86c924c4 FEAT_B: 5154fc30 - Correlation: 0.9988936322895169\n",
      "263: FEAT_A: 86c924c4 FEAT_B: 3babcb9b - Correlation: 0.9988970478897697\n",
      "264: FEAT_A: 86c924c4 FEAT_B: Crystals Rule_3110 - Correlation: 0.9988970478897697\n",
      "265: FEAT_A: 86c924c4 FEAT_B: Crystals Rule_3010 - Correlation: 0.9988936322895169\n",
      "266: FEAT_A: 86c924c4 FEAT_B: Crystals Rule_4020 - Correlation: 1.0\n",
      "267: FEAT_A: 4ef8cdd3 FEAT_B: Chow Time_4020 - Correlation: 1.0\n",
      "268: FEAT_A: c58186bf FEAT_B: Sandcastle Builder (Activity)_4035 - Correlation: 1.0\n",
      "269: FEAT_A: 7040c096 FEAT_B: Scrub-A-Dub_4010 - Correlation: 1.0\n",
      "270: FEAT_A: d2659ab4 FEAT_B: Air Show_2075 - Correlation: 1.0\n",
      "271: FEAT_A: 795e4a37 FEAT_B: Cart Balancer (Assessment)_3010 - Correlation: 1.0\n",
      "272: FEAT_A: b012cd7f FEAT_B: e5c9df6f - Correlation: 0.9990885395773584\n",
      "273: FEAT_A: b012cd7f FEAT_B: 3afde5dd - Correlation: 0.9999689260314981\n",
      "274: FEAT_A: b012cd7f FEAT_B: Leaf Leader_3121 - Correlation: 0.9990885395773584\n",
      "275: FEAT_A: b012cd7f FEAT_B: Leaf Leader_2030 - Correlation: 1.0\n",
      "276: FEAT_A: b012cd7f FEAT_B: Leaf Leader_3021 - Correlation: 0.9999689260314981\n",
      "277: FEAT_A: 763fc34e FEAT_B: e57dd7af - Correlation: 0.9972721980394412\n",
      "278: FEAT_A: 763fc34e FEAT_B: Leaf Leader_3120 - Correlation: 0.9972721980394412\n",
      "279: FEAT_A: 763fc34e FEAT_B: Leaf Leader_3020 - Correlation: 1.0\n",
      "280: FEAT_A: 74e5f8a7 FEAT_B: Dino Drink_4020 - Correlation: 1.0\n",
      "281: FEAT_A: 587b5989 FEAT_B: All Star Sorting_4070 - Correlation: 1.0\n",
      "282: FEAT_A: 155f62a4 FEAT_B: 5b49460a - Correlation: 0.9999999999999999\n",
      "283: FEAT_A: 155f62a4 FEAT_B: Chest Sorter (Assessment)_2020 - Correlation: 0.9999999999999999\n",
      "284: FEAT_A: 155f62a4 FEAT_B: Chest Sorter (Assessment)_2000 - Correlation: 0.9999999999999999\n",
      "285: FEAT_A: 02a42007 FEAT_B: Fireworks (Activity)_4030 - Correlation: 1.0\n",
      "286: FEAT_A: e694a35b FEAT_B: Fireworks (Activity)_4020 - Correlation: 1.0\n",
      "287: FEAT_A: 26a5a3dd FEAT_B: All Star Sorting_4080 - Correlation: 1.0\n",
      "288: FEAT_A: 48349b14 FEAT_B: Crystals Rule_2000 - Correlation: 1.0\n",
      "289: FEAT_A: 070a5291 FEAT_B: Bird Measurer (Assessment)_4100 - Correlation: 1.0\n",
      "290: FEAT_A: d3268efa FEAT_B: 28520915 - Correlation: 0.9989055023166136\n",
      "291: FEAT_A: d3268efa FEAT_B: b5053438 - Correlation: 0.999576326704631\n",
      "292: FEAT_A: d3268efa FEAT_B: Cauldron Filler (Assessment)_3021 - Correlation: 1.0\n",
      "293: FEAT_A: d3268efa FEAT_B: Cauldron Filler (Assessment)_3121 - Correlation: 0.999576326704631\n",
      "294: FEAT_A: d3268efa FEAT_B: Cauldron Filler (Assessment)_2030 - Correlation: 0.9989055023166136\n",
      "295: FEAT_A: 3d8c61b0 FEAT_B: Happy Camel_4030 - Correlation: 0.9999999999999999\n",
      "296: FEAT_A: de26c3a6 FEAT_B: Flower Waterer (Activity)_4020 - Correlation: 0.9999999999999999\n",
      "297: FEAT_A: 0413e89d FEAT_B: 15eb4a7d - Correlation: 0.9997266832893074\n",
      "298: FEAT_A: 0413e89d FEAT_B: Bubble Bath_3010 - Correlation: 1.0\n",
      "299: FEAT_A: 0413e89d FEAT_B: Bubble Bath_3110 - Correlation: 0.9997266832893074\n",
      "300: FEAT_A: 46b50ba8 FEAT_B: Happy Camel_4095 - Correlation: 0.9999999999999998\n",
      "301: FEAT_A: 7372e1a5 FEAT_B: Chow Time_4070 - Correlation: 1.0\n",
      "302: FEAT_A: c7128948 FEAT_B: Mushroom Sorter (Assessment)_4040 - Correlation: 1.0\n",
      "303: FEAT_A: d51b1749 FEAT_B: Happy Camel_2080 - Correlation: 1.0\n",
      "304: FEAT_A: 3393b68b FEAT_B: Bird Measurer (Assessment)_2010 - Correlation: 1.0\n",
      "305: FEAT_A: 262136f4 FEAT_B: Leaf Leader_4020 - Correlation: 1.0\n",
      "306: FEAT_A: 30df3273 FEAT_B: Sandcastle Builder (Activity)_4080 - Correlation: 1.0\n",
      "307: FEAT_A: 8f094001 FEAT_B: 1beb320a - Correlation: 0.9980774800878145\n",
      "308: FEAT_A: 8f094001 FEAT_B: Bubble Bath_2020 - Correlation: 0.9980774800878145\n",
      "309: FEAT_A: 8f094001 FEAT_B: Bubble Bath_4045 - Correlation: 1.0\n",
      "310: FEAT_A: d38c2fd7 FEAT_B: Bird Measurer (Assessment)_4035 - Correlation: 0.9999999999999999\n",
      "311: FEAT_A: cdd22e43 FEAT_B: Chicken Balancer (Activity)_4035 - Correlation: 1.0\n",
      "312: FEAT_A: 63f13dd7 FEAT_B: Chow Time_2020 - Correlation: 1.0\n",
      "313: FEAT_A: 46cd75b4 FEAT_B: Chicken Balancer (Activity)_4022 - Correlation: 0.9999999999999998\n",
      "314: FEAT_A: c51d8688 FEAT_B: 907a054b - Correlation: 0.9999667370361688\n",
      "315: FEAT_A: c51d8688 FEAT_B: Pan Balance_3120 - Correlation: 1.0\n",
      "316: FEAT_A: c51d8688 FEAT_B: Pan Balance_3020 - Correlation: 0.9999667370361688\n",
      "317: FEAT_A: 792530f8 FEAT_B: Dino Drink_4030 - Correlation: 1.0\n",
      "318: FEAT_A: 47f43a44 FEAT_B: Flower Waterer (Activity)_4090 - Correlation: 1.0\n",
      "319: FEAT_A: 9de5e594 FEAT_B: 28a4eb9a - Correlation: 0.9995923561196808\n",
      "320: FEAT_A: 9de5e594 FEAT_B: Dino Dive_3120 - Correlation: 0.9995923561196808\n",
      "321: FEAT_A: 9de5e594 FEAT_B: Dino Dive_3020 - Correlation: 1.0\n",
      "322: FEAT_A: 7dfe6d8a FEAT_B: Leaf Leader_4070 - Correlation: 0.9999999999999998\n",
      "323: FEAT_A: 7fd1ac25 FEAT_B: Egg Dropper (Activity)_4080 - Correlation: 1.0\n",
      "324: FEAT_A: 857f21c0 FEAT_B: Bubble Bath_4040 - Correlation: 1.0\n",
      "325: FEAT_A: 0a08139c FEAT_B: 71fe8f75 - Correlation: 0.9999850342981554\n",
      "326: FEAT_A: 0a08139c FEAT_B: Bug Measurer (Activity)_3010 - Correlation: 1.0\n",
      "327: FEAT_A: 0a08139c FEAT_B: Bug Measurer (Activity)_3110 - Correlation: 0.9999850342981554\n",
      "328: FEAT_A: 1f19558b FEAT_B: daac11b0 - Correlation: 0.9991356096406655\n",
      "329: FEAT_A: 1f19558b FEAT_B: ca11f653 - Correlation: 0.998316427341082\n",
      "330: FEAT_A: 1f19558b FEAT_B: All Star Sorting_2030 - Correlation: 0.998316427341082\n",
      "331: FEAT_A: 1f19558b FEAT_B: All Star Sorting_3021 - Correlation: 0.9991356096406655\n",
      "332: FEAT_A: 1f19558b FEAT_B: All Star Sorting_3121 - Correlation: 1.0\n",
      "333: FEAT_A: cf82af56 FEAT_B: Scrub-A-Dub_4070 - Correlation: 1.0\n",
      "334: FEAT_A: f7e47413 FEAT_B: f71c4741 - Correlation: 0.9999426890770878\n",
      "335: FEAT_A: f7e47413 FEAT_B: Scrub-A-Dub_3110 - Correlation: 1.0\n",
      "336: FEAT_A: f7e47413 FEAT_B: Scrub-A-Dub_3010 - Correlation: 0.9999426890770878\n",
      "337: FEAT_A: f93fc684 FEAT_B: Chow Time_4010 - Correlation: 0.9999999999999999\n",
      "338: FEAT_A: a7640a16 FEAT_B: Happy Camel_4070 - Correlation: 1.0\n",
      "339: FEAT_A: 87d743c1 FEAT_B: Dino Dive_4010 - Correlation: 1.0\n",
      "340: FEAT_A: 1b54d27f FEAT_B: Watering Hole (Activity)_2010 - Correlation: 1.0\n",
      "341: FEAT_A: 4b5efe37 FEAT_B: b7dc8128 - Correlation: 0.9980151285383981\n",
      "342: FEAT_A: 4b5efe37 FEAT_B: All Star Sorting_2000 - Correlation: 0.9980151285383981\n",
      "343: FEAT_A: 4b5efe37 FEAT_B: All Star Sorting_4010 - Correlation: 1.0\n",
      "344: FEAT_A: a76029ee FEAT_B: Bird Measurer (Assessment)_4040 - Correlation: 0.9999999999999999\n",
      "345: FEAT_A: e3ff61fb FEAT_B: Dino Dive_3021 - Correlation: 1.0\n",
      "346: FEAT_A: c189aaf2 FEAT_B: Happy Camel_2083 - Correlation: 0.9999999999999999\n",
      "347: FEAT_A: 77261ab5 FEAT_B: Sandcastle Builder (Activity)_2000 - Correlation: 0.9999999999999999\n",
      "348: FEAT_A: 1575e76c FEAT_B: Air Show_2020 - Correlation: 1.0\n",
      "349: FEAT_A: 2ec694de FEAT_B: Bug Measurer (Activity)_4080 - Correlation: 0.9999999999999998\n",
      "350: FEAT_A: 4a4c3d21 FEAT_B: Bird Measurer (Assessment)_4025 - Correlation: 1.0\n",
      "351: FEAT_A: 7f0836bf FEAT_B: a29c5338 - Correlation: 0.9986531654717627\n",
      "352: FEAT_A: 7f0836bf FEAT_B: Dino Drink_3010 - Correlation: 0.9986531654717627\n",
      "353: FEAT_A: 7f0836bf FEAT_B: Dino Drink_3110 - Correlation: 1.0\n",
      "354: FEAT_A: 499edb7c FEAT_B: Chicken Balancer (Activity)_4020 - Correlation: 1.0\n",
      "355: FEAT_A: 6f4bd64e FEAT_B: Air Show_4090 - Correlation: 0.9999999999999998\n",
      "356: FEAT_A: d45ed6a1 FEAT_B: b120f2ac - Correlation: 0.9979707847816691\n",
      "357: FEAT_A: d45ed6a1 FEAT_B: c277e121 - Correlation: 0.9979692879543319\n",
      "358: FEAT_A: d45ed6a1 FEAT_B: All Star Sorting_3020 - Correlation: 0.9979692879543319\n",
      "359: FEAT_A: d45ed6a1 FEAT_B: All Star Sorting_2025 - Correlation: 0.9979707847816691\n",
      "360: FEAT_A: d45ed6a1 FEAT_B: All Star Sorting_3120 - Correlation: 1.0\n",
      "361: FEAT_A: ab4ec3a4 FEAT_B: Dino Drink_4080 - Correlation: 1.0\n",
      "362: FEAT_A: 9b01374f FEAT_B: Flower Waterer (Activity)_2000 - Correlation: 1.0\n",
      "363: FEAT_A: 363d3849 FEAT_B: 9e4c8c7b - Correlation: 0.9992130941883633\n",
      "364: FEAT_A: 363d3849 FEAT_B: All Star Sorting_3010 - Correlation: 1.0\n",
      "365: FEAT_A: 363d3849 FEAT_B: All Star Sorting_3110 - Correlation: 0.9992130941883633\n",
      "366: FEAT_A: a44b10dc FEAT_B: Flower Waterer (Activity)_4070 - Correlation: 1.0\n",
      "367: FEAT_A: 3ccd3f02 FEAT_B: 3dcdda7f - Correlation: 0.9977337946782758\n",
      "368: FEAT_A: 3ccd3f02 FEAT_B: Chest Sorter (Assessment)_3110 - Correlation: 1.0\n",
      "369: FEAT_A: 3ccd3f02 FEAT_B: Chest Sorter (Assessment)_3010 - Correlation: 0.9977337946782758\n",
      "370: FEAT_A: 1bb5fbdb FEAT_B: b2dba42b - Correlation: 0.9999521729413294\n",
      "371: FEAT_A: 1bb5fbdb FEAT_B: Sandcastle Builder (Activity)_3110 - Correlation: 1.0\n",
      "372: FEAT_A: 1bb5fbdb FEAT_B: Sandcastle Builder (Activity)_3010 - Correlation: 0.9999521729413294\n",
      "373: FEAT_A: df4940d3 FEAT_B: 67439901 - Correlation: 0.999935162643595\n",
      "374: FEAT_A: df4940d3 FEAT_B: Bottle Filler (Activity)_3110 - Correlation: 0.9999999999999999\n",
      "375: FEAT_A: df4940d3 FEAT_B: Bottle Filler (Activity)_3010 - Correlation: 0.999935162643595\n",
      "376: FEAT_A: eb2c19cd FEAT_B: Mushroom Sorter (Assessment)_4090 - Correlation: 1.0\n",
      "377: FEAT_A: 37937459 FEAT_B: Sandcastle Builder (Activity)_4090 - Correlation: 0.9999999999999998\n",
      "378: FEAT_A: 832735e1 FEAT_B: ab3136ba - Correlation: 0.9998637945770242\n",
      "379: FEAT_A: 832735e1 FEAT_B: Dino Dive_3110 - Correlation: 0.9998637945770242\n",
      "380: FEAT_A: 832735e1 FEAT_B: Dino Dive_3010 - Correlation: 1.0\n",
      "381: FEAT_A: b1d5101d FEAT_B: All Star Sorting_4095 - Correlation: 1.0\n",
      "382: FEAT_A: 29bdd9ba FEAT_B: Dino Dive_2000 - Correlation: 1.0\n",
      "383: FEAT_A: f6947f54 FEAT_B: Bird Measurer (Assessment)_2030 - Correlation: 1.0\n",
      "384: FEAT_A: 6cf7d25c FEAT_B: 15f99afc - Correlation: 0.9994848234397947\n",
      "385: FEAT_A: 6cf7d25c FEAT_B: Pan Balance_3010 - Correlation: 1.0\n",
      "386: FEAT_A: 6cf7d25c FEAT_B: Pan Balance_3110 - Correlation: 0.9994848234397947\n",
      "387: FEAT_A: f50fc6c1 FEAT_B: Watering Hole (Activity)_4021 - Correlation: 1.0\n",
      "388: FEAT_A: 6f8106d9 FEAT_B: Dino Drink_4090 - Correlation: 1.0\n",
      "389: FEAT_A: 04df9b66 FEAT_B: 5290eab1 - Correlation: 0.9998190477466209\n",
      "390: FEAT_A: 04df9b66 FEAT_B: Cauldron Filler (Assessment)_3020 - Correlation: 1.0\n",
      "391: FEAT_A: 04df9b66 FEAT_B: Cauldron Filler (Assessment)_3120 - Correlation: 0.9998190477466209\n",
      "392: FEAT_A: bcceccc6 FEAT_B: Air Show_4070 - Correlation: 1.0\n",
      "393: FEAT_A: d2278a3b FEAT_B: Bottle Filler (Activity)_2000 - Correlation: 1.0\n",
      "394: FEAT_A: f32856e4 FEAT_B: Leaf Leader_2020 - Correlation: 1.0\n",
      "395: FEAT_A: 29a42aea FEAT_B: Bubble Bath_4080 - Correlation: 1.0\n",
      "396: FEAT_A: b2e5b0f1 FEAT_B: b74258a0 - Correlation: 0.999849464604504\n",
      "397: FEAT_A: b2e5b0f1 FEAT_B: ecaab346 - Correlation: 0.999849464604504\n",
      "398: FEAT_A: b2e5b0f1 FEAT_B: Cart Balancer (Assessment)_2010 - Correlation: 1.0\n",
      "399: FEAT_A: b2e5b0f1 FEAT_B: Cart Balancer (Assessment)_2030 - Correlation: 0.999849464604504\n",
      "400: FEAT_A: b2e5b0f1 FEAT_B: Cart Balancer (Assessment)_3121 - Correlation: 0.999849464604504\n",
      "401: FEAT_A: ac92046e FEAT_B: d88e8f25 - Correlation: 0.9999763070332107\n",
      "402: FEAT_A: ac92046e FEAT_B: Scrub-A-Dub_3120 - Correlation: 1.0\n",
      "403: FEAT_A: ac92046e FEAT_B: Scrub-A-Dub_3020 - Correlation: 0.9999763070332107\n",
      "404: FEAT_A: 3edf6747 FEAT_B: Cauldron Filler (Assessment)_4035 - Correlation: 0.9999999999999999\n",
      "405: FEAT_A: 2a512369 FEAT_B: 33505eae - Correlation: 0.9994585292841954\n",
      "406: FEAT_A: 2a512369 FEAT_B: Leaf Leader_3010 - Correlation: 0.9994585292841954\n",
      "407: FEAT_A: 2a512369 FEAT_B: Leaf Leader_3110 - Correlation: 1.0\n",
      "408: FEAT_A: 4a09ace1 FEAT_B: Scrub-A-Dub_2083 - Correlation: 1.0\n",
      "409: FEAT_A: a2df0760 FEAT_B: Happy Camel_4035 - Correlation: 0.9999999999999999\n",
      "410: FEAT_A: 5859dfb6 FEAT_B: 90ea0bac - Correlation: 0.998105247261057\n",
      "411: FEAT_A: 5859dfb6 FEAT_B: Bubble Bath_3120 - Correlation: 0.9999999999999998\n",
      "412: FEAT_A: 5859dfb6 FEAT_B: Bubble Bath_3020 - Correlation: 0.998105247261057\n",
      "413: FEAT_A: bd612267 FEAT_B: Chest Sorter (Assessment)_4070 - Correlation: 0.9999999999999999\n",
      "414: FEAT_A: 5f5b2617 FEAT_B: Bottle Filler (Activity)_4080 - Correlation: 1.0\n",
      "415: FEAT_A: 9ee1c98c FEAT_B: Sandcastle Builder (Activity)_4021 - Correlation: 0.9999999999999999\n",
      "416: FEAT_A: 363c86c9 FEAT_B: Bug Measurer (Activity)_4035 - Correlation: 1.0\n",
      "417: FEAT_A: a5be6304 FEAT_B: Mushroom Sorter (Assessment)_2010 - Correlation: 1.0\n",
      "418: FEAT_A: 65abac75 FEAT_B: Air Show_4010 - Correlation: 1.0\n",
      "419: FEAT_A: 77c76bc5 FEAT_B: Cauldron Filler (Assessment)_4090 - Correlation: 1.0\n",
      "420: FEAT_A: d185d3ea FEAT_B: Chow Time_4035 - Correlation: 1.0\n",
      "421: FEAT_A: a16a373e FEAT_B: Bird Measurer (Assessment)_4070 - Correlation: 1.0\n",
      "422: FEAT_A: ec138c1c FEAT_B: Bird Measurer (Assessment)_2020 - Correlation: 0.9999999999999999\n",
      "423: FEAT_A: 2c4e6db0 FEAT_B: All Star Sorting_2020 - Correlation: 1.0\n",
      "424: FEAT_A: 6077cc36 FEAT_B: Bird Measurer (Assessment)_4080 - Correlation: 0.9999999999999998\n",
      "425: FEAT_A: ecc36b7f FEAT_B: Bubble Bath_4095 - Correlation: 1.0\n",
      "426: FEAT_A: 84538528 FEAT_B: Sandcastle Builder (Activity)_4020 - Correlation: 1.0\n",
      "427: FEAT_A: 69fdac0a FEAT_B: 8d7e386c - Correlation: 0.9996590210382708\n",
      "428: FEAT_A: 69fdac0a FEAT_B: Happy Camel_3110 - Correlation: 0.9999999999999999\n",
      "429: FEAT_A: 69fdac0a FEAT_B: Happy Camel_3010 - Correlation: 0.9996590210382708\n",
      "430: FEAT_A: 28f975ea FEAT_B: Air Show_4020 - Correlation: 1.0\n",
      "431: FEAT_A: 1cc7cfca FEAT_B: All Star Sorting_4030 - Correlation: 1.0\n",
      "432: FEAT_A: 00c73085 FEAT_B: Dino Dive_2030 - Correlation: 0.9999999999999999\n",
      "433: FEAT_A: 222660ff FEAT_B: 38074c54 - Correlation: 0.9999999999999998\n",
      "434: FEAT_A: 222660ff FEAT_B: Chest Sorter (Assessment)_2010 - Correlation: 0.9999999999999998\n",
      "435: FEAT_A: 222660ff FEAT_B: Chest Sorter (Assessment)_2030 - Correlation: 0.9999999999999998\n",
      "436: FEAT_A: 19967db1 FEAT_B: Chow Time_4090 - Correlation: 1.0\n",
      "437: FEAT_A: 01ca3a3c FEAT_B: Leaf Leader_4080 - Correlation: 0.9999999999999999\n",
      "438: FEAT_A: 8fee50e2 FEAT_B: Bird Measurer (Assessment)_4020 - Correlation: 1.0\n",
      "439: FEAT_A: 9c5ef70c FEAT_B: Pan Balance_2000 - Correlation: 1.0\n",
      "440: FEAT_A: f3cd5473 FEAT_B: Pan Balance_4070 - Correlation: 1.0\n",
      "441: FEAT_A: 5e109ec3 FEAT_B: Cart Balancer (Assessment)_4030 - Correlation: 1.0\n",
      "442: FEAT_A: c6971acf FEAT_B: Dino Drink_2060 - Correlation: 0.9999999999999999\n",
      "443: FEAT_A: a1e4395d FEAT_B: a52b92d5 - Correlation: 0.9991003891313368\n",
      "444: FEAT_A: a1e4395d FEAT_B: Mushroom Sorter (Assessment)_3010 - Correlation: 1.0\n",
      "445: FEAT_A: a1e4395d FEAT_B: Mushroom Sorter (Assessment)_3110 - Correlation: 0.9991003891313368\n",
      "446: FEAT_A: 0d18d96c FEAT_B: Mushroom Sorter (Assessment)_4035 - Correlation: 1.0\n",
      "447: FEAT_A: 53c6e11a FEAT_B: Leaf Leader_2075 - Correlation: 0.9999999999999999\n",
      "448: FEAT_A: 92687c59 FEAT_B: Scrub-A-Dub_4090 - Correlation: 1.0\n",
      "449: FEAT_A: 9e34ea74 FEAT_B: Egg Dropper (Activity)_4070 - Correlation: 1.0\n",
      "450: FEAT_A: 9ce586dd FEAT_B: Chest Sorter (Assessment)_4035 - Correlation: 1.0\n",
      "451: FEAT_A: 9d4e7b25 FEAT_B: Cart Balancer (Assessment)_4040 - Correlation: 1.0\n",
      "452: FEAT_A: 91561152 FEAT_B: Cauldron Filler (Assessment)_4025 - Correlation: 1.0\n",
      "453: FEAT_A: 15ba1109 FEAT_B: Air Show_2000 - Correlation: 1.0\n",
      "454: FEAT_A: c2baf0bd FEAT_B: Happy Camel_2020 - Correlation: 1.0\n",
      "455: FEAT_A: 5a848010 FEAT_B: Scrub-A-Dub_2080 - Correlation: 1.0\n",
      "456: FEAT_A: 8af75982 FEAT_B: Happy Camel_4020 - Correlation: 1.0\n",
      "457: FEAT_A: bc8f2793 FEAT_B: Pan Balance_4035 - Correlation: 0.9999999999999999\n",
      "458: FEAT_A: d9c005dd FEAT_B: Happy Camel_2000 - Correlation: 1.0\n",
      "459: FEAT_A: a5e9da97 FEAT_B: Pan Balance_4100 - Correlation: 1.0\n",
      "460: FEAT_A: 562cec5f FEAT_B: Chest Sorter (Assessment)_4025 - Correlation: 1.0\n",
      "461: FEAT_A: 25fa8af4 FEAT_B: Mushroom Sorter (Assessment)_4100 - Correlation: 1.0\n",
      "462: FEAT_A: c7f7f0e1 FEAT_B: Bug Measurer (Activity)_2000 - Correlation: 1.0\n",
      "463: FEAT_A: 08ff79ad FEAT_B: Egg Dropper (Activity)_4090 - Correlation: 1.0\n",
      "464: FEAT_A: 0db6d71d FEAT_B: Chest Sorter (Assessment)_4020 - Correlation: 1.0\n",
      "465: FEAT_A: 1375ccb7 FEAT_B: bdf49a58 - Correlation: 0.9993801763820348\n",
      "466: FEAT_A: 1375ccb7 FEAT_B: Bird Measurer (Assessment)_3110 - Correlation: 0.9993801763820348\n",
      "467: FEAT_A: 1375ccb7 FEAT_B: Bird Measurer (Assessment)_3010 - Correlation: 1.0\n",
      "468: FEAT_A: c0415e5c FEAT_B: Dino Dive_4020 - Correlation: 1.0\n",
      "469: FEAT_A: cb1178ad FEAT_B: Chest Sorter (Assessment)_4090 - Correlation: 1.0\n",
      "470: FEAT_A: 461eace6 FEAT_B: Egg Dropper (Activity)_4020 - Correlation: 1.0\n",
      "471: FEAT_A: e7561dd2 FEAT_B: Pan Balance_4025 - Correlation: 0.9999999999999998\n",
      "472: FEAT_A: 06372577 FEAT_B: Air Show_2060 - Correlation: 1.0\n",
      "473: FEAT_A: 6088b756 FEAT_B: Dino Dive_2070 - Correlation: 1.0\n",
      "474: FEAT_A: d3640339 FEAT_B: Dino Dive_4090 - Correlation: 1.0\n",
      "475: FEAT_A: 4d6737eb FEAT_B: Dino Drink_2070 - Correlation: 1.0\n",
      "476: FEAT_A: 884228c8 FEAT_B: Fireworks (Activity)_4070 - Correlation: 1.0\n",
      "477: FEAT_A: fd20ea40 FEAT_B: Leaf Leader_4010 - Correlation: 0.9999999999999998\n",
      "478: FEAT_A: 3d0b9317 FEAT_B: Chest Sorter (Assessment)_4040 - Correlation: 1.0\n",
      "479: FEAT_A: 828e68f9 FEAT_B: Cart Balancer (Assessment)_3110 - Correlation: 1.0\n",
      "480: FEAT_A: dcb55a27 FEAT_B: Air Show_4110 - Correlation: 1.0\n",
      "481: FEAT_A: d88ca108 FEAT_B: Air Show_2070 - Correlation: 1.0\n",
      "482: FEAT_A: 90efca10 FEAT_B: Bottle Filler (Activity)_4020 - Correlation: 1.0\n",
      "483: FEAT_A: 6043a2b4 FEAT_B: All Star Sorting_4090 - Correlation: 0.9999999999999999\n",
      "484: FEAT_A: 0086365d FEAT_B: Pan Balance_4010 - Correlation: 0.9999999999999999\n",
      "485: FEAT_A: f5b8c21a FEAT_B: 9b4001e4 - Correlation: 0.9976837802056778\n",
      "486: FEAT_A: f5b8c21a FEAT_B: 58a0de5c - Correlation: 0.9977888184537717\n",
      "487: FEAT_A: f5b8c21a FEAT_B: Air Show_3121 - Correlation: 0.9977888184537717\n",
      "488: FEAT_A: f5b8c21a FEAT_B: Air Show_3021 - Correlation: 0.9976837802056778\n",
      "489: FEAT_A: f5b8c21a FEAT_B: Air Show_2030 - Correlation: 1.0\n",
      "490: FEAT_A: 8d84fa81 FEAT_B: Bubble Bath_4010 - Correlation: 1.0\n",
      "491: FEAT_A: 392e14df FEAT_B: Cauldron Filler (Assessment)_4100 - Correlation: 1.0\n",
      "492: FEAT_A: 5be391b5 FEAT_B: Dino Drink_4010 - Correlation: 1.0\n",
      "493: FEAT_A: 51311d7a FEAT_B: Dino Drink_2000 - Correlation: 1.0\n",
      "494: FEAT_A: 2b058fe3 FEAT_B: Cauldron Filler (Assessment)_2010 - Correlation: 1.0\n",
      "495: FEAT_A: 76babcde FEAT_B: Dino Dive_4070 - Correlation: 1.0\n",
      "496: FEAT_A: cc5087a3 FEAT_B: Crystals Rule_4010 - Correlation: 1.0\n",
      "497: FEAT_A: d3f1e122 FEAT_B: Bottle Filler (Activity)_4035 - Correlation: 1.0\n",
      "498: FEAT_A: cfbd47c8 FEAT_B: Chow Time_4030 - Correlation: 1.0\n",
      "499: FEAT_A: 4bb2f698 FEAT_B: Chicken Balancer (Activity)_4070 - Correlation: 0.9999999999999998\n",
      "500: FEAT_A: f54238ee FEAT_B: Fireworks (Activity)_4090 - Correlation: 1.0\n",
      "501: FEAT_A: fbaf3456 FEAT_B: Mushroom Sorter (Assessment)_4030 - Correlation: 0.9999999999999999\n",
      "502: FEAT_A: 13f56524 FEAT_B: Mushroom Sorter (Assessment)_4080 - Correlation: 1.0\n",
      "503: FEAT_A: 1cf54632 FEAT_B: Bubble Bath_2000 - Correlation: 0.9999999999999999\n",
      "504: FEAT_A: 85d1b0de FEAT_B: Chicken Balancer (Activity)_4090 - Correlation: 1.0\n",
      "505: FEAT_A: 804ee27f FEAT_B: Pan Balance_4020 - Correlation: 1.0\n",
      "506: FEAT_A: 7ec0c298 FEAT_B: Chow Time_3010 - Correlation: 0.9999999999999999\n",
      "507: FEAT_A: 0d1da71f FEAT_B: Chow Time_3110 - Correlation: 1.0\n",
      "508: FEAT_A: 7d093bf9 FEAT_B: Chow Time_2000 - Correlation: 0.9999999999999999\n",
      "509: FEAT_A: 022b4259 FEAT_B: Bug Measurer (Activity)_4025 - Correlation: 0.9999999999999998\n",
      "510: FEAT_A: 5d042115 FEAT_B: Flower Waterer (Activity)_4030 - Correlation: 1.0\n",
      "511: FEAT_A: 4901243f FEAT_B: Fireworks (Activity)_2000 - Correlation: 1.0\n",
      "512: FEAT_A: f56e0afc FEAT_B: Bird Measurer (Assessment)_2000 - Correlation: 1.0\n",
      "513: FEAT_A: 2dc29e21 FEAT_B: All Star Sorting_4020 - Correlation: 1.0\n"
     ]
    }
   ],
   "source": [
    "to_remove = remove_correlated_features(reduce_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 427 features in LGBM\n",
      "Training with 426 features in LR\n",
      "Training with 426 features in NN\n"
     ]
    }
   ],
   "source": [
    "features = [col for col in features if col not in to_remove]\n",
    "features = [col for col in features if col not in ['Heavy, Heavier, Heaviest_2000', 'Heavy, Heavier, Heaviest']]\n",
    "features.append('installation_id')\n",
    "print('Training with {} features in LGBM'.format(len(features)))\n",
    "\n",
    "lr_features = [col for col in lr_features if col not in to_remove]\n",
    "lr_features = [col for col in lr_features if col not in ['Heavy, Heavier, Heaviest_2000', 'Heavy, Heavier, Heaviest', \"session_title\"]]\n",
    "nn_features = [col for col in nn_features if col not in to_remove]\n",
    "nn_features = [col for col in nn_features if col not in ['Heavy, Heavier, Heaviest_2000', 'Heavy, Heavier, Heaviest', \"session_title\"]]\n",
    "lr_features.append('installation_id')\n",
    "nn_features.append('installation_id')\n",
    "print('Training with {} features in LR'.format(len(lr_features)))\n",
    "print('Training with {} features in NN'.format(len(nn_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_Cart Balancer (Assessment)\n",
      "misses\n",
      "6aeafed4\n",
      "ecc6157f\n",
      "611485c5\n",
      "e4d32835\n",
      "0ce40006\n",
      "bfc77bd6\n",
      "119b5b02\n",
      "7fd1ac25\n",
      "1b54d27f\n",
      "dcb1663e\n",
      "a8cc6fec\n",
      "2ec694de\n",
      "ab4ec3a4\n",
      "eb2c19cd\n",
      "29a42aea\n",
      "01ca3a3c\n",
      "17ca3959\n",
      "4074bac2\n",
      "5dc079d8\n",
      "13f56524\n",
      "003cd2ee\n",
      "Air Show_4080\n",
      "Pan Balance_2010\n",
      "Bottle Filler (Activity)_2010\n",
      "Scrub-A-Dub_4080\n",
      "Crystals Rule_2010\n",
      "Sandcastle Builder (Activity)_2010\n"
     ]
    }
   ],
   "source": [
    "to_exclude, ajusted_test = exclude(reduce_train, reduce_test, features)\n",
    "features = [col for col in features if col not in to_exclude]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modelling and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "used validation data:  721\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870306\tvalid_1's rmse: 1.06162\n",
      "[200]\ttraining's rmse: 0.791709\tvalid_1's rmse: 1.0591\n",
      "[300]\ttraining's rmse: 0.730245\tvalid_1's rmse: 1.05992\n",
      "Early stopping, best iteration is:\n",
      "[248]\ttraining's rmse: 0.760546\tvalid_1's rmse: 1.05776\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.868974\tvalid_1's rmse: 1.04824\n",
      "[200]\ttraining's rmse: 0.791071\tvalid_1's rmse: 1.04825\n",
      "Early stopping, best iteration is:\n",
      "[177]\ttraining's rmse: 0.806403\tvalid_1's rmse: 1.0466\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870179\tvalid_1's rmse: 1.0058\n",
      "Early stopping, best iteration is:\n",
      "[80]\ttraining's rmse: 0.890744\tvalid_1's rmse: 1.00387\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.865543\tvalid_1's rmse: 1.09874\n",
      "[200]\ttraining's rmse: 0.786239\tvalid_1's rmse: 1.10234\n",
      "Early stopping, best iteration is:\n",
      "[141]\ttraining's rmse: 0.8301\tvalid_1's rmse: 1.09772\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.858832\tvalid_1's rmse: 1.05483\n",
      "Early stopping, best iteration is:\n",
      "[99]\ttraining's rmse: 0.859751\tvalid_1's rmse: 1.05436\n",
      "Our oof rmse score is: 1.052497920656123\n",
      "Our oof cohen kappa score is: 0.5333036538638295\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870306\tvalid_1's rmse: 1.04735\n",
      "[200]\ttraining's rmse: 0.791709\tvalid_1's rmse: 1.0441\n",
      "Early stopping, best iteration is:\n",
      "[193]\ttraining's rmse: 0.796445\tvalid_1's rmse: 1.04326\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.868974\tvalid_1's rmse: 1.00079\n",
      "[200]\ttraining's rmse: 0.791071\tvalid_1's rmse: 0.993593\n",
      "[300]\ttraining's rmse: 0.73196\tvalid_1's rmse: 0.99453\n",
      "Early stopping, best iteration is:\n",
      "[249]\ttraining's rmse: 0.760025\tvalid_1's rmse: 0.99249\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870179\tvalid_1's rmse: 1.01163\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttraining's rmse: 0.901881\tvalid_1's rmse: 1.00688\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.865543\tvalid_1's rmse: 1.05454\n",
      "Early stopping, best iteration is:\n",
      "[88]\ttraining's rmse: 0.876913\tvalid_1's rmse: 1.05371\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.858832\tvalid_1's rmse: 1.07393\n",
      "[200]\ttraining's rmse: 0.780875\tvalid_1's rmse: 1.07892\n",
      "Early stopping, best iteration is:\n",
      "[124]\ttraining's rmse: 0.83727\tvalid_1's rmse: 1.07231\n",
      "Our oof rmse score is: 1.0341788597151582\n",
      "Our oof cohen kappa score is: 0.5404966741476249\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870306\tvalid_1's rmse: 1.06117\n",
      "[200]\ttraining's rmse: 0.791709\tvalid_1's rmse: 1.05934\n",
      "[300]\ttraining's rmse: 0.730245\tvalid_1's rmse: 1.06291\n",
      "Early stopping, best iteration is:\n",
      "[218]\ttraining's rmse: 0.779333\tvalid_1's rmse: 1.0587\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.868974\tvalid_1's rmse: 1.0013\n",
      "[200]\ttraining's rmse: 0.791071\tvalid_1's rmse: 0.999971\n",
      "[300]\ttraining's rmse: 0.73196\tvalid_1's rmse: 0.99938\n",
      "Early stopping, best iteration is:\n",
      "[204]\ttraining's rmse: 0.788218\tvalid_1's rmse: 0.998461\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870179\tvalid_1's rmse: 1.01919\n",
      "Early stopping, best iteration is:\n",
      "[81]\ttraining's rmse: 0.889673\tvalid_1's rmse: 1.01777\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.865543\tvalid_1's rmse: 1.05222\n",
      "[200]\ttraining's rmse: 0.786239\tvalid_1's rmse: 1.05115\n",
      "Early stopping, best iteration is:\n",
      "[158]\ttraining's rmse: 0.816499\tvalid_1's rmse: 1.04662\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.858832\tvalid_1's rmse: 1.01068\n",
      "[200]\ttraining's rmse: 0.780875\tvalid_1's rmse: 1.0199\n",
      "Early stopping, best iteration is:\n",
      "[101]\ttraining's rmse: 0.857904\tvalid_1's rmse: 1.01004\n",
      "Our oof rmse score is: 1.0265602838563732\n",
      "Our oof cohen kappa score is: 0.557528541608324\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870306\tvalid_1's rmse: 1.04543\n",
      "[200]\ttraining's rmse: 0.791709\tvalid_1's rmse: 1.0444\n",
      "Early stopping, best iteration is:\n",
      "[193]\ttraining's rmse: 0.796445\tvalid_1's rmse: 1.04366\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.868974\tvalid_1's rmse: 1.00174\n",
      "[200]\ttraining's rmse: 0.791071\tvalid_1's rmse: 1.0011\n",
      "Early stopping, best iteration is:\n",
      "[178]\ttraining's rmse: 0.805495\tvalid_1's rmse: 0.999617\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870179\tvalid_1's rmse: 1.00254\n",
      "[200]\ttraining's rmse: 0.791546\tvalid_1's rmse: 1.00107\n",
      "Early stopping, best iteration is:\n",
      "[183]\ttraining's rmse: 0.802999\tvalid_1's rmse: 1.00013\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.865543\tvalid_1's rmse: 1.08637\n",
      "[200]\ttraining's rmse: 0.786239\tvalid_1's rmse: 1.08868\n",
      "Early stopping, best iteration is:\n",
      "[157]\ttraining's rmse: 0.817073\tvalid_1's rmse: 1.0858\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.858832\tvalid_1's rmse: 1.04948\n",
      "[200]\ttraining's rmse: 0.780875\tvalid_1's rmse: 1.05467\n",
      "Early stopping, best iteration is:\n",
      "[101]\ttraining's rmse: 0.857904\tvalid_1's rmse: 1.04893\n",
      "Our oof rmse score is: 1.0361637088436488\n",
      "Our oof cohen kappa score is: 0.5357295779563778\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870306\tvalid_1's rmse: 1.05919\n",
      "[200]\ttraining's rmse: 0.791709\tvalid_1's rmse: 1.05575\n",
      "Early stopping, best iteration is:\n",
      "[193]\ttraining's rmse: 0.796445\tvalid_1's rmse: 1.05487\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.868974\tvalid_1's rmse: 1.00982\n",
      "[200]\ttraining's rmse: 0.791071\tvalid_1's rmse: 1.00502\n",
      "[300]\ttraining's rmse: 0.73196\tvalid_1's rmse: 1.00514\n",
      "Early stopping, best iteration is:\n",
      "[249]\ttraining's rmse: 0.760025\tvalid_1's rmse: 1.00341\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870179\tvalid_1's rmse: 1.02241\n",
      "[200]\ttraining's rmse: 0.791546\tvalid_1's rmse: 1.01927\n",
      "[300]\ttraining's rmse: 0.731719\tvalid_1's rmse: 1.02354\n",
      "Early stopping, best iteration is:\n",
      "[209]\ttraining's rmse: 0.78555\tvalid_1's rmse: 1.01872\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.865543\tvalid_1's rmse: 1.06721\n",
      "[200]\ttraining's rmse: 0.786239\tvalid_1's rmse: 1.06637\n",
      "Early stopping, best iteration is:\n",
      "[157]\ttraining's rmse: 0.817073\tvalid_1's rmse: 1.06189\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.858832\tvalid_1's rmse: 1.04003\n",
      "[200]\ttraining's rmse: 0.780875\tvalid_1's rmse: 1.04394\n",
      "Early stopping, best iteration is:\n",
      "[128]\ttraining's rmse: 0.833672\tvalid_1's rmse: 1.03877\n",
      "Our oof rmse score is: 1.0357697787209357\n",
      "Our oof cohen kappa score is: 0.5531116324469374\n",
      "Our mean rmse score is:  1.0370341103584477\n",
      "Our mean cohen kappa score is:  0.5440340160046186\n"
     ]
    }
   ],
   "source": [
    "# train 5 times because the evaluation and training data change with the randomness\n",
    "y_pred_1, oof_rmse_score_1, oof_cohen_score_1 = run_lgb(reduce_train, ajusted_test, features)\n",
    "y_pred_2, oof_rmse_score_2, oof_cohen_score_2 = run_lgb(reduce_train, ajusted_test, features)\n",
    "y_pred_3, oof_rmse_score_3, oof_cohen_score_3 = run_lgb(reduce_train, ajusted_test, features)\n",
    "y_pred_4, oof_rmse_score_4, oof_cohen_score_4 = run_lgb(reduce_train, ajusted_test, features)\n",
    "y_pred_5, oof_rmse_score_5, oof_cohen_score_5 = run_lgb(reduce_train, ajusted_test, features)\n",
    "mean_rmse = (oof_rmse_score_1 + oof_rmse_score_2 + oof_rmse_score_3 + oof_rmse_score_4 + oof_rmse_score_5) / 5\n",
    "mean_cohen_kappa = (oof_cohen_score_1 + oof_cohen_score_2 + oof_cohen_score_3 + oof_cohen_score_4 + oof_cohen_score_5) / 5\n",
    "print('Our mean rmse score is: ', mean_rmse)\n",
    "print('Our mean cohen kappa score is: ', mean_cohen_kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression and neutal network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_std, test_std = standardize_data(reduce_train, ajusted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "used validation data:  721\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Our oof rmse score is: 196862575.0537234\n",
      "Our oof cohen kappa score is: 0.4974067691952717\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Our oof rmse score is: 1.124965311999468\n",
      "Our oof cohen kappa score is: 0.4970729427453816\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Our oof rmse score is: 1.1293442011697825\n",
      "Our oof cohen kappa score is: 0.48759707230625504\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Our oof rmse score is: 196862575.0537234\n",
      "Our oof cohen kappa score is: 0.49424345588672447\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Our oof rmse score is: 196862575.0537234\n",
      "Our oof cohen kappa score is: 0.5140669706166923\n",
      "Our mean rmse score is:  118117545.48309593\n",
      "Our mean cohen kappa score is:  0.498077442150065\n"
     ]
    }
   ],
   "source": [
    "for i in train_std.columns:\n",
    "    if \"session_title\" in str(i):\n",
    "        lr_features.append(i)\n",
    "y_pred_1_lr, oof_rmse_score_1_lr, oof_cohen_score_1_lr = run_lr(train_std, test_std, lr_features)\n",
    "y_pred_2_lr, oof_rmse_score_2_lr, oof_cohen_score_2_lr = run_lr(train_std, test_std, lr_features)\n",
    "y_pred_3_lr, oof_rmse_score_3_lr, oof_cohen_score_3_lr = run_lr(train_std, test_std, lr_features)\n",
    "y_pred_4_lr, oof_rmse_score_4_lr, oof_cohen_score_4_lr = run_lr(train_std, test_std, lr_features)\n",
    "y_pred_5_lr, oof_rmse_score_5_lr, oof_cohen_score_5_lr = run_lr(train_std, test_std, lr_features)\n",
    "mean_rmse_lr = (oof_rmse_score_1_lr + oof_rmse_score_2_lr + oof_rmse_score_3_lr + oof_rmse_score_4_lr + oof_rmse_score_5_lr) / 5\n",
    "mean_cohen_kappa_lr = (oof_cohen_score_1_lr + oof_cohen_score_2_lr + oof_cohen_score_3_lr + oof_cohen_score_4_lr + oof_cohen_score_5_lr) / 5\n",
    "print('Our mean rmse score is: ', mean_rmse_lr)\n",
    "print('Our mean cohen kappa score is: ', mean_cohen_kappa_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "used validation data:  721\n",
      "Train on 14152 samples, validate on 721 samples\n",
      "Epoch 1/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.4027\n",
      "Epoch 00001: val_loss improved from inf to 1.24632, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 241us/sample - loss: 1.4021 - val_loss: 1.2463\n",
      "Epoch 2/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1623\n",
      "Epoch 00002: val_loss improved from 1.24632 to 1.23167, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.1622 - val_loss: 1.2317\n",
      "Epoch 3/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1115\n",
      "Epoch 00003: val_loss did not improve from 1.23167\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.1112 - val_loss: 1.2820\n",
      "Epoch 4/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0946\n",
      "Epoch 00004: val_loss improved from 1.23167 to 1.22336, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 1.0962 - val_loss: 1.2234\n",
      "Epoch 5/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0730\n",
      "Epoch 00005: val_loss improved from 1.22336 to 1.19131, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 1.0731 - val_loss: 1.1913\n",
      "Epoch 6/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0728\n",
      "Epoch 00006: val_loss did not improve from 1.19131\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.0710 - val_loss: 1.2018\n",
      "Epoch 7/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0534\n",
      "Epoch 00007: val_loss did not improve from 1.19131\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.0538 - val_loss: 1.2196\n",
      "Epoch 8/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0464\n",
      "Epoch 00008: val_loss did not improve from 1.19131\n",
      "14152/14152 [==============================] - 2s 168us/sample - loss: 1.0457 - val_loss: 1.1992\n",
      "Epoch 9/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0448\n",
      "Epoch 00009: val_loss did not improve from 1.19131\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 1.0436 - val_loss: 1.2100\n",
      "Epoch 10/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0228\n",
      "Epoch 00010: val_loss did not improve from 1.19131\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 1.0205 - val_loss: 1.1925\n",
      "Epoch 11/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0305\n",
      "Epoch 00011: val_loss improved from 1.19131 to 1.17347, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0325 - val_loss: 1.1735\n",
      "Epoch 12/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0178\n",
      "Epoch 00012: val_loss did not improve from 1.17347\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.0151 - val_loss: 1.2305\n",
      "Epoch 13/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0147\n",
      "Epoch 00013: val_loss did not improve from 1.17347\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.0146 - val_loss: 1.1751\n",
      "Epoch 14/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0093\n",
      "Epoch 00014: val_loss did not improve from 1.17347\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 1.0088 - val_loss: 1.1735\n",
      "Epoch 15/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0042\n",
      "Epoch 00015: val_loss did not improve from 1.17347\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0034 - val_loss: 1.2137\n",
      "Epoch 16/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0036\n",
      "Epoch 00016: val_loss did not improve from 1.17347\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.0027 - val_loss: 1.1989\n",
      "Epoch 17/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9943\n",
      "Epoch 00017: val_loss did not improve from 1.17347\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 0.9932 - val_loss: 1.1932\n",
      "Epoch 18/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9802\n",
      "Epoch 00018: val_loss did not improve from 1.17347\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 0.9816 - val_loss: 1.2037\n",
      "Epoch 19/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9791\n",
      "Epoch 00019: val_loss did not improve from 1.17347\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 0.9789 - val_loss: 1.1811\n",
      "Epoch 20/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9756\n",
      "Epoch 00020: val_loss did not improve from 1.17347\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 0.9762 - val_loss: 1.2076\n",
      "Epoch 21/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9741\n",
      "Epoch 00021: val_loss did not improve from 1.17347\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 0.9722 - val_loss: 1.2063\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Train on 14152 samples, validate on 722 samples\n",
      "Epoch 1/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.3199\n",
      "Epoch 00001: val_loss improved from inf to 1.17465, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 228us/sample - loss: 1.3196 - val_loss: 1.1747\n",
      "Epoch 2/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.1379\n",
      "Epoch 00002: val_loss improved from 1.17465 to 1.15624, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.1368 - val_loss: 1.1562\n",
      "Epoch 3/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.1139\n",
      "Epoch 00003: val_loss did not improve from 1.15624\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.1139 - val_loss: 1.1676\n",
      "Epoch 4/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0763\n",
      "Epoch 00004: val_loss improved from 1.15624 to 1.11966, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 1.0750 - val_loss: 1.1197\n",
      "Epoch 5/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0670\n",
      "Epoch 00005: val_loss did not improve from 1.11966\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 1.0662 - val_loss: 1.1811\n",
      "Epoch 6/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0522\n",
      "Epoch 00006: val_loss did not improve from 1.11966\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0510 - val_loss: 1.1479\n",
      "Epoch 7/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0446\n",
      "Epoch 00007: val_loss improved from 1.11966 to 1.11786, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0464 - val_loss: 1.1179\n",
      "Epoch 8/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0315\n",
      "Epoch 00008: val_loss did not improve from 1.11786\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.0327 - val_loss: 1.1853\n",
      "Epoch 9/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0187\n",
      "Epoch 00009: val_loss improved from 1.11786 to 1.10926, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 184us/sample - loss: 1.0201 - val_loss: 1.1093\n",
      "Epoch 10/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0227\n",
      "Epoch 00010: val_loss did not improve from 1.10926\n",
      "14152/14152 [==============================] - 3s 178us/sample - loss: 1.0226 - val_loss: 1.1644\n",
      "Epoch 11/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0308\n",
      "Epoch 00011: val_loss did not improve from 1.10926\n",
      "14152/14152 [==============================] - 3s 183us/sample - loss: 1.0304 - val_loss: 1.1604\n",
      "Epoch 12/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0108\n",
      "Epoch 00012: val_loss did not improve from 1.10926\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 1.0103 - val_loss: 1.1133\n",
      "Epoch 13/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0063\n",
      "Epoch 00013: val_loss did not improve from 1.10926\n",
      "14152/14152 [==============================] - 2s 170us/sample - loss: 1.0079 - val_loss: 1.1462\n",
      "Epoch 14/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9984\n",
      "Epoch 00014: val_loss did not improve from 1.10926\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 0.9985 - val_loss: 1.1234\n",
      "Epoch 15/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0046\n",
      "Epoch 00015: val_loss did not improve from 1.10926\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0047 - val_loss: 1.1334\n",
      "Epoch 16/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9846\n",
      "Epoch 00016: val_loss did not improve from 1.10926\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 0.9871 - val_loss: 1.1348\n",
      "Epoch 17/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9960\n",
      "Epoch 00017: val_loss did not improve from 1.10926\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 0.9931 - val_loss: 1.1340\n",
      "Epoch 18/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9969\n",
      "Epoch 00018: val_loss did not improve from 1.10926\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 0.9981 - val_loss: 1.1392\n",
      "Epoch 19/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9975\n",
      "Epoch 00019: val_loss did not improve from 1.10926\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 0.9970 - val_loss: 1.1422\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Train on 14152 samples, validate on 723 samples\n",
      "Epoch 1/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.3101\n",
      "Epoch 00001: val_loss improved from inf to 1.21941, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 4s 257us/sample - loss: 1.3098 - val_loss: 1.2194\n",
      "Epoch 2/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.1407\n",
      "Epoch 00002: val_loss improved from 1.21941 to 1.14179, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.1413 - val_loss: 1.1418\n",
      "Epoch 3/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.1138\n",
      "Epoch 00003: val_loss did not improve from 1.14179\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.1133 - val_loss: 1.1422\n",
      "Epoch 4/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0880\n",
      "Epoch 00004: val_loss improved from 1.14179 to 1.11673, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0880 - val_loss: 1.1167\n",
      "Epoch 5/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0686\n",
      "Epoch 00005: val_loss did not improve from 1.11673\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0669 - val_loss: 1.1514\n",
      "Epoch 6/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0642\n",
      "Epoch 00006: val_loss did not improve from 1.11673\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0646 - val_loss: 1.1272\n",
      "Epoch 7/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0650\n",
      "Epoch 00007: val_loss did not improve from 1.11673\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0612 - val_loss: 1.1376\n",
      "Epoch 8/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0388\n",
      "Epoch 00008: val_loss did not improve from 1.11673\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0388 - val_loss: 1.1340\n",
      "Epoch 9/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0441\n",
      "Epoch 00009: val_loss did not improve from 1.11673\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.0432 - val_loss: 1.1390\n",
      "Epoch 10/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0366\n",
      "Epoch 00010: val_loss did not improve from 1.11673\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0378 - val_loss: 1.1308\n",
      "Epoch 11/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0399\n",
      "Epoch 00011: val_loss did not improve from 1.11673\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0392 - val_loss: 1.1462\n",
      "Epoch 12/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0280\n",
      "Epoch 00012: val_loss improved from 1.11673 to 1.09412, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 178us/sample - loss: 1.0292 - val_loss: 1.0941\n",
      "Epoch 13/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0189\n",
      "Epoch 00013: val_loss did not improve from 1.09412\n",
      "14152/14152 [==============================] - 3s 183us/sample - loss: 1.0195 - val_loss: 1.1010\n",
      "Epoch 14/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0147\n",
      "Epoch 00014: val_loss improved from 1.09412 to 1.08771, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 168us/sample - loss: 1.0142 - val_loss: 1.0877\n",
      "Epoch 15/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0089\n",
      "Epoch 00015: val_loss did not improve from 1.08771\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0090 - val_loss: 1.0953\n",
      "Epoch 16/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0037\n",
      "Epoch 00016: val_loss did not improve from 1.08771\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0048 - val_loss: 1.1286\n",
      "Epoch 17/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9964\n",
      "Epoch 00017: val_loss did not improve from 1.08771\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 0.9966 - val_loss: 1.1142\n",
      "Epoch 18/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9956\n",
      "Epoch 00018: val_loss did not improve from 1.08771\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 0.9960 - val_loss: 1.0879\n",
      "Epoch 19/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9847\n",
      "Epoch 00019: val_loss did not improve from 1.08771\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 0.9839 - val_loss: 1.1527\n",
      "Epoch 20/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9819\n",
      "Epoch 00020: val_loss did not improve from 1.08771\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 0.9838 - val_loss: 1.1025\n",
      "Epoch 21/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9961\n",
      "Epoch 00021: val_loss did not improve from 1.08771\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 0.9946 - val_loss: 1.0921\n",
      "Epoch 22/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9709\n",
      "Epoch 00022: val_loss did not improve from 1.08771\n",
      "14152/14152 [==============================] - 3s 218us/sample - loss: 0.9743 - val_loss: 1.0921\n",
      "Epoch 23/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9761\n",
      "Epoch 00023: val_loss did not improve from 1.08771\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 0.9778 - val_loss: 1.1175\n",
      "Epoch 24/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9741\n",
      "Epoch 00024: val_loss did not improve from 1.08771\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 0.9731 - val_loss: 1.0967\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Train on 14152 samples, validate on 724 samples\n",
      "Epoch 1/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.4118\n",
      "Epoch 00001: val_loss improved from inf to 1.26528, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 234us/sample - loss: 1.4067 - val_loss: 1.2653\n",
      "Epoch 2/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1528\n",
      "Epoch 00002: val_loss did not improve from 1.26528\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.1532 - val_loss: 1.2880\n",
      "Epoch 3/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.1064\n",
      "Epoch 00003: val_loss improved from 1.26528 to 1.26246, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 1.1043 - val_loss: 1.2625\n",
      "Epoch 4/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0832\n",
      "Epoch 00004: val_loss improved from 1.26246 to 1.23426, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.0834 - val_loss: 1.2343\n",
      "Epoch 5/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0579\n",
      "Epoch 00005: val_loss did not improve from 1.23426\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0579 - val_loss: 1.3214\n",
      "Epoch 6/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0528\n",
      "Epoch 00006: val_loss did not improve from 1.23426\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0522 - val_loss: 1.2729\n",
      "Epoch 7/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0363\n",
      "Epoch 00007: val_loss improved from 1.23426 to 1.21276, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.0380 - val_loss: 1.2128\n",
      "Epoch 8/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0300\n",
      "Epoch 00008: val_loss did not improve from 1.21276\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0324 - val_loss: 1.2390\n",
      "Epoch 9/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0209\n",
      "Epoch 00009: val_loss did not improve from 1.21276\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 1.0229 - val_loss: 1.2325\n",
      "Epoch 10/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0286\n",
      "Epoch 00010: val_loss did not improve from 1.21276\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0285 - val_loss: 1.3089\n",
      "Epoch 11/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0173\n",
      "Epoch 00011: val_loss improved from 1.21276 to 1.20638, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.0184 - val_loss: 1.2064\n",
      "Epoch 12/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0099\n",
      "Epoch 00012: val_loss did not improve from 1.20638\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.0083 - val_loss: 1.3409\n",
      "Epoch 13/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0138\n",
      "Epoch 00013: val_loss did not improve from 1.20638\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0092 - val_loss: 1.3143\n",
      "Epoch 14/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0089\n",
      "Epoch 00014: val_loss improved from 1.20638 to 1.19701, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0116 - val_loss: 1.1970\n",
      "Epoch 15/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0019\n",
      "Epoch 00015: val_loss did not improve from 1.19701\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 1.0012 - val_loss: 1.2069\n",
      "Epoch 16/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0046\n",
      "Epoch 00016: val_loss did not improve from 1.19701\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0078 - val_loss: 1.2786\n",
      "Epoch 17/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9984\n",
      "Epoch 00017: val_loss did not improve from 1.19701\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 0.9993 - val_loss: 1.2280\n",
      "Epoch 18/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9811\n",
      "Epoch 00018: val_loss did not improve from 1.19701\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 0.9812 - val_loss: 1.2233\n",
      "Epoch 19/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9795\n",
      "Epoch 00019: val_loss did not improve from 1.19701\n",
      "14152/14152 [==============================] - 2s 168us/sample - loss: 0.9787 - val_loss: 1.2652\n",
      "Epoch 20/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9754\n",
      "Epoch 00020: val_loss did not improve from 1.19701\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.9778 - val_loss: 1.2166\n",
      "Epoch 21/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9751\n",
      "Epoch 00021: val_loss did not improve from 1.19701\n",
      "14152/14152 [==============================] - 3s 180us/sample - loss: 0.9731 - val_loss: 1.2032\n",
      "Epoch 22/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9798\n",
      "Epoch 00022: val_loss did not improve from 1.19701\n",
      "14152/14152 [==============================] - 2s 174us/sample - loss: 0.9800 - val_loss: 1.2619\n",
      "Epoch 23/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9584\n",
      "Epoch 00023: val_loss did not improve from 1.19701\n",
      "14152/14152 [==============================] - 3s 182us/sample - loss: 0.9604 - val_loss: 1.2326\n",
      "Epoch 24/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9629\n",
      "Epoch 00024: val_loss did not improve from 1.19701\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 0.9627 - val_loss: 1.2194\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Train on 14152 samples, validate on 724 samples\n",
      "Epoch 1/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.3499\n",
      "Epoch 00001: val_loss improved from inf to 1.18457, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 230us/sample - loss: 1.3463 - val_loss: 1.1846\n",
      "Epoch 2/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1288\n",
      "Epoch 00002: val_loss did not improve from 1.18457\n",
      "14152/14152 [==============================] - 2s 169us/sample - loss: 1.1277 - val_loss: 1.1984\n",
      "Epoch 3/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1058\n",
      "Epoch 00003: val_loss did not improve from 1.18457\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 1.1040 - val_loss: 1.2176\n",
      "Epoch 4/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0727\n",
      "Epoch 00004: val_loss did not improve from 1.18457\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0721 - val_loss: 1.1895\n",
      "Epoch 5/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0472\n",
      "Epoch 00005: val_loss did not improve from 1.18457\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0469 - val_loss: 1.1886\n",
      "Epoch 6/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0405\n",
      "Epoch 00006: val_loss did not improve from 1.18457\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 1.0409 - val_loss: 1.2297\n",
      "Epoch 7/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0338\n",
      "Epoch 00007: val_loss did not improve from 1.18457\n",
      "14152/14152 [==============================] - 2s 151us/sample - loss: 1.0340 - val_loss: 1.2023\n",
      "Epoch 8/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0237\n",
      "Epoch 00008: val_loss did not improve from 1.18457\n",
      "14152/14152 [==============================] - 2s 170us/sample - loss: 1.0241 - val_loss: 1.1908\n",
      "Epoch 9/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0137\n",
      "Epoch 00009: val_loss did not improve from 1.18457\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 1.0136 - val_loss: 1.2110\n",
      "Epoch 10/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0183\n",
      "Epoch 00010: val_loss did not improve from 1.18457\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0189 - val_loss: 1.1886\n",
      "Epoch 11/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0095\n",
      "Epoch 00011: val_loss did not improve from 1.18457\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0072 - val_loss: 1.1876\n",
      "Our oof rmse score is: 1.0725790841944554\n",
      "Our oof cohen kappa score is: 0.515492442298995\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Train on 14152 samples, validate on 721 samples\n",
      "Epoch 1/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.3472\n",
      "Epoch 00001: val_loss improved from inf to 1.11274, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 230us/sample - loss: 1.3434 - val_loss: 1.1127\n",
      "Epoch 2/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.1511\n",
      "Epoch 00002: val_loss did not improve from 1.11274\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.1510 - val_loss: 1.1375\n",
      "Epoch 3/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1124\n",
      "Epoch 00003: val_loss did not improve from 1.11274\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.1124 - val_loss: 1.1221\n",
      "Epoch 4/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0995\n",
      "Epoch 00004: val_loss improved from 1.11274 to 1.08454, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 1.0995 - val_loss: 1.0845\n",
      "Epoch 5/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0741\n",
      "Epoch 00005: val_loss did not improve from 1.08454\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0733 - val_loss: 1.0901\n",
      "Epoch 6/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0597\n",
      "Epoch 00006: val_loss did not improve from 1.08454\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.0600 - val_loss: 1.0882\n",
      "Epoch 7/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0500\n",
      "Epoch 00007: val_loss did not improve from 1.08454\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 1.0494 - val_loss: 1.1243\n",
      "Epoch 8/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0379\n",
      "Epoch 00008: val_loss did not improve from 1.08454\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 1.0414 - val_loss: 1.1228\n",
      "Epoch 9/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0377\n",
      "Epoch 00009: val_loss did not improve from 1.08454\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 1.0375 - val_loss: 1.0866\n",
      "Epoch 10/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0345\n",
      "Epoch 00010: val_loss did not improve from 1.08454\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0324 - val_loss: 1.0986\n",
      "Epoch 11/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0246\n",
      "Epoch 00011: val_loss improved from 1.08454 to 1.06954, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0230 - val_loss: 1.0695\n",
      "Epoch 12/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0171\n",
      "Epoch 00012: val_loss did not improve from 1.06954\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0155 - val_loss: 1.0707\n",
      "Epoch 13/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0191\n",
      "Epoch 00013: val_loss improved from 1.06954 to 1.03166, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 1.0178 - val_loss: 1.0317\n",
      "Epoch 14/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0069\n",
      "Epoch 00014: val_loss did not improve from 1.03166\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0090 - val_loss: 1.0759\n",
      "Epoch 15/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0127\n",
      "Epoch 00015: val_loss did not improve from 1.03166\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.0120 - val_loss: 1.0930\n",
      "Epoch 16/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9976\n",
      "Epoch 00016: val_loss did not improve from 1.03166\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 0.9991 - val_loss: 1.0847\n",
      "Epoch 17/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9955\n",
      "Epoch 00017: val_loss did not improve from 1.03166\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 0.9950 - val_loss: 1.0767\n",
      "Epoch 18/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9852\n",
      "Epoch 00018: val_loss did not improve from 1.03166\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 0.9849 - val_loss: 1.0453\n",
      "Epoch 19/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9818\n",
      "Epoch 00019: val_loss did not improve from 1.03166\n",
      "14152/14152 [==============================] - 2s 173us/sample - loss: 0.9805 - val_loss: 1.1144\n",
      "Epoch 20/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9855\n",
      "Epoch 00020: val_loss did not improve from 1.03166\n",
      "14152/14152 [==============================] - 2s 175us/sample - loss: 0.9853 - val_loss: 1.1211\n",
      "Epoch 21/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9861\n",
      "Epoch 00021: val_loss did not improve from 1.03166\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 0.9872 - val_loss: 1.0547\n",
      "Epoch 22/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9663\n",
      "Epoch 00022: val_loss did not improve from 1.03166\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 0.9690 - val_loss: 1.0924\n",
      "Epoch 23/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9640\n",
      "Epoch 00023: val_loss did not improve from 1.03166\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 0.9659 - val_loss: 1.0916\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Train on 14152 samples, validate on 722 samples\n",
      "Epoch 1/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.4052\n",
      "Epoch 00001: val_loss improved from inf to 1.16402, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 231us/sample - loss: 1.4022 - val_loss: 1.1640\n",
      "Epoch 2/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.1248\n",
      "Epoch 00002: val_loss improved from 1.16402 to 1.08544, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.1221 - val_loss: 1.0854\n",
      "Epoch 3/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.1071\n",
      "Epoch 00003: val_loss did not improve from 1.08544\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.1066 - val_loss: 1.0928\n",
      "Epoch 4/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0815\n",
      "Epoch 00004: val_loss improved from 1.08544 to 1.08473, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.0813 - val_loss: 1.0847\n",
      "Epoch 5/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0585\n",
      "Epoch 00005: val_loss improved from 1.08473 to 1.08233, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0577 - val_loss: 1.0823\n",
      "Epoch 6/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0408\n",
      "Epoch 00006: val_loss improved from 1.08233 to 1.08144, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 171us/sample - loss: 1.0429 - val_loss: 1.0814\n",
      "Epoch 7/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0339\n",
      "Epoch 00007: val_loss did not improve from 1.08144\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.0344 - val_loss: 1.1171\n",
      "Epoch 8/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0300\n",
      "Epoch 00008: val_loss did not improve from 1.08144\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0303 - val_loss: 1.1087\n",
      "Epoch 9/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0319\n",
      "Epoch 00009: val_loss improved from 1.08144 to 1.07300, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.0326 - val_loss: 1.0730\n",
      "Epoch 10/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0307\n",
      "Epoch 00010: val_loss did not improve from 1.07300\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0296 - val_loss: 1.0838\n",
      "Epoch 11/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0097\n",
      "Epoch 00011: val_loss did not improve from 1.07300\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.0125 - val_loss: 1.0973\n",
      "Epoch 12/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0011\n",
      "Epoch 00012: val_loss did not improve from 1.07300\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 1.0014 - val_loss: 1.0765\n",
      "Epoch 13/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0009\n",
      "Epoch 00013: val_loss improved from 1.07300 to 1.06949, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 0.9986 - val_loss: 1.0695\n",
      "Epoch 14/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9925\n",
      "Epoch 00014: val_loss did not improve from 1.06949\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 0.9928 - val_loss: 1.0752\n",
      "Epoch 15/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9931\n",
      "Epoch 00015: val_loss improved from 1.06949 to 1.06565, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 171us/sample - loss: 0.9936 - val_loss: 1.0656\n",
      "Epoch 16/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9857\n",
      "Epoch 00016: val_loss did not improve from 1.06565\n",
      "14152/14152 [==============================] - 2s 168us/sample - loss: 0.9853 - val_loss: 1.0858\n",
      "Epoch 17/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9925\n",
      "Epoch 00017: val_loss did not improve from 1.06565\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 0.9916 - val_loss: 1.0840\n",
      "Epoch 18/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9843\n",
      "Epoch 00018: val_loss did not improve from 1.06565\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 0.9849 - val_loss: 1.0969\n",
      "Epoch 19/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9791\n",
      "Epoch 00019: val_loss did not improve from 1.06565\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 0.9813 - val_loss: 1.0780\n",
      "Epoch 20/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9816\n",
      "Epoch 00020: val_loss did not improve from 1.06565\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 0.9809 - val_loss: 1.1101\n",
      "Epoch 21/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9734\n",
      "Epoch 00021: val_loss did not improve from 1.06565\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 0.9749 - val_loss: 1.1240\n",
      "Epoch 22/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9658\n",
      "Epoch 00022: val_loss did not improve from 1.06565\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 0.9652 - val_loss: 1.0842\n",
      "Epoch 23/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9674\n",
      "Epoch 00023: val_loss did not improve from 1.06565\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 0.9658 - val_loss: 1.1136\n",
      "Epoch 24/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9598\n",
      "Epoch 00024: val_loss did not improve from 1.06565\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 0.9597 - val_loss: 1.0912\n",
      "Epoch 25/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9555\n",
      "Epoch 00025: val_loss did not improve from 1.06565\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 0.9531 - val_loss: 1.0885\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Train on 14152 samples, validate on 723 samples\n",
      "Epoch 1/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.3461\n",
      "Epoch 00001: val_loss improved from inf to 1.16364, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 221us/sample - loss: 1.3439 - val_loss: 1.1636\n",
      "Epoch 2/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1582\n",
      "Epoch 00002: val_loss improved from 1.16364 to 1.07490, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 173us/sample - loss: 1.1582 - val_loss: 1.0749\n",
      "Epoch 3/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.1059\n",
      "Epoch 00003: val_loss improved from 1.07490 to 1.05892, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.1090 - val_loss: 1.0589\n",
      "Epoch 4/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0908\n",
      "Epoch 00004: val_loss did not improve from 1.05892\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.0911 - val_loss: 1.0776\n",
      "Epoch 5/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0801\n",
      "Epoch 00005: val_loss did not improve from 1.05892\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0825 - val_loss: 1.0786\n",
      "Epoch 6/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0541\n",
      "Epoch 00006: val_loss did not improve from 1.05892\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 1.0521 - val_loss: 1.0682\n",
      "Epoch 7/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0505\n",
      "Epoch 00007: val_loss did not improve from 1.05892\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 1.0507 - val_loss: 1.0948\n",
      "Epoch 8/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0418\n",
      "Epoch 00008: val_loss did not improve from 1.05892\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0434 - val_loss: 1.0727\n",
      "Epoch 9/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0284\n",
      "Epoch 00009: val_loss did not improve from 1.05892\n",
      "14152/14152 [==============================] - 3s 182us/sample - loss: 1.0280 - val_loss: 1.0712\n",
      "Epoch 10/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0207\n",
      "Epoch 00010: val_loss did not improve from 1.05892\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 1.0217 - val_loss: 1.0879\n",
      "Epoch 11/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0182\n",
      "Epoch 00011: val_loss did not improve from 1.05892\n",
      "14152/14152 [==============================] - 3s 178us/sample - loss: 1.0211 - val_loss: 1.1400\n",
      "Epoch 12/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0175\n",
      "Epoch 00012: val_loss improved from 1.05892 to 1.05265, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 227us/sample - loss: 1.0181 - val_loss: 1.0527\n",
      "Epoch 13/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0096\n",
      "Epoch 00013: val_loss improved from 1.05265 to 1.04251, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.0107 - val_loss: 1.0425\n",
      "Epoch 14/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0125\n",
      "Epoch 00014: val_loss improved from 1.04251 to 1.03848, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 4s 284us/sample - loss: 1.0123 - val_loss: 1.0385\n",
      "Epoch 15/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9953\n",
      "Epoch 00015: val_loss did not improve from 1.03848\n",
      "14152/14152 [==============================] - 3s 214us/sample - loss: 0.9954 - val_loss: 1.0627\n",
      "Epoch 16/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9972\n",
      "Epoch 00016: val_loss did not improve from 1.03848\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 0.9984 - val_loss: 1.1189\n",
      "Epoch 17/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9993\n",
      "Epoch 00017: val_loss did not improve from 1.03848\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 0.9996 - val_loss: 1.0562\n",
      "Epoch 18/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0031\n",
      "Epoch 00018: val_loss did not improve from 1.03848\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 1.0027 - val_loss: 1.0841\n",
      "Epoch 19/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9921\n",
      "Epoch 00019: val_loss did not improve from 1.03848\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 0.9928 - val_loss: 1.0661\n",
      "Epoch 20/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9835\n",
      "Epoch 00020: val_loss did not improve from 1.03848\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 0.9832 - val_loss: 1.0467\n",
      "Epoch 21/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9811\n",
      "Epoch 00021: val_loss did not improve from 1.03848\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 0.9803 - val_loss: 1.0453\n",
      "Epoch 22/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9831\n",
      "Epoch 00022: val_loss did not improve from 1.03848\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 0.9831 - val_loss: 1.0777\n",
      "Epoch 23/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9759\n",
      "Epoch 00023: val_loss did not improve from 1.03848\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 0.9760 - val_loss: 1.1079\n",
      "Epoch 24/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9635\n",
      "Epoch 00024: val_loss did not improve from 1.03848\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 0.9638 - val_loss: 1.0976\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Train on 14152 samples, validate on 724 samples\n",
      "Epoch 1/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.2866\n",
      "Epoch 00001: val_loss improved from inf to 1.28996, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 231us/sample - loss: 1.2863 - val_loss: 1.2900\n",
      "Epoch 2/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.1404\n",
      "Epoch 00002: val_loss improved from 1.28996 to 1.26581, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.1400 - val_loss: 1.2658\n",
      "Epoch 3/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0874\n",
      "Epoch 00003: val_loss improved from 1.26581 to 1.24646, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0900 - val_loss: 1.2465\n",
      "Epoch 4/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0751\n",
      "Epoch 00004: val_loss did not improve from 1.24646\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0736 - val_loss: 1.2480\n",
      "Epoch 5/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0570\n",
      "Epoch 00005: val_loss improved from 1.24646 to 1.22373, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.0574 - val_loss: 1.2237\n",
      "Epoch 6/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0420\n",
      "Epoch 00006: val_loss did not improve from 1.22373\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 1.0389 - val_loss: 1.2539\n",
      "Epoch 7/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0290\n",
      "Epoch 00007: val_loss improved from 1.22373 to 1.21938, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.0301 - val_loss: 1.2194\n",
      "Epoch 8/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0258\n",
      "Epoch 00008: val_loss did not improve from 1.21938\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.0261 - val_loss: 1.2362\n",
      "Epoch 9/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0161\n",
      "Epoch 00009: val_loss improved from 1.21938 to 1.21857, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0172 - val_loss: 1.2186\n",
      "Epoch 10/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0094\n",
      "Epoch 00010: val_loss did not improve from 1.21857\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 1.0102 - val_loss: 1.2192\n",
      "Epoch 11/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0083\n",
      "Epoch 00011: val_loss did not improve from 1.21857\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.0052 - val_loss: 1.2687\n",
      "Epoch 12/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9905\n",
      "Epoch 00012: val_loss did not improve from 1.21857\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 0.9916 - val_loss: 1.2498\n",
      "Epoch 13/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0030\n",
      "Epoch 00013: val_loss did not improve from 1.21857\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 1.0024 - val_loss: 1.2216\n",
      "Epoch 14/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9774\n",
      "Epoch 00014: val_loss improved from 1.21857 to 1.21733, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 0.9764 - val_loss: 1.2173\n",
      "Epoch 15/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9858\n",
      "Epoch 00015: val_loss did not improve from 1.21733\n",
      "14152/14152 [==============================] - 2s 171us/sample - loss: 0.9854 - val_loss: 1.2835\n",
      "Epoch 16/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9863\n",
      "Epoch 00016: val_loss did not improve from 1.21733\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 0.9840 - val_loss: 1.2493\n",
      "Epoch 17/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9942\n",
      "Epoch 00017: val_loss did not improve from 1.21733\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 0.9936 - val_loss: 1.2359\n",
      "Epoch 18/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9800\n",
      "Epoch 00018: val_loss did not improve from 1.21733\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 0.9769 - val_loss: 1.2492\n",
      "Epoch 19/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9742\n",
      "Epoch 00019: val_loss did not improve from 1.21733\n",
      "14152/14152 [==============================] - 2s 153us/sample - loss: 0.9756 - val_loss: 1.2360\n",
      "Epoch 20/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9686\n",
      "Epoch 00020: val_loss did not improve from 1.21733\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 0.9678 - val_loss: 1.2532\n",
      "Epoch 21/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9440\n",
      "Epoch 00021: val_loss did not improve from 1.21733\n",
      "14152/14152 [==============================] - 2s 152us/sample - loss: 0.9450 - val_loss: 1.2366\n",
      "Epoch 22/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9561\n",
      "Epoch 00022: val_loss did not improve from 1.21733\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 0.9542 - val_loss: 1.2273\n",
      "Epoch 23/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9457\n",
      "Epoch 00023: val_loss did not improve from 1.21733\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 0.9466 - val_loss: 1.2593\n",
      "Epoch 24/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9503\n",
      "Epoch 00024: val_loss did not improve from 1.21733\n",
      "14152/14152 [==============================] - 2s 157us/sample - loss: 0.9509 - val_loss: 1.2265\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Train on 14152 samples, validate on 724 samples\n",
      "Epoch 1/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.2806\n",
      "Epoch 00001: val_loss improved from inf to 1.20017, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 4s 254us/sample - loss: 1.2791 - val_loss: 1.2002\n",
      "Epoch 2/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.1417\n",
      "Epoch 00002: val_loss improved from 1.20017 to 1.15854, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.1418 - val_loss: 1.1585\n",
      "Epoch 3/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0850\n",
      "Epoch 00003: val_loss improved from 1.15854 to 1.15069, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0830 - val_loss: 1.1507\n",
      "Epoch 4/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0732\n",
      "Epoch 00004: val_loss improved from 1.15069 to 1.14909, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.0699 - val_loss: 1.1491\n",
      "Epoch 5/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0461\n",
      "Epoch 00005: val_loss did not improve from 1.14909\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0448 - val_loss: 1.2183\n",
      "Epoch 6/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0355\n",
      "Epoch 00006: val_loss did not improve from 1.14909\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 1.0372 - val_loss: 1.1523\n",
      "Epoch 7/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0222\n",
      "Epoch 00007: val_loss did not improve from 1.14909\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.0215 - val_loss: 1.1605\n",
      "Epoch 8/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0173\n",
      "Epoch 00008: val_loss did not improve from 1.14909\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 1.0166 - val_loss: 1.1837\n",
      "Epoch 9/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0223\n",
      "Epoch 00009: val_loss did not improve from 1.14909\n",
      "14152/14152 [==============================] - 2s 170us/sample - loss: 1.0226 - val_loss: 1.1493\n",
      "Epoch 10/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9982\n",
      "Epoch 00010: val_loss improved from 1.14909 to 1.12961, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 0.9983 - val_loss: 1.1296\n",
      "Epoch 11/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0065\n",
      "Epoch 00011: val_loss improved from 1.12961 to 1.11903, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 169us/sample - loss: 1.0063 - val_loss: 1.1190\n",
      "Epoch 12/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0130\n",
      "Epoch 00012: val_loss improved from 1.11903 to 1.11336, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 1.0143 - val_loss: 1.1134\n",
      "Epoch 13/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9818\n",
      "Epoch 00013: val_loss did not improve from 1.11336\n",
      "14152/14152 [==============================] - 3s 179us/sample - loss: 0.9834 - val_loss: 1.1262\n",
      "Epoch 14/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9968\n",
      "Epoch 00014: val_loss did not improve from 1.11336\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 0.9953 - val_loss: 1.1599\n",
      "Epoch 15/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9781\n",
      "Epoch 00015: val_loss did not improve from 1.11336\n",
      "14152/14152 [==============================] - 2s 171us/sample - loss: 0.9781 - val_loss: 1.1613\n",
      "Epoch 16/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9848\n",
      "Epoch 00016: val_loss did not improve from 1.11336\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 0.9836 - val_loss: 1.1355\n",
      "Epoch 17/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9834\n",
      "Epoch 00017: val_loss did not improve from 1.11336\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 0.9849 - val_loss: 1.1594\n",
      "Epoch 18/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9678\n",
      "Epoch 00018: val_loss did not improve from 1.11336\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 0.9681 - val_loss: 1.1886\n",
      "Epoch 19/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9600\n",
      "Epoch 00019: val_loss did not improve from 1.11336\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 0.9614 - val_loss: 1.1449\n",
      "Epoch 20/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9607\n",
      "Epoch 00020: val_loss did not improve from 1.11336\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 0.9606 - val_loss: 1.1157\n",
      "Epoch 21/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9651\n",
      "Epoch 00021: val_loss did not improve from 1.11336\n",
      "14152/14152 [==============================] - 2s 168us/sample - loss: 0.9654 - val_loss: 1.1351\n",
      "Epoch 22/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9531\n",
      "Epoch 00022: val_loss did not improve from 1.11336\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 0.9538 - val_loss: 1.1430\n",
      "Our oof rmse score is: 1.0456472297006447\n",
      "Our oof cohen kappa score is: 0.5426766150798439\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Train on 14152 samples, validate on 721 samples\n",
      "Epoch 1/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.2813\n",
      "Epoch 00001: val_loss improved from inf to 1.29164, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 233us/sample - loss: 1.2804 - val_loss: 1.2916\n",
      "Epoch 2/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.1580\n",
      "Epoch 00002: val_loss did not improve from 1.29164\n",
      "14152/14152 [==============================] - 2s 170us/sample - loss: 1.1573 - val_loss: 1.3017\n",
      "Epoch 3/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.1091\n",
      "Epoch 00003: val_loss improved from 1.29164 to 1.25149, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 168us/sample - loss: 1.1099 - val_loss: 1.2515\n",
      "Epoch 4/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0890\n",
      "Epoch 00004: val_loss did not improve from 1.25149\n",
      "14152/14152 [==============================] - 2s 169us/sample - loss: 1.0887 - val_loss: 1.2639\n",
      "Epoch 5/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0707\n",
      "Epoch 00005: val_loss improved from 1.25149 to 1.24500, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 176us/sample - loss: 1.0735 - val_loss: 1.2450\n",
      "Epoch 6/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0536\n",
      "Epoch 00006: val_loss improved from 1.24500 to 1.20177, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.0538 - val_loss: 1.2018\n",
      "Epoch 7/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0496\n",
      "Epoch 00007: val_loss did not improve from 1.20177\n",
      "14152/14152 [==============================] - 2s 170us/sample - loss: 1.0504 - val_loss: 1.2498\n",
      "Epoch 8/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0477\n",
      "Epoch 00008: val_loss did not improve from 1.20177\n",
      "14152/14152 [==============================] - 2s 168us/sample - loss: 1.0479 - val_loss: 1.2474\n",
      "Epoch 9/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0368\n",
      "Epoch 00009: val_loss did not improve from 1.20177\n",
      "14152/14152 [==============================] - 3s 178us/sample - loss: 1.0381 - val_loss: 1.2461\n",
      "Epoch 10/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0313\n",
      "Epoch 00010: val_loss did not improve from 1.20177\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.0335 - val_loss: 1.2590\n",
      "Epoch 11/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0273\n",
      "Epoch 00011: val_loss did not improve from 1.20177\n",
      "14152/14152 [==============================] - 2s 174us/sample - loss: 1.0263 - val_loss: 1.2549\n",
      "Epoch 12/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0154\n",
      "Epoch 00012: val_loss did not improve from 1.20177\n",
      "14152/14152 [==============================] - 2s 173us/sample - loss: 1.0147 - val_loss: 1.2696\n",
      "Epoch 13/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0120\n",
      "Epoch 00013: val_loss did not improve from 1.20177\n",
      "14152/14152 [==============================] - 2s 169us/sample - loss: 1.0100 - val_loss: 1.2618\n",
      "Epoch 14/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0129\n",
      "Epoch 00014: val_loss did not improve from 1.20177\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0108 - val_loss: 1.3614\n",
      "Epoch 15/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9939\n",
      "Epoch 00015: val_loss did not improve from 1.20177\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 0.9950 - val_loss: 1.2689\n",
      "Epoch 16/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9986\n",
      "Epoch 00016: val_loss did not improve from 1.20177\n",
      "14152/14152 [==============================] - 2s 170us/sample - loss: 0.9976 - val_loss: 1.2858\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Train on 14152 samples, validate on 722 samples\n",
      "Epoch 1/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.3224\n",
      "Epoch 00001: val_loss improved from inf to 1.28834, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 230us/sample - loss: 1.3222 - val_loss: 1.2883\n",
      "Epoch 2/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.1410\n",
      "Epoch 00002: val_loss improved from 1.28834 to 1.19121, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.1436 - val_loss: 1.1912\n",
      "Epoch 3/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.1128\n",
      "Epoch 00003: val_loss did not improve from 1.19121\n",
      "14152/14152 [==============================] - 2s 171us/sample - loss: 1.1135 - val_loss: 1.2003\n",
      "Epoch 4/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0807\n",
      "Epoch 00004: val_loss improved from 1.19121 to 1.15580, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 169us/sample - loss: 1.0795 - val_loss: 1.1558\n",
      "Epoch 5/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0705\n",
      "Epoch 00005: val_loss improved from 1.15580 to 1.15507, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.0707 - val_loss: 1.1551\n",
      "Epoch 6/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0457\n",
      "Epoch 00006: val_loss improved from 1.15507 to 1.12175, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.0454 - val_loss: 1.1218\n",
      "Epoch 7/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0361\n",
      "Epoch 00007: val_loss did not improve from 1.12175\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 1.0353 - val_loss: 1.1703\n",
      "Epoch 8/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0268\n",
      "Epoch 00008: val_loss did not improve from 1.12175\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.0262 - val_loss: 1.2134\n",
      "Epoch 9/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0293\n",
      "Epoch 00009: val_loss did not improve from 1.12175\n",
      "14152/14152 [==============================] - 2s 170us/sample - loss: 1.0302 - val_loss: 1.1321\n",
      "Epoch 10/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0086\n",
      "Epoch 00010: val_loss did not improve from 1.12175\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0091 - val_loss: 1.1583\n",
      "Epoch 11/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0065\n",
      "Epoch 00011: val_loss did not improve from 1.12175\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.0068 - val_loss: 1.1550\n",
      "Epoch 12/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0114\n",
      "Epoch 00012: val_loss did not improve from 1.12175\n",
      "14152/14152 [==============================] - 2s 168us/sample - loss: 1.0108 - val_loss: 1.1323\n",
      "Epoch 13/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0097\n",
      "Epoch 00013: val_loss did not improve from 1.12175\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.0070 - val_loss: 1.1508\n",
      "Epoch 14/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9992\n",
      "Epoch 00014: val_loss did not improve from 1.12175\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 0.9975 - val_loss: 1.1977\n",
      "Epoch 15/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0090\n",
      "Epoch 00015: val_loss did not improve from 1.12175\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 1.0112 - val_loss: 1.1362\n",
      "Epoch 16/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0013\n",
      "Epoch 00016: val_loss did not improve from 1.12175\n",
      "14152/14152 [==============================] - 2s 169us/sample - loss: 0.9991 - val_loss: 1.1824\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Train on 14152 samples, validate on 723 samples\n",
      "Epoch 1/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.3313\n",
      "Epoch 00001: val_loss improved from inf to 1.21416, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 228us/sample - loss: 1.3292 - val_loss: 1.2142\n",
      "Epoch 2/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.1561\n",
      "Epoch 00002: val_loss improved from 1.21416 to 1.19419, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.1564 - val_loss: 1.1942\n",
      "Epoch 3/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.1215\n",
      "Epoch 00003: val_loss improved from 1.19419 to 1.17101, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.1216 - val_loss: 1.1710\n",
      "Epoch 4/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0986\n",
      "Epoch 00004: val_loss improved from 1.17101 to 1.15086, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 170us/sample - loss: 1.0986 - val_loss: 1.1509\n",
      "Epoch 5/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0736\n",
      "Epoch 00005: val_loss did not improve from 1.15086\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0742 - val_loss: 1.1511\n",
      "Epoch 6/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0552\n",
      "Epoch 00006: val_loss did not improve from 1.15086\n",
      "14152/14152 [==============================] - 3s 195us/sample - loss: 1.0582 - val_loss: 1.1641\n",
      "Epoch 7/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0534\n",
      "Epoch 00007: val_loss did not improve from 1.15086\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.0531 - val_loss: 1.1521\n",
      "Epoch 8/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0436\n",
      "Epoch 00008: val_loss did not improve from 1.15086\n",
      "14152/14152 [==============================] - 2s 169us/sample - loss: 1.0420 - val_loss: 1.1516\n",
      "Epoch 9/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0307\n",
      "Epoch 00009: val_loss did not improve from 1.15086\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 1.0307 - val_loss: 1.1988\n",
      "Epoch 10/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0293\n",
      "Epoch 00010: val_loss did not improve from 1.15086\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.0283 - val_loss: 1.1951\n",
      "Epoch 11/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0240\n",
      "Epoch 00011: val_loss did not improve from 1.15086\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0242 - val_loss: 1.1659\n",
      "Epoch 12/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0265\n",
      "Epoch 00012: val_loss did not improve from 1.15086\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.0255 - val_loss: 1.1784\n",
      "Epoch 13/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0109\n",
      "Epoch 00013: val_loss did not improve from 1.15086\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 1.0079 - val_loss: 1.2074\n",
      "Epoch 14/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0100\n",
      "Epoch 00014: val_loss did not improve from 1.15086\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.0084 - val_loss: 1.2723\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Train on 14152 samples, validate on 724 samples\n",
      "Epoch 1/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.3058\n",
      "Epoch 00001: val_loss improved from inf to 1.32760, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 232us/sample - loss: 1.3062 - val_loss: 1.3276\n",
      "Epoch 2/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1345\n",
      "Epoch 00002: val_loss improved from 1.32760 to 1.28228, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.1341 - val_loss: 1.2823\n",
      "Epoch 3/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0904\n",
      "Epoch 00003: val_loss improved from 1.28228 to 1.24192, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.0894 - val_loss: 1.2419\n",
      "Epoch 4/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0720\n",
      "Epoch 00004: val_loss did not improve from 1.24192\n",
      "14152/14152 [==============================] - 2s 161us/sample - loss: 1.0716 - val_loss: 1.2910\n",
      "Epoch 5/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0556\n",
      "Epoch 00005: val_loss did not improve from 1.24192\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.0553 - val_loss: 1.2604\n",
      "Epoch 6/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0462\n",
      "Epoch 00006: val_loss did not improve from 1.24192\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 1.0479 - val_loss: 1.2433\n",
      "Epoch 7/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0459\n",
      "Epoch 00007: val_loss did not improve from 1.24192\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.0438 - val_loss: 1.2701\n",
      "Epoch 8/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0323\n",
      "Epoch 00008: val_loss did not improve from 1.24192\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.0311 - val_loss: 1.2844\n",
      "Epoch 9/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0250\n",
      "Epoch 00009: val_loss did not improve from 1.24192\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0252 - val_loss: 1.3287\n",
      "Epoch 10/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0114\n",
      "Epoch 00010: val_loss did not improve from 1.24192\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 1.0123 - val_loss: 1.2871\n",
      "Epoch 11/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0153\n",
      "Epoch 00011: val_loss did not improve from 1.24192\n",
      "14152/14152 [==============================] - 2s 160us/sample - loss: 1.0149 - val_loss: 1.3090\n",
      "Epoch 12/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0093\n",
      "Epoch 00012: val_loss did not improve from 1.24192\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.0084 - val_loss: 1.2568\n",
      "Epoch 13/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0079\n",
      "Epoch 00013: val_loss did not improve from 1.24192\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.0090 - val_loss: 1.2850\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Train on 14152 samples, validate on 724 samples\n",
      "Epoch 1/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.3155\n",
      "Epoch 00001: val_loss improved from inf to 1.29657, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 4s 259us/sample - loss: 1.3150 - val_loss: 1.2966\n",
      "Epoch 2/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.1399\n",
      "Epoch 00002: val_loss improved from 1.29657 to 1.23699, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 203us/sample - loss: 1.1380 - val_loss: 1.2370\n",
      "Epoch 3/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0966\n",
      "Epoch 00003: val_loss did not improve from 1.23699\n",
      "14152/14152 [==============================] - 3s 203us/sample - loss: 1.0965 - val_loss: 1.2388\n",
      "Epoch 4/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0691\n",
      "Epoch 00004: val_loss improved from 1.23699 to 1.16745, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 193us/sample - loss: 1.0685 - val_loss: 1.1674\n",
      "Epoch 5/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0546\n",
      "Epoch 00005: val_loss did not improve from 1.16745\n",
      "14152/14152 [==============================] - 3s 194us/sample - loss: 1.0543 - val_loss: 1.2051\n",
      "Epoch 6/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0455\n",
      "Epoch 00006: val_loss did not improve from 1.16745\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 1.0440 - val_loss: 1.2246\n",
      "Epoch 7/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0398\n",
      "Epoch 00007: val_loss did not improve from 1.16745\n",
      "14152/14152 [==============================] - 2s 171us/sample - loss: 1.0410 - val_loss: 1.2043\n",
      "Epoch 8/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0238\n",
      "Epoch 00008: val_loss did not improve from 1.16745\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.0225 - val_loss: 1.2137\n",
      "Epoch 9/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0105\n",
      "Epoch 00009: val_loss did not improve from 1.16745\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.0109 - val_loss: 1.2131\n",
      "Epoch 10/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0083\n",
      "Epoch 00010: val_loss improved from 1.16745 to 1.16013, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.0088 - val_loss: 1.1601\n",
      "Epoch 11/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0138\n",
      "Epoch 00011: val_loss did not improve from 1.16013\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0137 - val_loss: 1.2067\n",
      "Epoch 12/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0036\n",
      "Epoch 00012: val_loss did not improve from 1.16013\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.0031 - val_loss: 1.1792\n",
      "Epoch 13/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9930\n",
      "Epoch 00013: val_loss did not improve from 1.16013\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 0.9931 - val_loss: 1.2020\n",
      "Epoch 14/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9970\n",
      "Epoch 00014: val_loss did not improve from 1.16013\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 0.9982 - val_loss: 1.1725\n",
      "Epoch 15/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9917\n",
      "Epoch 00015: val_loss did not improve from 1.16013\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 0.9900 - val_loss: 1.2271\n",
      "Epoch 16/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9800\n",
      "Epoch 00016: val_loss did not improve from 1.16013\n",
      "14152/14152 [==============================] - 2s 168us/sample - loss: 0.9794 - val_loss: 1.1944\n",
      "Epoch 17/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9803\n",
      "Epoch 00017: val_loss did not improve from 1.16013\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 0.9800 - val_loss: 1.1657\n",
      "Epoch 18/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9717\n",
      "Epoch 00018: val_loss did not improve from 1.16013\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 0.9718 - val_loss: 1.1820\n",
      "Epoch 19/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9838\n",
      "Epoch 00019: val_loss did not improve from 1.16013\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 0.9851 - val_loss: 1.1848\n",
      "Epoch 20/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9791\n",
      "Epoch 00020: val_loss did not improve from 1.16013\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 0.9792 - val_loss: 1.1829\n",
      "Our oof rmse score is: 1.084112901918652\n",
      "Our oof cohen kappa score is: 0.5008990572132386\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Train on 14152 samples, validate on 721 samples\n",
      "Epoch 1/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.4242\n",
      "Epoch 00001: val_loss improved from inf to 1.23432, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 223us/sample - loss: 1.4218 - val_loss: 1.2343\n",
      "Epoch 2/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.1540\n",
      "Epoch 00002: val_loss improved from 1.23432 to 1.20423, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 1.1525 - val_loss: 1.2042\n",
      "Epoch 3/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.1088\n",
      "Epoch 00003: val_loss did not improve from 1.20423\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 1.1087 - val_loss: 1.2450\n",
      "Epoch 4/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0951\n",
      "Epoch 00004: val_loss did not improve from 1.20423\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.0955 - val_loss: 1.2279\n",
      "Epoch 5/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0695\n",
      "Epoch 00005: val_loss did not improve from 1.20423\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.0700 - val_loss: 1.2184\n",
      "Epoch 6/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0586\n",
      "Epoch 00006: val_loss did not improve from 1.20423\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0593 - val_loss: 1.2056\n",
      "Epoch 7/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0491\n",
      "Epoch 00007: val_loss did not improve from 1.20423\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.0483 - val_loss: 1.2231\n",
      "Epoch 8/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0432\n",
      "Epoch 00008: val_loss did not improve from 1.20423\n",
      "14152/14152 [==============================] - 2s 170us/sample - loss: 1.0406 - val_loss: 1.2441\n",
      "Epoch 9/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0236\n",
      "Epoch 00009: val_loss did not improve from 1.20423\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.0222 - val_loss: 1.2513\n",
      "Epoch 10/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0259\n",
      "Epoch 00010: val_loss improved from 1.20423 to 1.18682, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0286 - val_loss: 1.1868\n",
      "Epoch 11/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0138\n",
      "Epoch 00011: val_loss did not improve from 1.18682\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0145 - val_loss: 1.1941\n",
      "Epoch 12/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0103\n",
      "Epoch 00012: val_loss did not improve from 1.18682\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0092 - val_loss: 1.2172\n",
      "Epoch 13/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0187\n",
      "Epoch 00013: val_loss did not improve from 1.18682\n",
      "14152/14152 [==============================] - 2s 147us/sample - loss: 1.0186 - val_loss: 1.2454\n",
      "Epoch 14/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0150\n",
      "Epoch 00014: val_loss did not improve from 1.18682\n",
      "14152/14152 [==============================] - 2s 155us/sample - loss: 1.0148 - val_loss: 1.2111\n",
      "Epoch 15/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0038\n",
      "Epoch 00015: val_loss did not improve from 1.18682\n",
      "14152/14152 [==============================] - 2s 156us/sample - loss: 1.0072 - val_loss: 1.1883\n",
      "Epoch 16/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9954\n",
      "Epoch 00016: val_loss did not improve from 1.18682\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 0.9967 - val_loss: 1.1992\n",
      "Epoch 17/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9813\n",
      "Epoch 00017: val_loss did not improve from 1.18682\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 0.9795 - val_loss: 1.1925\n",
      "Epoch 18/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9797\n",
      "Epoch 00018: val_loss did not improve from 1.18682\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 0.9801 - val_loss: 1.2287\n",
      "Epoch 19/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9843\n",
      "Epoch 00019: val_loss did not improve from 1.18682\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 0.9836 - val_loss: 1.2579\n",
      "Epoch 20/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9831\n",
      "Epoch 00020: val_loss did not improve from 1.18682\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 0.9825 - val_loss: 1.2359\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Train on 14152 samples, validate on 722 samples\n",
      "Epoch 1/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.4085\n",
      "Epoch 00001: val_loss improved from inf to 1.21354, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 215us/sample - loss: 1.4033 - val_loss: 1.2135\n",
      "Epoch 2/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.1512\n",
      "Epoch 00002: val_loss improved from 1.21354 to 1.16498, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 141us/sample - loss: 1.1510 - val_loss: 1.1650\n",
      "Epoch 3/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.1052\n",
      "Epoch 00003: val_loss did not improve from 1.16498\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.1031 - val_loss: 1.1936\n",
      "Epoch 4/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0898\n",
      "Epoch 00004: val_loss did not improve from 1.16498\n",
      "14152/14152 [==============================] - 2s 141us/sample - loss: 1.0917 - val_loss: 1.1805\n",
      "Epoch 5/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0606\n",
      "Epoch 00005: val_loss did not improve from 1.16498\n",
      "14152/14152 [==============================] - 2s 143us/sample - loss: 1.0611 - val_loss: 1.1824\n",
      "Epoch 6/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0641\n",
      "Epoch 00006: val_loss improved from 1.16498 to 1.14222, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 142us/sample - loss: 1.0644 - val_loss: 1.1422\n",
      "Epoch 7/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0381\n",
      "Epoch 00007: val_loss did not improve from 1.14222\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0390 - val_loss: 1.1726\n",
      "Epoch 8/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0362\n",
      "Epoch 00008: val_loss improved from 1.14222 to 1.13662, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 141us/sample - loss: 1.0365 - val_loss: 1.1366\n",
      "Epoch 9/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0189\n",
      "Epoch 00009: val_loss improved from 1.13662 to 1.13237, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 1.0209 - val_loss: 1.1324\n",
      "Epoch 10/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0252\n",
      "Epoch 00010: val_loss improved from 1.13237 to 1.11879, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 142us/sample - loss: 1.0251 - val_loss: 1.1188\n",
      "Epoch 11/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0362\n",
      "Epoch 00011: val_loss did not improve from 1.11879\n",
      "14152/14152 [==============================] - 2s 141us/sample - loss: 1.0343 - val_loss: 1.1670\n",
      "Epoch 12/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0059\n",
      "Epoch 00012: val_loss did not improve from 1.11879\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 1.0059 - val_loss: 1.1281\n",
      "Epoch 13/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0234\n",
      "Epoch 00013: val_loss did not improve from 1.11879\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 1.0231 - val_loss: 1.1550\n",
      "Epoch 14/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0138\n",
      "Epoch 00014: val_loss improved from 1.11879 to 1.10970, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 154us/sample - loss: 1.0152 - val_loss: 1.1097\n",
      "Epoch 15/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0054\n",
      "Epoch 00015: val_loss did not improve from 1.10970\n",
      "14152/14152 [==============================] - 2s 158us/sample - loss: 1.0061 - val_loss: 1.1259\n",
      "Epoch 16/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9909\n",
      "Epoch 00016: val_loss did not improve from 1.10970\n",
      "14152/14152 [==============================] - 2s 150us/sample - loss: 0.9918 - val_loss: 1.1303\n",
      "Epoch 17/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9948\n",
      "Epoch 00017: val_loss did not improve from 1.10970\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 0.9944 - val_loss: 1.1274\n",
      "Epoch 18/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9828\n",
      "Epoch 00018: val_loss did not improve from 1.10970\n",
      "14152/14152 [==============================] - 2s 141us/sample - loss: 0.9832 - val_loss: 1.1363\n",
      "Epoch 19/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9759\n",
      "Epoch 00019: val_loss did not improve from 1.10970\n",
      "14152/14152 [==============================] - 2s 144us/sample - loss: 0.9767 - val_loss: 1.1559\n",
      "Epoch 20/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9945\n",
      "Epoch 00020: val_loss did not improve from 1.10970\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 0.9966 - val_loss: 1.1661\n",
      "Epoch 21/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9812\n",
      "Epoch 00021: val_loss did not improve from 1.10970\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 0.9817 - val_loss: 1.1452\n",
      "Epoch 22/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9776\n",
      "Epoch 00022: val_loss did not improve from 1.10970\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 0.9765 - val_loss: 1.1203\n",
      "Epoch 23/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9603\n",
      "Epoch 00023: val_loss did not improve from 1.10970\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 0.9639 - val_loss: 1.1509\n",
      "Epoch 24/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9792\n",
      "Epoch 00024: val_loss did not improve from 1.10970\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 0.9788 - val_loss: 1.1550\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Train on 14152 samples, validate on 723 samples\n",
      "Epoch 1/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.2993\n",
      "Epoch 00001: val_loss improved from inf to 1.10709, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 205us/sample - loss: 1.3012 - val_loss: 1.1071\n",
      "Epoch 2/100\n",
      "13760/14152 [============================>.] - ETA: 0s - loss: 1.1619\n",
      "Epoch 00002: val_loss did not improve from 1.10709\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 1.1587 - val_loss: 1.1273\n",
      "Epoch 3/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1081\n",
      "Epoch 00003: val_loss improved from 1.10709 to 1.10615, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 142us/sample - loss: 1.1087 - val_loss: 1.1061\n",
      "Epoch 4/100\n",
      "10688/14152 [=====================>........] - ETA: 0s - loss: 1.0701"
     ]
    }
   ],
   "source": [
    "for i in train_std.columns:\n",
    "    if \"session_title\" in str(i):\n",
    "        nn_features.append(i)\n",
    "y_pred_1_nn, oof_rmse_score_1_nn, oof_cohen_score_1_nn = run_nn(train_std, test_std, nn_features)\n",
    "y_pred_2_nn, oof_rmse_score_2_nn, oof_cohen_score_2_nn = run_nn(train_std, test_std, nn_features)\n",
    "y_pred_3_nn, oof_rmse_score_3_nn, oof_cohen_score_3_nn = run_nn(train_std, test_std, nn_features)\n",
    "y_pred_4_nn, oof_rmse_score_4_nn, oof_cohen_score_4_nn = run_nn(train_std, test_std, nn_features)\n",
    "y_pred_5_nn, oof_rmse_score_5_nn, oof_cohen_score_5_nn = run_nn(train_std, test_std, nn_features)\n",
    "mean_rmse_nn = (oof_rmse_score_1_nn + oof_rmse_score_2_nn + oof_rmse_score_3_nn + oof_rmse_score_4_nn + oof_rmse_score_5_nn) / 5\n",
    "mean_cohen_kappa_nn = (oof_cohen_score_1_nn + oof_cohen_score_2_nn + oof_cohen_score_3_nn + oof_cohen_score_4_nn + oof_cohen_score_5_nn) / 5\n",
    "print('Our mean rmse score is: ', mean_rmse_nn)\n",
    "print('Our mean cohen kappa score is: ', mean_cohen_kappa_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    0.500\n",
      "0    0.239\n",
      "1    0.136\n",
      "2    0.125\n",
      "Name: accuracy_group, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "y_final_lgb = (y_pred_1 + y_pred_2 + y_pred_3 + y_pred_4 + y_pred_5) / 5\n",
    "y_final_lr = (y_pred_1_lr + y_pred_2_lr + y_pred_3_lr + y_pred_4_lr + y_pred_5_lr) / 5\n",
    "y_final_nn = (y_pred_1_nn + y_pred_2_nn + y_pred_3_nn + y_pred_4_nn + y_pred_5_nn) / 5\n",
    "y_final = y_final_lgb * 0.6 + y_final_nn * 0.4 # + y_final_lr * 0.1\n",
    "y_final = eval_qwk_lgb_regr(y_final_nn, reduce_train)\n",
    "predict(sample_submission, y_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "024b7ebef9824df59d7bc5dfe96c39b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6d24c505189f4bfab1904c3e983344b6",
       "placeholder": "",
       "style": "IPY_MODEL_2a02d885106e42dba75f5ddc0ff7b962",
       "value": " 17000/17000 [13:03&lt;00:00, 21.69it/s]"
      }
     },
     "060faa842a2b46cd9a195ea5228e2080": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_af7378b30e664238974a5a6160d13ff7",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9e02a1a3509341acaf1309784517673d",
       "value": 1000
      }
     },
     "11bff0d2c32142f981c868bf4953c6e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c87d430436be4cd889a8025053620d79",
       "max": 17000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dae5ccba337b478f9b822b157304fadb",
       "value": 17000
      }
     },
     "28f4011ca844455298f3b1349cba5611": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2a02d885106e42dba75f5ddc0ff7b962": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2a52ae2104804aa5ab22f77667793359": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_28f4011ca844455298f3b1349cba5611",
       "placeholder": "",
       "style": "IPY_MODEL_d34c73d58efa4d71b81673ad5e50e8a5",
       "value": " 1000/1000 [12:44&lt;00:00,  1.31it/s]"
      }
     },
     "6d24c505189f4bfab1904c3e983344b6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "70300ba649ec46f4a219894123547193": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7e79a889db214fabb1ad6bbd00aa8cba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_11bff0d2c32142f981c868bf4953c6e1",
        "IPY_MODEL_024b7ebef9824df59d7bc5dfe96c39b9"
       ],
       "layout": "IPY_MODEL_aac00af93c09457087c1b30fb8b99daa"
      }
     },
     "9b8a42b109cb4125a4cb754cc0285be3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e4e2c548db7d4a6d95709673b4d3d85f",
       "placeholder": "",
       "style": "IPY_MODEL_a370a6ddb458459a874e65bd7a8091fb",
       "value": " 1000/1000 [01:58&lt;00:00,  8.45it/s]"
      }
     },
     "9e02a1a3509341acaf1309784517673d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "a370a6ddb458459a874e65bd7a8091fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "aac00af93c09457087c1b30fb8b99daa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "af7378b30e664238974a5a6160d13ff7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c0764b23abfa4e4bbd6d406ec0cfde35": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f8ac63082ee1420faec7b831191d2be9",
        "IPY_MODEL_2a52ae2104804aa5ab22f77667793359"
       ],
       "layout": "IPY_MODEL_c8bdf44012634342bb471dd4b5e47d11"
      }
     },
     "c87d430436be4cd889a8025053620d79": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c8bdf44012634342bb471dd4b5e47d11": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d34c73d58efa4d71b81673ad5e50e8a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "dae5ccba337b478f9b822b157304fadb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "e4e2c548db7d4a6d95709673b4d3d85f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e90c0bcf050d4d43bd1fa8a8318f7f7e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f62412ed55ed457aa8381489bad5b350": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_060faa842a2b46cd9a195ea5228e2080",
        "IPY_MODEL_9b8a42b109cb4125a4cb754cc0285be3"
       ],
       "layout": "IPY_MODEL_70300ba649ec46f4a219894123547193"
      }
     },
     "f8ac63082ee1420faec7b831191d2be9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e90c0bcf050d4d43bd1fa8a8318f7f7e",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fe4b20461f5b42cdb788c1306590a9b9",
       "value": 1000
      }
     },
     "fe4b20461f5b42cdb788c1306590a9b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
