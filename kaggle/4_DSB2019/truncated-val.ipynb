{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- consider all outputs from lgb, nn and lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "import json\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    print('Reading train.csv file....')\n",
    "    train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\n",
    "\n",
    "    print('Reading test.csv file....')\n",
    "    test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n",
    "\n",
    "    print('Reading train_labels.csv file....')\n",
    "    train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\n",
    "\n",
    "    print('Reading specs.csv file....')\n",
    "    specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n",
    "\n",
    "    print('Reading sample_submission.csv file....')\n",
    "    sample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n",
    "    return train, test, train_labels, specs, sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_data(train, train_labels):\n",
    "    keep_id = train[train.type == \"Assessment\"][['installation_id']].drop_duplicates()\n",
    "    train = pd.merge(train, keep_id, on=\"installation_id\", how=\"inner\")\n",
    "    train = train[train.installation_id.isin(train_labels.installation_id.unique())]\n",
    "    assess_title = ['Mushroom Sorter (Assessment)', 'Bird Measurer (Assessment)',\n",
    "       'Cauldron Filler (Assessment)', 'Cart Balancer (Assessment)', 'Chest Sorter (Assessment)']\n",
    "    additional_remove_index = []\n",
    "    for i, session in train.groupby('installation_id', sort=False):\n",
    "        last_row = session.index[-1]\n",
    "        session = session[session.title.isin(assess_title)]\n",
    "        first_row = session.index[-1] + 1\n",
    "        for j in range(first_row, last_row+1):\n",
    "            additional_remove_index.append(j)                \n",
    "    train = train[~train.index.isin(additional_remove_index)]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_title(train, test, train_labels):\n",
    "    # encode title\n",
    "    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n",
    "    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n",
    "    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n",
    "    # make a list with all the unique 'titles' from the train and test set\n",
    "    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n",
    "    # make a list with all the unique 'event_code' from the train and test set\n",
    "    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n",
    "    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n",
    "    # make a list with all the unique worlds from the train and test set\n",
    "    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n",
    "    # create a dictionary numerating the titles\n",
    "    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n",
    "    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n",
    "    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n",
    "    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n",
    "    # replace the text titles with the number titles from the dict\n",
    "    train['title'] = train['title'].map(activities_map)\n",
    "    test['title'] = test['title'].map(activities_map)\n",
    "    train['world'] = train['world'].map(activities_world)\n",
    "    test['world'] = test['world'].map(activities_world)\n",
    "    train_labels['title'] = train_labels['title'].map(activities_map)\n",
    "    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n",
    "    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n",
    "    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n",
    "    # convert text into datetime\n",
    "    train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "    test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "    train[\"misses\"] = train[\"event_data\"].apply(lambda x: json.loads(x)[\"misses\"] if \"\\\"misses\\\"\" in x else np.nan)\n",
    "    test[\"misses\"] = test[\"event_data\"].apply(lambda x: json.loads(x)[\"misses\"] if \"\\\"misses\\\"\" in x else np.nan)\n",
    "    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code, activities_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(user_sample, test_set=False):\n",
    "    '''\n",
    "    The user_sample is a DataFrame from train or test where the only one \n",
    "    installation_id is filtered\n",
    "    And the test_set parameter is related with the labels processing, that is only requered\n",
    "    if test_set=False\n",
    "    '''\n",
    "    # Constants and parameters declaration\n",
    "    last_activity = 0\n",
    "    \n",
    "    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "    \n",
    "    # new features: time spent in each activity\n",
    "    last_session_time_sec = 0\n",
    "    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n",
    "    all_assessments = []\n",
    "    accumulated_accuracy_group = 0\n",
    "    accumulated_accuracy = 0\n",
    "    accumulated_correct_attempts = 0 \n",
    "    accumulated_uncorrect_attempts = 0\n",
    "    accumulated_actions = 0\n",
    "    counter = 0\n",
    "    time_first_activity = float(user_sample['timestamp'].values[0])\n",
    "    durations = []\n",
    "    durations_game = []\n",
    "    durations_activity = []\n",
    "    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n",
    "    last_game_time_title = {'lgt_' + title: 0 for title in assess_titles}\n",
    "    ac_game_time_title = {'agt_' + title: 0 for title in assess_titles}\n",
    "    ac_true_attempts_title = {'ata_' + title: 0 for title in assess_titles}\n",
    "    ac_false_attempts_title = {'afa_' + title: 0 for title in assess_titles}\n",
    "    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n",
    "    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n",
    "    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n",
    "    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n",
    "    session_count = 0\n",
    "    miss = 0\n",
    "    crys_game_true = 0; crys_game_false = 0\n",
    "    tree_game_true = 0; tree_game_false = 0\n",
    "    magma_game_true = 0; magma_game_false = 0\n",
    "    crys_game_acc = []; tree_game_acc = []; magma_game_acc = []\n",
    "    crys_act_true = 0; crys_act_false = 0\n",
    "    tree_act_true = 0; tree_act_false = 0\n",
    "    magma_act_true = 0; magma_act_false = 0\n",
    "    crys_act_acc = []; tree_act_acc = []; magma_act_acc = []\n",
    "    \n",
    "    # itarates through each session of one instalation_id\n",
    "    for i, session in user_sample.groupby('game_session', sort=False):\n",
    "        # i = game_session_id\n",
    "        # session is a DataFrame that contain only one game_session\n",
    "        \n",
    "        # get some sessions information\n",
    "        session_type = session['type'].iloc[0]\n",
    "        session_title = session['title'].iloc[0]\n",
    "        session_title_text = activities_labels[session_title]  \n",
    "        session_world = session[\"world\"].iloc[0]\n",
    "            \n",
    "        # for each assessment, and only this kind off session, the features below are processed\n",
    "        # and a register are generated      \n",
    "        \n",
    "        if (session_type == 'Assessment') & (test_set or len(session)>1):\n",
    "            # search for event_code 4100, that represents the assessments trial\n",
    "            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n",
    "            # then, check the numbers of wins and the number of losses\n",
    "            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n",
    "            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n",
    "            # copy a dict to use as feature template, it's initialized with some itens: \n",
    "            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "            features = user_activities_count.copy()\n",
    "            features.update(last_accuracy_title.copy())\n",
    "            features.update(event_code_count.copy())\n",
    "            features.update(title_count.copy())\n",
    "            features.update(event_id_count.copy())\n",
    "            features.update(title_event_code_count.copy())\n",
    "            features.update(last_game_time_title.copy())\n",
    "            features.update(ac_game_time_title.copy())\n",
    "            features.update(ac_true_attempts_title.copy())\n",
    "            features.update(ac_false_attempts_title.copy())\n",
    "            features['installation_session_count'] = session_count\n",
    "            \n",
    "            variety_features = [('var_event_code', event_code_count), \n",
    "                                ('var_event_id', event_id_count), \n",
    "                                ('var_title', title_count), \n",
    "                                ('var_title_event_code', title_event_code_count)]\n",
    "            \n",
    "            for name, dict_counts in variety_features:\n",
    "                arr = np.array(list(dict_counts.values()))\n",
    "                features[name] = np.count_nonzero(arr)\n",
    "                \n",
    "            # get installation_id for aggregated features\n",
    "            features['installation_id'] = session['installation_id'].iloc[-1]\n",
    "            # add title as feature, remembering that title represents the name of the game\n",
    "            features['session_title'] = session['title'].iloc[0]\n",
    "            # the 4 lines below add the feature of the history of the trials of this player\n",
    "            # this is based on the all time attempts so far, at the moment of this assessment\n",
    "            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n",
    "            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n",
    "            accumulated_correct_attempts += true_attempts \n",
    "            accumulated_uncorrect_attempts += false_attempts\n",
    "            \n",
    "            # ----------------------------------------------\n",
    "            ac_true_attempts_title['ata_' + session_title_text] += true_attempts\n",
    "            ac_false_attempts_title['afa_' + session_title_text] += false_attempts\n",
    "            \n",
    "            \n",
    "            last_game_time_title['lgt_' + session_title_text] = session['game_time'].iloc[-1]\n",
    "            ac_game_time_title['agt_' + session_title_text] += session['game_time'].iloc[-1]\n",
    "            # ----------------------------------------------\n",
    "            \n",
    "            # the time spent in the app so far\n",
    "            if durations == []:\n",
    "                features['duration_mean'] = 0\n",
    "                features['duration_std'] = 0\n",
    "                features['last_duration'] = 0\n",
    "                features['duration_max'] = 0\n",
    "            else:\n",
    "                features['duration_mean'] = np.mean(durations)\n",
    "                features['duration_std'] = np.std(durations)\n",
    "                features['last_duration'] = durations[-1]\n",
    "                features['duration_max'] = np.max(durations)\n",
    "            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            \n",
    "            if durations_game == []:\n",
    "                features['duration_game_mean'] = 0\n",
    "                features['duration_game_std'] = 0\n",
    "                features['game_last_duration'] = 0\n",
    "                features['game_max_duration'] = 0\n",
    "            else:\n",
    "                features['duration_game_mean'] = np.mean(durations_game)\n",
    "                features['duration_game_std'] = np.std(durations_game)\n",
    "                features['game_last_duration'] = durations_game[-1]\n",
    "                features['game_max_duration'] = np.max(durations_game)\n",
    "                \n",
    "            if durations_activity == []:\n",
    "                features['duration_activity_mean'] = 0\n",
    "                features['duration_activity_std'] = 0\n",
    "                features['game_activity_duration'] = 0\n",
    "                features['game_activity_max'] = 0\n",
    "            else:\n",
    "                features['duration_activity_mean'] = np.mean(durations_activity)\n",
    "                features['duration_activity_std'] = np.std(durations_activity)\n",
    "                features['game_activity_duration'] = durations_activity[-1]\n",
    "                features['game_activity_max'] = np.max(durations_activity)\n",
    "                \n",
    "            features[\"misses\"] = miss\n",
    "            if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                features[\"game_true\"] = crys_game_true\n",
    "                features[\"game_false\"] = crys_game_false\n",
    "                features['game_accuracy'] = crys_game_true / (crys_game_true + crys_game_false) if (crys_game_true + crys_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(crys_game_acc) if len(crys_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = crys_game_acc[-1] if len(crys_game_acc) >=1 else 0\n",
    "                features[\"act_true\"] = crys_act_true\n",
    "                features[\"act_false\"] = crys_act_false\n",
    "                features['act_accuracy'] = crys_act_true / (crys_act_true + crys_act_false) if (crys_act_true + crys_act_false) != 0 else 0\n",
    "                features[\"act_accuracy_std\"] = np.std(crys_act_acc) if len(crys_act_acc) >=1 else 0\n",
    "                features[\"last_act_acc\"] = crys_act_acc[-1] if len(crys_act_acc) >=1 else 0\n",
    "            elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                features[\"game_true\"] = tree_game_true\n",
    "                features[\"game_false\"] = tree_game_false\n",
    "                features['game_accuracy'] = tree_game_true / (tree_game_true + tree_game_false) if (tree_game_true + tree_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(tree_game_acc) if len(tree_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = tree_game_acc[-1] if len(tree_game_acc) >=1 else 0\n",
    "                features[\"act_true\"] = tree_act_true\n",
    "                features[\"act_false\"] = tree_act_false\n",
    "                features['act_accuracy'] = tree_act_true / (tree_act_true + tree_act_false) if (tree_act_true + tree_act_false) != 0 else 0\n",
    "                features[\"act_accuracy_std\"] = np.std(tree_act_acc) if len(tree_act_acc) >=1 else 0\n",
    "                features[\"last_act_acc\"] = tree_act_acc[-1] if len(tree_act_acc) >=1 else 0\n",
    "            elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                features[\"game_true\"] = magma_game_true\n",
    "                features[\"game_false\"] = magma_game_false\n",
    "                features['game_accuracy'] = magma_game_true / (magma_game_true + magma_game_false) if (magma_game_true + magma_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(magma_game_acc) if len(magma_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = magma_game_acc[-1] if len(magma_game_acc) >=1 else 0\n",
    "                features[\"act_true\"] = magma_act_true\n",
    "                features[\"act_false\"] = magma_act_false\n",
    "                features['act_accuracy'] = magma_act_true / (magma_act_true + magma_act_false) if (magma_act_true + magma_act_false) != 0 else 0\n",
    "                features[\"act_accuracy_std\"] = np.std(magma_act_acc) if len(magma_act_acc) >=1 else 0\n",
    "                features[\"last_act_acc\"] = magma_act_acc[-1] if len(magma_act_acc) >=1 else 0\n",
    "            \n",
    "            # the accuracy is the all time wins divided by the all time attempts\n",
    "            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n",
    "            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n",
    "            accumulated_accuracy += accuracy\n",
    "            last_accuracy_title['acc_' + session_title_text] = accuracy\n",
    "            # a feature of the current accuracy categorized\n",
    "            # it is a counter of how many times this player was in each accuracy group\n",
    "            if accuracy == 0:\n",
    "                features['accuracy_group'] = 0\n",
    "            elif accuracy == 1:\n",
    "                features['accuracy_group'] = 3\n",
    "            elif accuracy == 0.5:\n",
    "                features['accuracy_group'] = 2\n",
    "            else:\n",
    "                features['accuracy_group'] = 1\n",
    "            features.update(accuracy_groups)\n",
    "            accuracy_groups[features['accuracy_group']] += 1\n",
    "            # mean of the all accuracy groups of this player\n",
    "            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n",
    "            accumulated_accuracy_group += features['accuracy_group']\n",
    "            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n",
    "            features['accumulated_actions'] = accumulated_actions\n",
    "            \n",
    "            # there are some conditions to allow this features to be inserted in the datasets\n",
    "            # if it's a test set, all sessions belong to the final dataset\n",
    "            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n",
    "            # that means, must exist an event_code 4100 or 4110\n",
    "            if test_set:\n",
    "                all_assessments.append(features)\n",
    "            elif true_attempts+false_attempts > 0:\n",
    "                all_assessments.append(features)\n",
    "                \n",
    "            counter += 1\n",
    "            \n",
    "        if session_type == 'Game':\n",
    "            durations_game.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            true = session['event_data'].str.contains('true').sum()\n",
    "            false = session['event_data'].str.contains('false').sum() \n",
    "            durations_game.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                crys_game_true += true\n",
    "                crys_game_false += false\n",
    "                crys_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "            elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                tree_game_true += true\n",
    "                tree_game_false += false\n",
    "                tree_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "            elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                magma_game_true += true\n",
    "                magma_game_false += false\n",
    "                magma_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        if session_type == 'Activity':\n",
    "            durations_activity.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            true = session['event_data'].str.contains('true').sum()\n",
    "            false = session['event_data'].str.contains('false').sum() \n",
    "            durations_activity.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                crys_act_true += true\n",
    "                crys_act_false += false\n",
    "                crys_act_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "            elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                tree_act_true += true\n",
    "                tree_act_false += false\n",
    "                tree_act_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "            elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                magma_act_true += true\n",
    "                magma_act_false += false\n",
    "                magma_act_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "            else:\n",
    "                pass    \n",
    "        \n",
    "        session_count += 1\n",
    "        # this piece counts how many actions was made in each event_code so far\n",
    "        def update_counters(counter: dict, col: str):\n",
    "                num_of_session_count = Counter(session[col])\n",
    "                for k in num_of_session_count.keys():\n",
    "                    x = k\n",
    "                    if col == 'title':\n",
    "                        x = activities_labels[k]\n",
    "                    counter[x] += num_of_session_count[k]\n",
    "                return counter\n",
    "            \n",
    "        event_code_count = update_counters(event_code_count, \"event_code\")\n",
    "        event_id_count = update_counters(event_id_count, \"event_id\")\n",
    "        title_count = update_counters(title_count, 'title')\n",
    "        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n",
    "\n",
    "        # counts how many actions the player has done so far, used in the feature of the same name\n",
    "        accumulated_actions += len(session)\n",
    "        if last_activity != session_type:\n",
    "            user_activities_count[session_type] += 1\n",
    "            last_activitiy = session_type \n",
    "                        \n",
    "    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n",
    "    if test_set:\n",
    "        return all_assessments[-1]\n",
    "    # in the train_set, all assessments goes to the dataset\n",
    "    return all_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test(train, test):\n",
    "    compiled_train = []\n",
    "    compiled_test = []\n",
    "    compiled_test_his = []\n",
    "    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total = 17000):\n",
    "        compiled_train += get_data(user_sample)\n",
    "    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n",
    "        test_data = get_data(user_sample, test_set = True)\n",
    "        compiled_test.append(test_data)\n",
    "    for i, (ins_id, user_sample) in tqdm(enumerate(test.groupby('installation_id', sort = False)), total = 1000):\n",
    "        compiled_test_his += get_data(user_sample)\n",
    "    reduce_train = pd.DataFrame(compiled_train)\n",
    "    reduce_test = pd.DataFrame(compiled_test)\n",
    "    reduce_test_his = pd.DataFrame(compiled_test_his)\n",
    "    \n",
    "    return reduce_train, reduce_test, reduce_test_his"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thank to Bruno\n",
    "def eval_qwk_lgb_regr(y_pred, train_t):\n",
    "    \"\"\"\n",
    "    Fast cappa eval function for lgb.\n",
    "    \"\"\"\n",
    "    dist = Counter(train_t['accuracy_group'])\n",
    "    for k in dist:\n",
    "        dist[k] /= len(train_t)\n",
    "    \n",
    "    acum = 0\n",
    "    bound = {}\n",
    "    for i in range(3):\n",
    "        acum += dist[i]\n",
    "        bound[i] = np.percentile(y_pred, acum * 100)\n",
    "\n",
    "    def classify(x):\n",
    "        if x <= bound[0]:\n",
    "            return 0\n",
    "        elif x <= bound[1]:\n",
    "            return 1\n",
    "        elif x <= bound[2]:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    y_pred = np.array(list(map(classify, y_pred)))\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def predict(sample_submission, y_pred):\n",
    "    sample_submission['accuracy_group'] = y_pred\n",
    "    sample_submission['accuracy_group'] = sample_submission['accuracy_group'].astype(int)\n",
    "    sample_submission.to_csv('submission.csv', index = False)\n",
    "    print(sample_submission['accuracy_group'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_assessment(reduce_train):\n",
    "    used_idx = []\n",
    "    for iid in set(reduce_train['installation_id']):\n",
    "        list_ = list(reduce_train[reduce_train['installation_id']==iid].index)\n",
    "        cur = random.choices(list_, k = 1)[0]\n",
    "        used_idx.append(cur)\n",
    "    reduce_train_t = reduce_train.loc[used_idx]\n",
    "    print(\"used validation data: \", len(used_idx))\n",
    "    return reduce_train_t, used_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each validation fold extract one random observation for each installation id to simulate the test set\n",
    "def run_lgb(reduce_train, reduce_test, features):\n",
    "    # features found in initial bayesian optimization\n",
    "    params = {'boosting_type': 'gbdt', \n",
    "              'metric': 'rmse', \n",
    "              'objective': 'regression', \n",
    "              'eval_metric': 'cappa', \n",
    "              'n_jobs': -1, \n",
    "              'seed': 42, \n",
    "              'num_leaves': 26, \n",
    "              'learning_rate': 0.077439684887749, \n",
    "              'max_depth': 33, \n",
    "              'lambda_l1': 3.27791989030057, \n",
    "              'lambda_l2': 1.3047627805931334, \n",
    "              'bagging_fraction': 0.896924978584253, \n",
    "              'bagging_freq': 1, \n",
    "              'colsample_bytree': 0.8710772167017853}\n",
    "\n",
    "    kf = GroupKFold(n_splits = 5)\n",
    "    target = 'accuracy_group'\n",
    "    oof_pred = np.zeros(len(reduce_train))\n",
    "    y_pred = np.zeros(len(reduce_test))\n",
    "    ind = []\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train, groups = reduce_train['installation_id'])):\n",
    "        print('Fold:', fold + 1)\n",
    "        x_train, x_val = reduce_train[features].iloc[tr_ind], reduce_train[features].iloc[val_ind]\n",
    "        y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n",
    "        x_train.drop('installation_id', inplace = True, axis = 1)\n",
    "        train_set = lgb.Dataset(x_train, y_train, categorical_feature = ['session_title'])\n",
    "\n",
    "\n",
    "        x_val, idx_val = get_random_assessment(x_val)\n",
    "        ind.extend(idx_val)\n",
    "        x_val.drop('installation_id', inplace = True, axis = 1)\n",
    "        y_val = y_val.loc[idx_val]\n",
    "        val_set = lgb.Dataset(x_val, y_val, categorical_feature = ['session_title'])\n",
    "\n",
    "        model = lgb.train(params, train_set, num_boost_round = 100000, early_stopping_rounds = 100, \n",
    "                         valid_sets = [train_set, val_set], verbose_eval = 100)\n",
    "\n",
    "        oof_pred[idx_val] = model.predict(x_val)\n",
    "        y_pred += model.predict(reduce_test[[x for x in features if x not in ['installation_id']]]) / kf.n_splits\n",
    "    oof_rmse_score = np.sqrt(mean_squared_error(reduce_train[target][ind], oof_pred[ind]))\n",
    "    oof_cohen_score = cohen_kappa_score(reduce_train[target][ind], eval_qwk_lgb_regr(oof_pred[ind], reduce_train), weights = 'quadratic')\n",
    "    print('Our oof rmse score is:', oof_rmse_score)\n",
    "    print('Our oof cohen kappa score is:', oof_cohen_score)\n",
    "    return y_pred, oof_rmse_score, oof_cohen_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each validation fold extract one random observation for each installation id to simulate the test set\n",
    "def run_lr(reduce_train, reduce_test, features):\n",
    "    kf = GroupKFold(n_splits = 5)\n",
    "    target = 'accuracy_group'\n",
    "    oof_pred = np.zeros(len(reduce_train))\n",
    "    y_pred = np.zeros(len(reduce_test))\n",
    "    ind = []\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train, groups = reduce_train['installation_id'])):\n",
    "        print('Fold:', fold + 1)\n",
    "        x_train, x_val = reduce_train[features].iloc[tr_ind], reduce_train[features].iloc[val_ind]\n",
    "        y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n",
    "        x_train.drop('installation_id', inplace = True, axis = 1)\n",
    "\n",
    "        x_val, idx_val = get_random_assessment(x_val)\n",
    "        ind.extend(idx_val)\n",
    "        x_val.drop('installation_id', inplace = True, axis = 1)\n",
    "        y_val = y_val.loc[idx_val]\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(x_train, y_train)  \n",
    "\n",
    "        oof_pred[idx_val] = model.predict(x_val)\n",
    "        y_pred += model.predict(reduce_test[[x for x in features if x not in ['installation_id']]]) / kf.n_splits\n",
    "    oof_rmse_score = np.sqrt(mean_squared_error(reduce_train[target][ind], oof_pred[ind]))\n",
    "    oof_cohen_score = cohen_kappa_score(reduce_train[target][ind], eval_qwk_lgb_regr(oof_pred[ind], reduce_train), weights = 'quadratic')\n",
    "    print('Our oof rmse score is:', oof_rmse_score)\n",
    "    print('Our oof cohen kappa score is:', oof_cohen_score)\n",
    "    return y_pred, oof_rmse_score, oof_cohen_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def run_nn(reduce_train, reduce_test, features):\n",
    "    kf = GroupKFold(n_splits = 5)\n",
    "    target = 'accuracy_group'\n",
    "    oof_pred = np.zeros(len(reduce_train))\n",
    "    y_pred = np.zeros(len(reduce_test))\n",
    "    ind = []\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train, groups = reduce_train['installation_id'])):\n",
    "        print('Fold:', fold + 1)\n",
    "        x_train, x_val = reduce_train[features].iloc[tr_ind], reduce_train[features].iloc[val_ind]\n",
    "        y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n",
    "        x_train.drop('installation_id', inplace = True, axis = 1)\n",
    "\n",
    "        x_val, idx_val = get_random_assessment(x_val)\n",
    "        ind.extend(idx_val)\n",
    "        x_val.drop('installation_id', inplace = True, axis = 1)\n",
    "        y_val = y_val.loc[idx_val]\n",
    "        \n",
    "        verbosity = 100\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(x_train.shape[1],)),\n",
    "            tf.keras.layers.Dense(200, activation='relu'), #, kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(100, activation='tanh'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(25, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1, activation='relu')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mse')\n",
    "        #print(model.summary())\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('./nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "        \n",
    "        model.fit(x_train, \n",
    "                y_train, \n",
    "                validation_data=(x_val, y_val),\n",
    "                epochs=100,\n",
    "                 callbacks=[save_best, early_stop])\n",
    "        model.load_weights('./nn_model.w8')\n",
    "        \n",
    "        oof_pred[idx_val] = model.predict(x_val).reshape(x_val.shape[0],)\n",
    "        y_pred += model.predict(reduce_test[[x for x in features if x not in ['installation_id']]]).reshape(reduce_test.shape[0],) / kf.n_splits\n",
    "    oof_rmse_score = np.sqrt(mean_squared_error(reduce_train[target][ind], oof_pred[ind]))\n",
    "    oof_cohen_score = cohen_kappa_score(reduce_train[target][ind], eval_qwk_lgb_regr(oof_pred[ind], reduce_train), weights = 'quadratic')\n",
    "    print('Our oof rmse score is:', oof_rmse_score)\n",
    "    print('Our oof cohen kappa score is:', oof_cohen_score)\n",
    "    return y_pred, oof_rmse_score, oof_cohen_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(reduce_train, reduce_test):\n",
    "    features = [i for i in reduce_train.columns if i not in [\"installation_id\", \"accuracy_group\"]]\n",
    "    categoricals = ['session_title']\n",
    "    features = features.copy()\n",
    "    new_train = reduce_train.copy()\n",
    "    new_test = reduce_test.copy()\n",
    "    if len(categoricals) > 0:\n",
    "        for cat in categoricals:\n",
    "            enc = OneHotEncoder()\n",
    "            train_cats = enc.fit_transform(new_train[[cat]])\n",
    "            test_cats = enc.transform(new_test[[cat]])\n",
    "            cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.active_features_]\n",
    "            features += cat_cols\n",
    "            train_cats = pd.DataFrame(train_cats.toarray(), columns=cat_cols)\n",
    "            test_cats = pd.DataFrame(test_cats.toarray(), columns=cat_cols)\n",
    "            new_train = pd.concat([new_train, train_cats], axis=1)\n",
    "            new_test = pd.concat([new_test, test_cats], axis=1)\n",
    "        scalar = MinMaxScaler()\n",
    "        new_train[features] = scalar.fit_transform(new_train[features])\n",
    "        new_test[features] = scalar.transform(new_test[features])\n",
    "    new_train = new_train.drop([\"session_title\"], axis=1)\n",
    "    new_test = new_test.drop([\"session_title\"], axis=1)\n",
    "    return new_train, new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_correlated_features(reduce_train):\n",
    "    counter = 0\n",
    "    to_remove = []\n",
    "    for feat_a in features:\n",
    "        for feat_b in features:\n",
    "            if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n",
    "                c = np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1]\n",
    "                if c > 0.995:\n",
    "                    counter += 1\n",
    "                    to_remove.append(feat_b)\n",
    "                    print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))\n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to exclude columns from the train and test set if the mean is different, also adjust test column by a factor to simulate the same distribution\n",
    "def exclude(reduce_train, reduce_test, features):\n",
    "    to_exclude = [] \n",
    "    ajusted_test = reduce_test.copy()\n",
    "    for feature in features:\n",
    "        if feature not in ['accuracy_group', 'installation_id', 'session_title']:\n",
    "            data = reduce_train[feature]\n",
    "            train_mean = data.mean()\n",
    "            data = ajusted_test[feature] \n",
    "            test_mean = data.mean()\n",
    "            try:\n",
    "                ajust_factor = train_mean / test_mean\n",
    "                if ajust_factor > 10 or ajust_factor < 0.1:# or error > 0.01:\n",
    "                    to_exclude.append(feature)\n",
    "                    print(feature)\n",
    "                else:\n",
    "                    ajusted_test[feature] *= ajust_factor\n",
    "            except:\n",
    "                to_exclude.append(feature)\n",
    "                print(feature)\n",
    "    return to_exclude, ajusted_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train.csv file....\n",
      "Reading test.csv file....\n",
      "Reading train_labels.csv file....\n",
      "Reading specs.csv file....\n",
      "Reading sample_submission.csv file....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c022f6d5941546109d7a7727ae8454ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=17000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86ce07eabda48109e4500a92c2ba05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac056ca5a6c405da752321a04b6a9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train, test, train_labels, specs, sample_submission = read_data()\n",
    "train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code, activities_world = encode_title(train, test, train_labels)\n",
    "#train = remove_data(train, train_labels)\n",
    "reduce_train, reduce_test, reduce_his_test = get_train_and_test(train, test)\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted from feature elimination first round script\n",
    "old_features = list(reduce_train.columns[0:95]) + list(reduce_train.columns[882:])\n",
    "el_features = ['accuracy_group', 'accuracy', 'installation_id']\n",
    "old_features = [col for col in old_features if col not in el_features]\n",
    "event_id_features = list_of_event_id #list(reduce_train.columns[95:479])\n",
    "title_event_code_cross = all_title_event_code #list(reduce_train.columns[479:882])\n",
    "features = old_features + event_id_features + title_event_code_cross\n",
    "lr_features = features.copy()\n",
    "nn_features = features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: FEAT_A: Clip FEAT_B: 27253bdc - Correlation: 0.9999999999999999\n",
      "2: FEAT_A: 2050 FEAT_B: 2040 - Correlation: 0.9965259434878118\n",
      "3: FEAT_A: 2050 FEAT_B: 26fd2d99 - Correlation: 0.9965084543995759\n",
      "4: FEAT_A: 2050 FEAT_B: 37c53127 - Correlation: 1.0\n",
      "5: FEAT_A: 2050 FEAT_B: dcaede90 - Correlation: 0.9965259434878118\n",
      "6: FEAT_A: 2050 FEAT_B: 73757a5e - Correlation: 0.9998050146713992\n",
      "7: FEAT_A: 2050 FEAT_B: 2b9272f4 - Correlation: 0.9999839030068793\n",
      "8: FEAT_A: 2050 FEAT_B: 08fd73f3 - Correlation: 0.9966123918733654\n",
      "9: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_2030 - Correlation: 0.9966123918733654\n",
      "10: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_3121 - Correlation: 0.9999839030068793\n",
      "11: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_2020 - Correlation: 0.9965084543995759\n",
      "12: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_2040 - Correlation: 0.9965259434878118\n",
      "13: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_2050 - Correlation: 1.0\n",
      "14: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_3021 - Correlation: 0.9998050146713992\n",
      "15: FEAT_A: 4230 FEAT_B: 4235 - Correlation: 0.9999995197498746\n",
      "16: FEAT_A: 4230 FEAT_B: ad148f58 - Correlation: 0.9999999999999998\n",
      "17: FEAT_A: 4230 FEAT_B: 85de926c - Correlation: 0.9999995197498746\n",
      "18: FEAT_A: 4230 FEAT_B: Bubble Bath_4235 - Correlation: 0.9999995197498746\n",
      "19: FEAT_A: 4230 FEAT_B: Bubble Bath_4230 - Correlation: 0.9999999999999998\n",
      "20: FEAT_A: 5000 FEAT_B: 5010 - Correlation: 0.9991849213605333\n",
      "21: FEAT_A: 5000 FEAT_B: 71e712d8 - Correlation: 0.9991849213605333\n",
      "22: FEAT_A: 5000 FEAT_B: a6d66e51 - Correlation: 1.0\n",
      "23: FEAT_A: 5000 FEAT_B: Watering Hole (Activity)_5010 - Correlation: 0.9991849213605333\n",
      "24: FEAT_A: 5000 FEAT_B: Watering Hole (Activity)_5000 - Correlation: 1.0\n",
      "25: FEAT_A: 3110 FEAT_B: 3010 - Correlation: 0.9999293402893735\n",
      "26: FEAT_A: 3120 FEAT_B: 3020 - Correlation: 0.9998761417908972\n",
      "27: FEAT_A: 3121 FEAT_B: 3021 - Correlation: 0.9999098200487934\n",
      "28: FEAT_A: 4031 FEAT_B: 1996c610 - Correlation: 1.0\n",
      "29: FEAT_A: 4031 FEAT_B: Dino Drink_4031 - Correlation: 1.0\n",
      "30: FEAT_A: 2000 FEAT_B: installation_session_count - Correlation: 1.0\n",
      "31: FEAT_A: 4050 FEAT_B: a1192f43 - Correlation: 0.9999999999999999\n",
      "32: FEAT_A: 4050 FEAT_B: Crystals Rule_4050 - Correlation: 0.9999999999999999\n",
      "33: FEAT_A: 2020 FEAT_B: 2030 - Correlation: 0.9959933262816534\n",
      "34: FEAT_A: 4220 FEAT_B: 1340b8d7 - Correlation: 1.0\n",
      "35: FEAT_A: 4220 FEAT_B: Bubble Bath_4220 - Correlation: 1.0\n",
      "36: FEAT_A: Tree Top City - Level 2 FEAT_B: Tree Top City - Level 2_2000 - Correlation: 1.0\n",
      "37: FEAT_A: Costume Box FEAT_B: Costume Box_2000 - Correlation: 1.0\n",
      "38: FEAT_A: 12 Monkeys FEAT_B: 12 Monkeys_2000 - Correlation: 1.0\n",
      "39: FEAT_A: Crystal Caves - Level 1 FEAT_B: Crystal Caves - Level 1_2000 - Correlation: 1.0\n",
      "40: FEAT_A: Welcome to Lost Lagoon! FEAT_B: Welcome to Lost Lagoon!_2000 - Correlation: 1.0\n",
      "41: FEAT_A: Magma Peak - Level 1 FEAT_B: Magma Peak - Level 1_2000 - Correlation: 1.0\n",
      "42: FEAT_A: Ordering Spheres FEAT_B: Ordering Spheres_2000 - Correlation: 1.0\n",
      "43: FEAT_A: Lifting Heavy Things FEAT_B: Lifting Heavy Things_2000 - Correlation: 0.9999999999999999\n",
      "44: FEAT_A: Magma Peak - Level 2 FEAT_B: Magma Peak - Level 2_2000 - Correlation: 1.0\n",
      "45: FEAT_A: Balancing Act FEAT_B: Balancing Act_2000 - Correlation: 1.0\n",
      "46: FEAT_A: Slop Problem FEAT_B: Slop Problem_2000 - Correlation: 1.0\n",
      "47: FEAT_A: Tree Top City - Level 3 FEAT_B: Tree Top City - Level 3_2000 - Correlation: 1.0\n",
      "48: FEAT_A: Crystal Caves - Level 2 FEAT_B: Crystal Caves - Level 2_2000 - Correlation: 1.0\n",
      "49: FEAT_A: Honey Cake FEAT_B: Honey Cake_2000 - Correlation: 1.0\n",
      "50: FEAT_A: Bottle Filler (Activity) FEAT_B: bb3e370b - Correlation: 0.9950043311420306\n",
      "51: FEAT_A: Bottle Filler (Activity) FEAT_B: Bottle Filler (Activity)_4030 - Correlation: 0.9950043311420306\n",
      "52: FEAT_A: Tree Top City - Level 1 FEAT_B: Tree Top City - Level 1_2000 - Correlation: 0.9999999999999999\n",
      "53: FEAT_A: Pirate's Tale FEAT_B: Pirate's Tale_2000 - Correlation: 0.9999999999999999\n",
      "54: FEAT_A: Heavy, Heavier, Heaviest FEAT_B: Heavy, Heavier, Heaviest_2000 - Correlation: 1.0\n",
      "55: FEAT_A: Rulers FEAT_B: Rulers_2000 - Correlation: 1.0\n",
      "56: FEAT_A: Crystal Caves - Level 3 FEAT_B: Crystal Caves - Level 3_2000 - Correlation: 1.0\n",
      "57: FEAT_A: Treasure Map FEAT_B: Treasure Map_2000 - Correlation: 1.0\n",
      "58: FEAT_A: lgt_Mushroom Sorter (Assessment) FEAT_B: agt_Mushroom Sorter (Assessment) - Correlation: 0.9954655260051172\n",
      "59: FEAT_A: ata_Mushroom Sorter (Assessment) FEAT_B: 6c930e6e - Correlation: 0.9985426312249887\n",
      "60: FEAT_A: ata_Mushroom Sorter (Assessment) FEAT_B: Mushroom Sorter (Assessment)_2030 - Correlation: 0.9985426312249887\n",
      "61: FEAT_A: ata_Cart Balancer (Assessment) FEAT_B: a8876db3 - Correlation: 0.9999170128095376\n",
      "62: FEAT_A: ata_Cart Balancer (Assessment) FEAT_B: Cart Balancer (Assessment)_3021 - Correlation: 0.9999170128095376\n",
      "63: FEAT_A: ata_Bird Measurer (Assessment) FEAT_B: 7525289a - Correlation: 0.9981555049446889\n",
      "64: FEAT_A: ata_Bird Measurer (Assessment) FEAT_B: 45d01abe - Correlation: 1.0\n",
      "65: FEAT_A: ata_Bird Measurer (Assessment) FEAT_B: Bird Measurer (Assessment)_3021 - Correlation: 1.0\n",
      "66: FEAT_A: ata_Bird Measurer (Assessment) FEAT_B: Bird Measurer (Assessment)_3121 - Correlation: 0.9981555049446889\n",
      "67: FEAT_A: ata_Chest Sorter (Assessment) FEAT_B: 3afb49e6 - Correlation: 0.9996140232445475\n",
      "68: FEAT_A: ata_Chest Sorter (Assessment) FEAT_B: e4f1efe6 - Correlation: 0.9981999245282789\n",
      "69: FEAT_A: ata_Chest Sorter (Assessment) FEAT_B: Chest Sorter (Assessment)_3021 - Correlation: 0.9996140232445475\n",
      "70: FEAT_A: ata_Chest Sorter (Assessment) FEAT_B: Chest Sorter (Assessment)_3121 - Correlation: 0.9981999245282789\n",
      "71: FEAT_A: afa_Mushroom Sorter (Assessment) FEAT_B: 160654fd - Correlation: 0.9999996751514757\n",
      "72: FEAT_A: afa_Mushroom Sorter (Assessment) FEAT_B: 88d4a5be - Correlation: 0.9989293679278667\n",
      "73: FEAT_A: afa_Mushroom Sorter (Assessment) FEAT_B: Mushroom Sorter (Assessment)_3020 - Correlation: 0.9999996751514757\n",
      "74: FEAT_A: afa_Mushroom Sorter (Assessment) FEAT_B: Mushroom Sorter (Assessment)_3120 - Correlation: 0.9989293679278667\n",
      "75: FEAT_A: afa_Bird Measurer (Assessment) FEAT_B: e37a2b78 - Correlation: 0.9983241920747599\n",
      "76: FEAT_A: afa_Bird Measurer (Assessment) FEAT_B: ad2fc29c - Correlation: 0.9991605930121138\n",
      "77: FEAT_A: afa_Bird Measurer (Assessment) FEAT_B: 17113b36 - Correlation: 0.998391883995427\n",
      "78: FEAT_A: afa_Bird Measurer (Assessment) FEAT_B: Bird Measurer (Assessment)_4110 - Correlation: 0.998391883995427\n",
      "79: FEAT_A: afa_Bird Measurer (Assessment) FEAT_B: Bird Measurer (Assessment)_3020 - Correlation: 0.9991605930121138\n",
      "80: FEAT_A: afa_Bird Measurer (Assessment) FEAT_B: Bird Measurer (Assessment)_3120 - Correlation: 0.9983241920747599\n",
      "81: FEAT_A: afa_Chest Sorter (Assessment) FEAT_B: ea296733 - Correlation: 0.9999880332296727\n",
      "82: FEAT_A: afa_Chest Sorter (Assessment) FEAT_B: df4fe8b6 - Correlation: 0.9972434688333254\n",
      "83: FEAT_A: afa_Chest Sorter (Assessment) FEAT_B: Chest Sorter (Assessment)_3020 - Correlation: 0.9999880332296727\n",
      "84: FEAT_A: afa_Chest Sorter (Assessment) FEAT_B: Chest Sorter (Assessment)_3120 - Correlation: 0.9972434688333254\n",
      "85: FEAT_A: var_event_id FEAT_B: var_title_event_code - Correlation: 0.9995254184300616\n",
      "86: FEAT_A: 857f21c0 FEAT_B: Bubble Bath_4040 - Correlation: 1.0\n",
      "87: FEAT_A: bbfe0445 FEAT_B: 56cd3b43 - Correlation: 0.9996926215355526\n",
      "88: FEAT_A: bbfe0445 FEAT_B: Flower Waterer (Activity)_3010 - Correlation: 0.9996926215355526\n",
      "89: FEAT_A: bbfe0445 FEAT_B: Flower Waterer (Activity)_3110 - Correlation: 1.0\n",
      "90: FEAT_A: 47026d5f FEAT_B: cb6010f8 - Correlation: 0.9996901555713447\n",
      "91: FEAT_A: 47026d5f FEAT_B: 56817e2b - Correlation: 0.9993683693704368\n",
      "92: FEAT_A: 47026d5f FEAT_B: Chow Time_3021 - Correlation: 1.0\n",
      "93: FEAT_A: 47026d5f FEAT_B: Chow Time_3121 - Correlation: 0.9996901555713447\n",
      "94: FEAT_A: 47026d5f FEAT_B: Chow Time_2030 - Correlation: 0.9993683693704368\n",
      "95: FEAT_A: 5f5b2617 FEAT_B: Bottle Filler (Activity)_4080 - Correlation: 1.0\n",
      "96: FEAT_A: 9e34ea74 FEAT_B: Egg Dropper (Activity)_4070 - Correlation: 1.0\n",
      "97: FEAT_A: 8d7e386c FEAT_B: 69fdac0a - Correlation: 0.9996590210382708\n",
      "98: FEAT_A: 8d7e386c FEAT_B: Happy Camel_3010 - Correlation: 1.0\n",
      "99: FEAT_A: 8d7e386c FEAT_B: Happy Camel_3110 - Correlation: 0.9996590210382708\n",
      "100: FEAT_A: d88ca108 FEAT_B: Air Show_2070 - Correlation: 1.0\n",
      "101: FEAT_A: 00c73085 FEAT_B: Dino Dive_2030 - Correlation: 0.9999999999999999\n",
      "102: FEAT_A: e080a381 FEAT_B: Pan Balance_4090 - Correlation: 1.0\n",
      "103: FEAT_A: fbaf3456 FEAT_B: Mushroom Sorter (Assessment)_4030 - Correlation: 0.9999999999999999\n",
      "104: FEAT_A: 3d63345e FEAT_B: Cart Balancer (Assessment)_4035 - Correlation: 1.0\n",
      "105: FEAT_A: 4901243f FEAT_B: Fireworks (Activity)_2000 - Correlation: 1.0\n",
      "106: FEAT_A: abc5811c FEAT_B: Happy Camel_4010 - Correlation: 1.0\n",
      "107: FEAT_A: 55115cbd FEAT_B: 6f4adc4b - Correlation: 0.9997831892615398\n",
      "108: FEAT_A: 55115cbd FEAT_B: Bubble Bath_3121 - Correlation: 0.9999999999999999\n",
      "109: FEAT_A: 55115cbd FEAT_B: Bubble Bath_3021 - Correlation: 0.9997831892615398\n",
      "110: FEAT_A: e9c52111 FEAT_B: b7530680 - Correlation: 0.998817689964623\n",
      "111: FEAT_A: e9c52111 FEAT_B: Bottle Filler (Activity)_2030 - Correlation: 1.0\n",
      "112: FEAT_A: e9c52111 FEAT_B: Bottle Filler (Activity)_2020 - Correlation: 0.998817689964623\n",
      "113: FEAT_A: c277e121 FEAT_B: b120f2ac - Correlation: 0.9999983835744551\n",
      "114: FEAT_A: c277e121 FEAT_B: d45ed6a1 - Correlation: 0.9979692879543319\n",
      "115: FEAT_A: c277e121 FEAT_B: All Star Sorting_2025 - Correlation: 0.9999983835744551\n",
      "116: FEAT_A: c277e121 FEAT_B: All Star Sorting_3020 - Correlation: 1.0\n",
      "117: FEAT_A: c277e121 FEAT_B: All Star Sorting_3120 - Correlation: 0.9979692879543319\n",
      "118: FEAT_A: 1c178d24 FEAT_B: 250513af - Correlation: 0.9999803429610506\n",
      "119: FEAT_A: 1c178d24 FEAT_B: a592d54e - Correlation: 0.9973125883100831\n",
      "120: FEAT_A: 1c178d24 FEAT_B: cf7638f3 - Correlation: 0.9996196393003264\n",
      "121: FEAT_A: 1c178d24 FEAT_B: Pan Balance_3021 - Correlation: 0.9999803429610506\n",
      "122: FEAT_A: 1c178d24 FEAT_B: Pan Balance_2030 - Correlation: 1.0\n",
      "123: FEAT_A: 1c178d24 FEAT_B: Pan Balance_3121 - Correlation: 0.9996196393003264\n",
      "124: FEAT_A: 1c178d24 FEAT_B: Pan Balance_2020 - Correlation: 0.9973125883100831\n",
      "125: FEAT_A: 9554a50b FEAT_B: Cauldron Filler (Assessment)_4080 - Correlation: 0.9999999999999999\n",
      "126: FEAT_A: 7dfe6d8a FEAT_B: Leaf Leader_4070 - Correlation: 0.9999999999999998\n",
      "127: FEAT_A: 44cb4907 FEAT_B: 8b757ab8 - Correlation: 0.999835058794711\n",
      "128: FEAT_A: 44cb4907 FEAT_B: Crystals Rule_3020 - Correlation: 1.0\n",
      "129: FEAT_A: 44cb4907 FEAT_B: Crystals Rule_3120 - Correlation: 0.999835058794711\n",
      "130: FEAT_A: 7d093bf9 FEAT_B: Chow Time_2000 - Correlation: 0.9999999999999999\n",
      "131: FEAT_A: 99ea62f3 FEAT_B: Bubble Bath_2083 - Correlation: 1.0\n",
      "132: FEAT_A: 804ee27f FEAT_B: Pan Balance_4020 - Correlation: 1.0\n",
      "133: FEAT_A: 5be391b5 FEAT_B: Dino Drink_4010 - Correlation: 1.0\n",
      "134: FEAT_A: 01ca3a3c FEAT_B: Leaf Leader_4080 - Correlation: 0.9999999999999999\n",
      "135: FEAT_A: a16a373e FEAT_B: Bird Measurer (Assessment)_4070 - Correlation: 1.0\n",
      "136: FEAT_A: 67aa2ada FEAT_B: Leaf Leader_4090 - Correlation: 1.0\n",
      "137: FEAT_A: 87d743c1 FEAT_B: Dino Dive_4010 - Correlation: 1.0\n",
      "138: FEAT_A: 0d1da71f FEAT_B: Chow Time_3110 - Correlation: 1.0\n",
      "139: FEAT_A: 5e109ec3 FEAT_B: Cart Balancer (Assessment)_4030 - Correlation: 1.0\n",
      "140: FEAT_A: 47f43a44 FEAT_B: Flower Waterer (Activity)_4090 - Correlation: 1.0\n",
      "141: FEAT_A: e57dd7af FEAT_B: 763fc34e - Correlation: 0.9972721980394412\n",
      "142: FEAT_A: e57dd7af FEAT_B: Leaf Leader_3020 - Correlation: 0.9972721980394412\n",
      "143: FEAT_A: e57dd7af FEAT_B: Leaf Leader_3120 - Correlation: 1.0\n",
      "144: FEAT_A: 4bb2f698 FEAT_B: Chicken Balancer (Activity)_4070 - Correlation: 0.9999999999999998\n",
      "145: FEAT_A: b2e5b0f1 FEAT_B: b74258a0 - Correlation: 0.999849464604504\n",
      "146: FEAT_A: b2e5b0f1 FEAT_B: ecaab346 - Correlation: 0.999849464604504\n",
      "147: FEAT_A: b2e5b0f1 FEAT_B: Cart Balancer (Assessment)_3121 - Correlation: 0.999849464604504\n",
      "148: FEAT_A: b2e5b0f1 FEAT_B: Cart Balancer (Assessment)_2030 - Correlation: 0.999849464604504\n",
      "149: FEAT_A: b2e5b0f1 FEAT_B: Cart Balancer (Assessment)_2010 - Correlation: 1.0\n",
      "150: FEAT_A: 1cc7cfca FEAT_B: All Star Sorting_4030 - Correlation: 1.0\n",
      "151: FEAT_A: bcceccc6 FEAT_B: Air Show_4070 - Correlation: 1.0\n",
      "152: FEAT_A: a76029ee FEAT_B: Bird Measurer (Assessment)_4040 - Correlation: 0.9999999999999999\n",
      "153: FEAT_A: 16667cc5 FEAT_B: Chicken Balancer (Activity)_4080 - Correlation: 0.9999999999999999\n",
      "154: FEAT_A: 3393b68b FEAT_B: Bird Measurer (Assessment)_2010 - Correlation: 1.0\n",
      "155: FEAT_A: 2a512369 FEAT_B: 33505eae - Correlation: 0.9994585292841954\n",
      "156: FEAT_A: 2a512369 FEAT_B: Leaf Leader_3110 - Correlation: 1.0\n",
      "157: FEAT_A: 2a512369 FEAT_B: Leaf Leader_3010 - Correlation: 0.9994585292841954\n",
      "158: FEAT_A: de26c3a6 FEAT_B: Flower Waterer (Activity)_4020 - Correlation: 0.9999999999999999\n",
      "159: FEAT_A: 9b01374f FEAT_B: Flower Waterer (Activity)_2000 - Correlation: 1.0\n",
      "160: FEAT_A: eb2c19cd FEAT_B: Mushroom Sorter (Assessment)_4090 - Correlation: 1.0\n",
      "161: FEAT_A: bfc77bd6 FEAT_B: Chest Sorter (Assessment)_4080 - Correlation: 0.9999999999999998\n",
      "162: FEAT_A: d122731b FEAT_B: Cart Balancer (Assessment)_4100 - Correlation: 0.9999999999999998\n",
      "163: FEAT_A: 4e5fc6f5 FEAT_B: Cart Balancer (Assessment)_4090 - Correlation: 1.0\n",
      "164: FEAT_A: 5e812b27 FEAT_B: Sandcastle Builder (Activity)_4030 - Correlation: 1.0\n",
      "165: FEAT_A: 08ff79ad FEAT_B: Egg Dropper (Activity)_4090 - Correlation: 1.0\n",
      "166: FEAT_A: c7f7f0e1 FEAT_B: Bug Measurer (Activity)_2000 - Correlation: 1.0\n",
      "167: FEAT_A: c54cf6c5 FEAT_B: d06f75b5 - Correlation: 0.9972383579301799\n",
      "168: FEAT_A: c54cf6c5 FEAT_B: 895865f3 - Correlation: 0.9964317167511039\n",
      "169: FEAT_A: c54cf6c5 FEAT_B: 3bb91dda - Correlation: 0.9999701603895903\n",
      "170: FEAT_A: c54cf6c5 FEAT_B: 8f094001 - Correlation: 0.9958447429109037\n",
      "171: FEAT_A: c54cf6c5 FEAT_B: Bubble Bath_2025 - Correlation: 1.0\n",
      "172: FEAT_A: c54cf6c5 FEAT_B: Bubble Bath_2035 - Correlation: 0.9972383579301799\n",
      "173: FEAT_A: c54cf6c5 FEAT_B: Bubble Bath_2030 - Correlation: 0.9964317167511039\n",
      "174: FEAT_A: c54cf6c5 FEAT_B: Bubble Bath_4020 - Correlation: 0.9999701603895903\n",
      "175: FEAT_A: c54cf6c5 FEAT_B: Bubble Bath_4045 - Correlation: 0.9958447429109037\n",
      "176: FEAT_A: 9c5ef70c FEAT_B: Pan Balance_2000 - Correlation: 1.0\n",
      "177: FEAT_A: a5e9da97 FEAT_B: Pan Balance_4100 - Correlation: 1.0\n",
      "178: FEAT_A: 48349b14 FEAT_B: Crystals Rule_2000 - Correlation: 1.0\n",
      "179: FEAT_A: 6bf9e3e1 FEAT_B: Happy Camel_4040 - Correlation: 0.9999999999999999\n",
      "180: FEAT_A: 5e3ea25a FEAT_B: Crystals Rule_4070 - Correlation: 1.0\n",
      "181: FEAT_A: 37937459 FEAT_B: Sandcastle Builder (Activity)_4090 - Correlation: 0.9999999999999998\n",
      "182: FEAT_A: d3f1e122 FEAT_B: Bottle Filler (Activity)_4035 - Correlation: 1.0\n",
      "183: FEAT_A: 15eb4a7d FEAT_B: 0413e89d - Correlation: 0.9997266832893074\n",
      "184: FEAT_A: 15eb4a7d FEAT_B: Bubble Bath_3110 - Correlation: 0.9999999999999998\n",
      "185: FEAT_A: 15eb4a7d FEAT_B: Bubble Bath_3010 - Correlation: 0.9997266832893074\n",
      "186: FEAT_A: a1bbe385 FEAT_B: f28c589a - Correlation: 0.999953679734225\n",
      "187: FEAT_A: a1bbe385 FEAT_B: Air Show_3010 - Correlation: 0.999953679734225\n",
      "188: FEAT_A: a1bbe385 FEAT_B: Air Show_3110 - Correlation: 1.0\n",
      "189: FEAT_A: 6077cc36 FEAT_B: Bird Measurer (Assessment)_4080 - Correlation: 0.9999999999999998\n",
      "190: FEAT_A: 709b1251 FEAT_B: e3ff61fb - Correlation: 0.9995444786291265\n",
      "191: FEAT_A: 709b1251 FEAT_B: 7961e599 - Correlation: 0.9950004881933643\n",
      "192: FEAT_A: 709b1251 FEAT_B: Dino Dive_2020 - Correlation: 0.9950004881933643\n",
      "193: FEAT_A: 709b1251 FEAT_B: Dino Dive_3121 - Correlation: 1.0\n",
      "194: FEAT_A: 709b1251 FEAT_B: Dino Dive_3021 - Correlation: 0.9995444786291265\n",
      "195: FEAT_A: 4d6737eb FEAT_B: Dino Drink_2070 - Correlation: 1.0\n",
      "196: FEAT_A: 2230fab4 FEAT_B: 0330ab6a - Correlation: 0.9998673365188064\n",
      "197: FEAT_A: 2230fab4 FEAT_B: Chow Time_3020 - Correlation: 0.9998673365188064\n",
      "198: FEAT_A: 2230fab4 FEAT_B: Chow Time_3120 - Correlation: 0.9999999999999999\n",
      "199: FEAT_A: 9de5e594 FEAT_B: 28a4eb9a - Correlation: 0.9995923561196808\n",
      "200: FEAT_A: 9de5e594 FEAT_B: Dino Dive_3020 - Correlation: 1.0\n",
      "201: FEAT_A: 9de5e594 FEAT_B: Dino Dive_3120 - Correlation: 0.9995923561196808\n",
      "202: FEAT_A: 51102b85 FEAT_B: Bird Measurer (Assessment)_4030 - Correlation: 1.0\n",
      "203: FEAT_A: 756e5507 FEAT_B: Chicken Balancer (Activity)_2000 - Correlation: 1.0\n",
      "204: FEAT_A: a0faea5d FEAT_B: Bubble Bath_4070 - Correlation: 1.0\n",
      "205: FEAT_A: cb1178ad FEAT_B: Chest Sorter (Assessment)_4090 - Correlation: 1.0\n",
      "206: FEAT_A: 7cf1bc53 FEAT_B: 5154fc30 - Correlation: 0.9961960780519677\n",
      "207: FEAT_A: 7cf1bc53 FEAT_B: 3babcb9b - Correlation: 0.996113675922675\n",
      "208: FEAT_A: 7cf1bc53 FEAT_B: e720d930 - Correlation: 0.9979345979643147\n",
      "209: FEAT_A: 7cf1bc53 FEAT_B: 3323d7e9 - Correlation: 0.9987451849598936\n",
      "210: FEAT_A: 7cf1bc53 FEAT_B: 3ddc79c3 - Correlation: 0.99823323877902\n",
      "211: FEAT_A: 7cf1bc53 FEAT_B: Crystals Rule_2030 - Correlation: 0.9987451849598936\n",
      "212: FEAT_A: 7cf1bc53 FEAT_B: Crystals Rule_3110 - Correlation: 0.996113675922675\n",
      "213: FEAT_A: 7cf1bc53 FEAT_B: Crystals Rule_2020 - Correlation: 1.0\n",
      "214: FEAT_A: 7cf1bc53 FEAT_B: Crystals Rule_3010 - Correlation: 0.9961960780519677\n",
      "215: FEAT_A: 7cf1bc53 FEAT_B: Crystals Rule_3121 - Correlation: 0.9979345979643147\n",
      "216: FEAT_A: 7cf1bc53 FEAT_B: Crystals Rule_3021 - Correlation: 0.99823323877902\n",
      "217: FEAT_A: 9b23e8ee FEAT_B: 736f9581 - Correlation: 0.9999999999999999\n",
      "218: FEAT_A: 9b23e8ee FEAT_B: Egg Dropper (Activity)_2020 - Correlation: 0.9999999999999999\n",
      "219: FEAT_A: 9b23e8ee FEAT_B: Egg Dropper (Activity)_2000 - Correlation: 0.9999999999999999\n",
      "220: FEAT_A: 63f13dd7 FEAT_B: Chow Time_2020 - Correlation: 1.0\n",
      "221: FEAT_A: 7ec0c298 FEAT_B: Chow Time_3010 - Correlation: 0.9999999999999999\n",
      "222: FEAT_A: 19967db1 FEAT_B: Chow Time_4090 - Correlation: 1.0\n",
      "223: FEAT_A: 25fa8af4 FEAT_B: Mushroom Sorter (Assessment)_4100 - Correlation: 1.0\n",
      "224: FEAT_A: 1cf54632 FEAT_B: Bubble Bath_2000 - Correlation: 0.9999999999999999\n",
      "225: FEAT_A: a8a78786 FEAT_B: c7fe2a55 - Correlation: 0.9981452039350778\n",
      "226: FEAT_A: a8a78786 FEAT_B: 36fa3ebe - Correlation: 0.9979559120623818\n",
      "227: FEAT_A: a8a78786 FEAT_B: Happy Camel_2030 - Correlation: 0.9979559120623818\n",
      "228: FEAT_A: a8a78786 FEAT_B: Happy Camel_3021 - Correlation: 0.9981452039350778\n",
      "229: FEAT_A: a8a78786 FEAT_B: Happy Camel_3121 - Correlation: 0.9999999999999999\n",
      "230: FEAT_A: ca11f653 FEAT_B: daac11b0 - Correlation: 0.9995251307611354\n",
      "231: FEAT_A: ca11f653 FEAT_B: 1f19558b - Correlation: 0.998316427341082\n",
      "232: FEAT_A: ca11f653 FEAT_B: All Star Sorting_3121 - Correlation: 0.998316427341082\n",
      "233: FEAT_A: ca11f653 FEAT_B: All Star Sorting_2030 - Correlation: 1.0\n",
      "234: FEAT_A: ca11f653 FEAT_B: All Star Sorting_3021 - Correlation: 0.9995251307611354\n",
      "235: FEAT_A: 3bb91ced FEAT_B: Happy Camel_2081 - Correlation: 1.0\n",
      "236: FEAT_A: 90efca10 FEAT_B: Bottle Filler (Activity)_4020 - Correlation: 1.0\n",
      "237: FEAT_A: a8efe47b FEAT_B: Chest Sorter (Assessment)_4030 - Correlation: 1.0\n",
      "238: FEAT_A: d2659ab4 FEAT_B: Air Show_2075 - Correlation: 1.0\n",
      "239: FEAT_A: 3d0b9317 FEAT_B: Chest Sorter (Assessment)_4040 - Correlation: 1.0\n",
      "240: FEAT_A: d3640339 FEAT_B: Dino Dive_4090 - Correlation: 1.0\n",
      "241: FEAT_A: 0086365d FEAT_B: Pan Balance_4010 - Correlation: 0.9999999999999999\n",
      "242: FEAT_A: 2dcad279 FEAT_B: 923afab1 - Correlation: 0.9998567985670083\n",
      "243: FEAT_A: 2dcad279 FEAT_B: Cauldron Filler (Assessment)_3110 - Correlation: 0.9999999999999999\n",
      "244: FEAT_A: 2dcad279 FEAT_B: Cauldron Filler (Assessment)_3010 - Correlation: 0.9998567985670083\n",
      "245: FEAT_A: b012cd7f FEAT_B: 3afde5dd - Correlation: 0.9999689260314981\n",
      "246: FEAT_A: b012cd7f FEAT_B: e5c9df6f - Correlation: 0.9990885395773584\n",
      "247: FEAT_A: b012cd7f FEAT_B: Leaf Leader_3121 - Correlation: 0.9990885395773584\n",
      "248: FEAT_A: b012cd7f FEAT_B: Leaf Leader_2030 - Correlation: 1.0\n",
      "249: FEAT_A: b012cd7f FEAT_B: Leaf Leader_3021 - Correlation: 0.9999689260314981\n",
      "250: FEAT_A: 5290eab1 FEAT_B: 04df9b66 - Correlation: 0.9998190477466209\n",
      "251: FEAT_A: 5290eab1 FEAT_B: Cauldron Filler (Assessment)_3120 - Correlation: 1.0\n",
      "252: FEAT_A: 5290eab1 FEAT_B: Cauldron Filler (Assessment)_3020 - Correlation: 0.9998190477466209\n",
      "253: FEAT_A: 0d18d96c FEAT_B: Mushroom Sorter (Assessment)_4035 - Correlation: 1.0\n",
      "254: FEAT_A: 77ead60d FEAT_B: 16dffff1 - Correlation: 0.9984674845132689\n",
      "255: FEAT_A: 77ead60d FEAT_B: 4d911100 - Correlation: 0.9998475927724766\n",
      "256: FEAT_A: 77ead60d FEAT_B: Dino Drink_3021 - Correlation: 0.9999999999999999\n",
      "257: FEAT_A: 77ead60d FEAT_B: Dino Drink_2030 - Correlation: 0.9984674845132689\n",
      "258: FEAT_A: 77ead60d FEAT_B: Dino Drink_3121 - Correlation: 0.9998475927724766\n",
      "259: FEAT_A: 9d29771f FEAT_B: 28ed704e - Correlation: 0.9954918958447296\n",
      "260: FEAT_A: 9d29771f FEAT_B: 83c6c409 - Correlation: 0.9963745894369908\n",
      "261: FEAT_A: 9d29771f FEAT_B: 3dfd4aa4 - Correlation: 0.9963727918475914\n",
      "262: FEAT_A: 9d29771f FEAT_B: c74f40cd - Correlation: 0.9999553655332928\n",
      "263: FEAT_A: 9d29771f FEAT_B: Mushroom Sorter (Assessment)_2020 - Correlation: 0.9963727918475914\n",
      "264: FEAT_A: 9d29771f FEAT_B: Mushroom Sorter (Assessment)_4025 - Correlation: 0.9954918958447296\n",
      "265: FEAT_A: 9d29771f FEAT_B: Mushroom Sorter (Assessment)_3121 - Correlation: 0.9999553655332928\n",
      "266: FEAT_A: 9d29771f FEAT_B: Mushroom Sorter (Assessment)_2035 - Correlation: 0.9963745894369908\n",
      "267: FEAT_A: 9d29771f FEAT_B: Mushroom Sorter (Assessment)_3021 - Correlation: 0.9999999999999999\n",
      "268: FEAT_A: 4a4c3d21 FEAT_B: Bird Measurer (Assessment)_4025 - Correlation: 1.0\n",
      "269: FEAT_A: 89aace00 FEAT_B: e5734469 - Correlation: 0.9998406115110345\n",
      "270: FEAT_A: 89aace00 FEAT_B: Dino Drink_3120 - Correlation: 1.0\n",
      "271: FEAT_A: 89aace00 FEAT_B: Dino Drink_3020 - Correlation: 0.9998406115110345\n",
      "272: FEAT_A: 92687c59 FEAT_B: Scrub-A-Dub_4090 - Correlation: 1.0\n",
      "273: FEAT_A: 4ef8cdd3 FEAT_B: Chow Time_4020 - Correlation: 1.0\n",
      "274: FEAT_A: 022b4259 FEAT_B: Bug Measurer (Activity)_4025 - Correlation: 0.9999999999999998\n",
      "275: FEAT_A: f806dc10 FEAT_B: Dino Drink_2020 - Correlation: 1.0\n",
      "276: FEAT_A: 587b5989 FEAT_B: All Star Sorting_4070 - Correlation: 1.0\n",
      "277: FEAT_A: 6d90d394 FEAT_B: Scrub-A-Dub_2000 - Correlation: 0.9999999999999999\n",
      "278: FEAT_A: 15a43e5b FEAT_B: Bottle Filler (Activity)_4070 - Correlation: 1.0\n",
      "279: FEAT_A: 8d748b58 FEAT_B: Bug Measurer (Activity)_4090 - Correlation: 0.9999999999999998\n",
      "280: FEAT_A: f71c4741 FEAT_B: f7e47413 - Correlation: 0.9999426890770878\n",
      "281: FEAT_A: f71c4741 FEAT_B: Scrub-A-Dub_3010 - Correlation: 1.0\n",
      "282: FEAT_A: f71c4741 FEAT_B: Scrub-A-Dub_3110 - Correlation: 0.9999426890770878\n",
      "283: FEAT_A: 30614231 FEAT_B: 37ee8496 - Correlation: 0.9967763987631819\n",
      "284: FEAT_A: 30614231 FEAT_B: Cauldron Filler (Assessment)_4030 - Correlation: 0.9967763987631819\n",
      "285: FEAT_A: 30614231 FEAT_B: Cauldron Filler (Assessment)_4020 - Correlation: 1.0\n",
      "286: FEAT_A: 2ec694de FEAT_B: Bug Measurer (Activity)_4080 - Correlation: 0.9999999999999998\n",
      "287: FEAT_A: f32856e4 FEAT_B: Leaf Leader_2020 - Correlation: 1.0\n",
      "288: FEAT_A: 6f4bd64e FEAT_B: Air Show_4090 - Correlation: 0.9999999999999998\n",
      "289: FEAT_A: 828e68f9 FEAT_B: Cart Balancer (Assessment)_3110 - Correlation: 1.0\n",
      "290: FEAT_A: 65abac75 FEAT_B: Air Show_4010 - Correlation: 1.0\n",
      "291: FEAT_A: 1375ccb7 FEAT_B: bdf49a58 - Correlation: 0.9993801763820348\n",
      "292: FEAT_A: 1375ccb7 FEAT_B: Bird Measurer (Assessment)_3010 - Correlation: 1.0\n",
      "293: FEAT_A: 1375ccb7 FEAT_B: Bird Measurer (Assessment)_3110 - Correlation: 0.9993801763820348\n",
      "294: FEAT_A: 26a5a3dd FEAT_B: All Star Sorting_4080 - Correlation: 1.0\n",
      "295: FEAT_A: db02c830 FEAT_B: 3bfd1a65 - Correlation: 0.9999982205265873\n",
      "296: FEAT_A: db02c830 FEAT_B: Mushroom Sorter (Assessment)_2000 - Correlation: 0.9999982205265873\n",
      "297: FEAT_A: db02c830 FEAT_B: Mushroom Sorter (Assessment)_2025 - Correlation: 0.9999999999999999\n",
      "298: FEAT_A: 2b058fe3 FEAT_B: Cauldron Filler (Assessment)_2010 - Correlation: 1.0\n",
      "299: FEAT_A: b80e5e84 FEAT_B: 7ab78247 - Correlation: 0.9998336590281085\n",
      "300: FEAT_A: b80e5e84 FEAT_B: Egg Dropper (Activity)_3010 - Correlation: 0.9998336590281085\n",
      "301: FEAT_A: b80e5e84 FEAT_B: Egg Dropper (Activity)_3110 - Correlation: 1.0\n",
      "302: FEAT_A: cc5087a3 FEAT_B: Crystals Rule_4010 - Correlation: 1.0\n",
      "303: FEAT_A: e7561dd2 FEAT_B: Pan Balance_4025 - Correlation: 0.9999999999999998\n",
      "304: FEAT_A: 28520915 FEAT_B: b5053438 - Correlation: 0.999090516610188\n",
      "305: FEAT_A: 28520915 FEAT_B: d3268efa - Correlation: 0.9989055023166136\n",
      "306: FEAT_A: 28520915 FEAT_B: Cauldron Filler (Assessment)_3121 - Correlation: 0.999090516610188\n",
      "307: FEAT_A: 28520915 FEAT_B: Cauldron Filler (Assessment)_3021 - Correlation: 0.9989055023166136\n",
      "308: FEAT_A: 28520915 FEAT_B: Cauldron Filler (Assessment)_2030 - Correlation: 1.0\n",
      "309: FEAT_A: 77261ab5 FEAT_B: Sandcastle Builder (Activity)_2000 - Correlation: 0.9999999999999999\n",
      "310: FEAT_A: 532a2afb FEAT_B: Cauldron Filler (Assessment)_2020 - Correlation: 1.0\n",
      "311: FEAT_A: e7e44842 FEAT_B: Watering Hole (Activity)_4090 - Correlation: 0.9999999999999999\n",
      "312: FEAT_A: 3d8c61b0 FEAT_B: Happy Camel_4030 - Correlation: 0.9999999999999999\n",
      "313: FEAT_A: 02a42007 FEAT_B: Fireworks (Activity)_4030 - Correlation: 1.0\n",
      "314: FEAT_A: 9b4001e4 FEAT_B: f5b8c21a - Correlation: 0.9976837802056778\n",
      "315: FEAT_A: 9b4001e4 FEAT_B: 58a0de5c - Correlation: 0.999915936221619\n",
      "316: FEAT_A: 9b4001e4 FEAT_B: Air Show_3121 - Correlation: 0.999915936221619\n",
      "317: FEAT_A: 9b4001e4 FEAT_B: Air Show_3021 - Correlation: 1.0\n",
      "318: FEAT_A: 9b4001e4 FEAT_B: Air Show_2030 - Correlation: 0.9976837802056778\n",
      "319: FEAT_A: 3edf6747 FEAT_B: Cauldron Filler (Assessment)_4035 - Correlation: 0.9999999999999999\n",
      "320: FEAT_A: 3b2048ee FEAT_B: Leaf Leader_4095 - Correlation: 1.0\n",
      "321: FEAT_A: dcb55a27 FEAT_B: Air Show_4110 - Correlation: 1.0\n",
      "322: FEAT_A: 565a3990 FEAT_B: Bug Measurer (Activity)_4070 - Correlation: 1.0\n",
      "323: FEAT_A: cfbd47c8 FEAT_B: Chow Time_4030 - Correlation: 1.0\n",
      "324: FEAT_A: 56bcd38d FEAT_B: Chicken Balancer (Activity)_4030 - Correlation: 0.9999999999999998\n",
      "325: FEAT_A: 070a5291 FEAT_B: Bird Measurer (Assessment)_4100 - Correlation: 1.0\n",
      "326: FEAT_A: 99abe2bb FEAT_B: Bubble Bath_2080 - Correlation: 1.0\n",
      "327: FEAT_A: 4c2ec19f FEAT_B: Egg Dropper (Activity)_4025 - Correlation: 1.0\n",
      "328: FEAT_A: 155f62a4 FEAT_B: 5b49460a - Correlation: 0.9999999999999999\n",
      "329: FEAT_A: 155f62a4 FEAT_B: Chest Sorter (Assessment)_2020 - Correlation: 0.9999999999999999\n",
      "330: FEAT_A: 155f62a4 FEAT_B: Chest Sorter (Assessment)_2000 - Correlation: 0.9999999999999999\n",
      "331: FEAT_A: 0db6d71d FEAT_B: Chest Sorter (Assessment)_4020 - Correlation: 1.0\n",
      "332: FEAT_A: 3dcdda7f FEAT_B: 3ccd3f02 - Correlation: 0.9977337946782758\n",
      "333: FEAT_A: 3dcdda7f FEAT_B: Chest Sorter (Assessment)_3110 - Correlation: 0.9977337946782758\n",
      "334: FEAT_A: 3dcdda7f FEAT_B: Chest Sorter (Assessment)_3010 - Correlation: 1.0\n",
      "335: FEAT_A: 262136f4 FEAT_B: Leaf Leader_4020 - Correlation: 1.0\n",
      "336: FEAT_A: 7372e1a5 FEAT_B: Chow Time_4070 - Correlation: 1.0\n",
      "337: FEAT_A: 67439901 FEAT_B: df4940d3 - Correlation: 0.999935162643595\n",
      "338: FEAT_A: 67439901 FEAT_B: Bottle Filler (Activity)_3110 - Correlation: 0.999935162643595\n",
      "339: FEAT_A: 67439901 FEAT_B: Bottle Filler (Activity)_3010 - Correlation: 1.0\n",
      "340: FEAT_A: f6947f54 FEAT_B: Bird Measurer (Assessment)_2030 - Correlation: 1.0\n",
      "341: FEAT_A: 7040c096 FEAT_B: Scrub-A-Dub_4010 - Correlation: 1.0\n",
      "342: FEAT_A: 8ac7cce4 FEAT_B: Leaf Leader_2000 - Correlation: 1.0\n",
      "343: FEAT_A: b7dc8128 FEAT_B: 4b5efe37 - Correlation: 0.9980151285383981\n",
      "344: FEAT_A: b7dc8128 FEAT_B: All Star Sorting_4010 - Correlation: 0.9980151285383981\n",
      "345: FEAT_A: b7dc8128 FEAT_B: All Star Sorting_2000 - Correlation: 1.0\n",
      "346: FEAT_A: e694a35b FEAT_B: Fireworks (Activity)_4020 - Correlation: 1.0\n",
      "347: FEAT_A: ab4ec3a4 FEAT_B: Dino Drink_4080 - Correlation: 1.0\n",
      "348: FEAT_A: 77c76bc5 FEAT_B: Cauldron Filler (Assessment)_4090 - Correlation: 1.0\n",
      "349: FEAT_A: 795e4a37 FEAT_B: Cart Balancer (Assessment)_3010 - Correlation: 1.0\n",
      "350: FEAT_A: 29a42aea FEAT_B: Bubble Bath_4080 - Correlation: 1.0\n",
      "351: FEAT_A: e64e2cfd FEAT_B: Watering Hole (Activity)_2000 - Correlation: 0.9999999999999998\n",
      "352: FEAT_A: 65a38bf7 FEAT_B: 7ad3efc6 - Correlation: 0.9999786265690429\n",
      "353: FEAT_A: 65a38bf7 FEAT_B: Cart Balancer (Assessment)_2020 - Correlation: 1.0\n",
      "354: FEAT_A: 65a38bf7 FEAT_B: Cart Balancer (Assessment)_2000 - Correlation: 0.9999786265690429\n",
      "355: FEAT_A: c1cac9a2 FEAT_B: Scrub-A-Dub_2081 - Correlation: 0.9999999999999998\n",
      "356: FEAT_A: 15f99afc FEAT_B: 6cf7d25c - Correlation: 0.9994848234397947\n",
      "357: FEAT_A: 15f99afc FEAT_B: Pan Balance_3010 - Correlation: 0.9994848234397947\n",
      "358: FEAT_A: 15f99afc FEAT_B: Pan Balance_3110 - Correlation: 1.0\n",
      "359: FEAT_A: 1bb5fbdb FEAT_B: b2dba42b - Correlation: 0.9999521729413294\n",
      "360: FEAT_A: 1bb5fbdb FEAT_B: Sandcastle Builder (Activity)_3110 - Correlation: 1.0\n",
      "361: FEAT_A: 1bb5fbdb FEAT_B: Sandcastle Builder (Activity)_3010 - Correlation: 0.9999521729413294\n",
      "362: FEAT_A: 611485c5 FEAT_B: Fireworks (Activity)_4080 - Correlation: 1.0\n",
      "363: FEAT_A: 5a848010 FEAT_B: Scrub-A-Dub_2080 - Correlation: 1.0\n",
      "364: FEAT_A: c0415e5c FEAT_B: Dino Dive_4020 - Correlation: 1.0\n",
      "365: FEAT_A: fd20ea40 FEAT_B: Leaf Leader_4010 - Correlation: 0.9999999999999998\n",
      "366: FEAT_A: e04fb33d FEAT_B: 7423acbc - Correlation: 0.999628997408126\n",
      "367: FEAT_A: e04fb33d FEAT_B: Air Show_3020 - Correlation: 0.999628997408126\n",
      "368: FEAT_A: e04fb33d FEAT_B: Air Show_3120 - Correlation: 1.0\n",
      "369: FEAT_A: 0ce40006 FEAT_B: Happy Camel_4080 - Correlation: 1.0\n",
      "370: FEAT_A: 7fd1ac25 FEAT_B: Egg Dropper (Activity)_4080 - Correlation: 1.0\n",
      "371: FEAT_A: d2e9262e FEAT_B: 2fb91ec1 - Correlation: 0.9991434495208743\n",
      "372: FEAT_A: d2e9262e FEAT_B: Watering Hole (Activity)_4025 - Correlation: 0.9991434495208743\n",
      "373: FEAT_A: d2e9262e FEAT_B: Watering Hole (Activity)_4020 - Correlation: 0.9999999999999998\n",
      "374: FEAT_A: 5c2f29ca FEAT_B: Cart Balancer (Assessment)_4020 - Correlation: 1.0\n",
      "375: FEAT_A: 6043a2b4 FEAT_B: All Star Sorting_4090 - Correlation: 0.9999999999999999\n",
      "376: FEAT_A: c2baf0bd FEAT_B: Happy Camel_2020 - Correlation: 1.0\n",
      "377: FEAT_A: 2a444e03 FEAT_B: Pan Balance_4030 - Correlation: 1.0\n",
      "378: FEAT_A: 9ed8f6da FEAT_B: Dino Drink_2075 - Correlation: 0.9999999999999999\n",
      "379: FEAT_A: 90d848e0 FEAT_B: Cauldron Filler (Assessment)_2000 - Correlation: 1.0\n",
      "380: FEAT_A: f56e0afc FEAT_B: Bird Measurer (Assessment)_2000 - Correlation: 1.0\n",
      "381: FEAT_A: 4a09ace1 FEAT_B: Scrub-A-Dub_2083 - Correlation: 1.0\n",
      "382: FEAT_A: 85d1b0de FEAT_B: Chicken Balancer (Activity)_4090 - Correlation: 1.0\n",
      "383: FEAT_A: d2278a3b FEAT_B: Bottle Filler (Activity)_2000 - Correlation: 1.0\n",
      "384: FEAT_A: 119b5b02 FEAT_B: Dino Dive_4080 - Correlation: 1.0\n",
      "385: FEAT_A: 6088b756 FEAT_B: Dino Dive_2070 - Correlation: 1.0\n",
      "386: FEAT_A: 461eace6 FEAT_B: Egg Dropper (Activity)_4020 - Correlation: 1.0\n",
      "387: FEAT_A: f50fc6c1 FEAT_B: Watering Hole (Activity)_4021 - Correlation: 1.0\n",
      "388: FEAT_A: f93fc684 FEAT_B: Chow Time_4010 - Correlation: 0.9999999999999999\n",
      "389: FEAT_A: c6971acf FEAT_B: Dino Drink_2060 - Correlation: 0.9999999999999999\n",
      "390: FEAT_A: cf82af56 FEAT_B: Scrub-A-Dub_4070 - Correlation: 1.0\n",
      "391: FEAT_A: d88e8f25 FEAT_B: ac92046e - Correlation: 0.9999763070332106\n",
      "392: FEAT_A: d88e8f25 FEAT_B: Scrub-A-Dub_3120 - Correlation: 0.9999763070332106\n",
      "393: FEAT_A: d88e8f25 FEAT_B: Scrub-A-Dub_3020 - Correlation: 1.0\n",
      "394: FEAT_A: 37db1c2f FEAT_B: Happy Camel_4045 - Correlation: 1.0\n",
      "395: FEAT_A: 5d042115 FEAT_B: Flower Waterer (Activity)_4030 - Correlation: 1.0\n",
      "396: FEAT_A: 84538528 FEAT_B: Sandcastle Builder (Activity)_4020 - Correlation: 1.0\n",
      "397: FEAT_A: 9e6b7fb5 FEAT_B: Chow Time_4095 - Correlation: 0.9999999999999998\n",
      "398: FEAT_A: 5c3d2b2f FEAT_B: Scrub-A-Dub_4020 - Correlation: 1.0\n",
      "399: FEAT_A: ecc6157f FEAT_B: Cart Balancer (Assessment)_4080 - Correlation: 0.9999999999999999\n",
      "400: FEAT_A: 3ee399c3 FEAT_B: Cauldron Filler (Assessment)_4070 - Correlation: 1.0\n",
      "401: FEAT_A: c189aaf2 FEAT_B: Happy Camel_2083 - Correlation: 0.9999999999999999\n",
      "402: FEAT_A: 71fe8f75 FEAT_B: 0a08139c - Correlation: 0.9999850342981554\n",
      "403: FEAT_A: 71fe8f75 FEAT_B: Bug Measurer (Activity)_3110 - Correlation: 1.0\n",
      "404: FEAT_A: 71fe8f75 FEAT_B: Bug Measurer (Activity)_3010 - Correlation: 0.9999850342981554\n",
      "405: FEAT_A: 9e4c8c7b FEAT_B: 363d3849 - Correlation: 0.9992130941883633\n",
      "406: FEAT_A: 9e4c8c7b FEAT_B: All Star Sorting_3010 - Correlation: 0.9992130941883633\n",
      "407: FEAT_A: 9e4c8c7b FEAT_B: All Star Sorting_3110 - Correlation: 0.9999999999999998\n",
      "408: FEAT_A: bc8f2793 FEAT_B: Pan Balance_4035 - Correlation: 0.9999999999999999\n",
      "409: FEAT_A: 2dc29e21 FEAT_B: All Star Sorting_4020 - Correlation: 1.0\n",
      "410: FEAT_A: 5de79a6a FEAT_B: 31973d56 - Correlation: 0.9973945339840646\n",
      "411: FEAT_A: 5de79a6a FEAT_B: Cart Balancer (Assessment)_3020 - Correlation: 1.0\n",
      "412: FEAT_A: 5de79a6a FEAT_B: Cart Balancer (Assessment)_3120 - Correlation: 0.9973945339840646\n",
      "413: FEAT_A: a1e4395d FEAT_B: a52b92d5 - Correlation: 0.9991003891313368\n",
      "414: FEAT_A: a1e4395d FEAT_B: Mushroom Sorter (Assessment)_3110 - Correlation: 0.9991003891313368\n",
      "415: FEAT_A: a1e4395d FEAT_B: Mushroom Sorter (Assessment)_3010 - Correlation: 1.0\n",
      "416: FEAT_A: acf5c23f FEAT_B: Cart Balancer (Assessment)_4070 - Correlation: 1.0\n",
      "417: FEAT_A: 46b50ba8 FEAT_B: Happy Camel_4095 - Correlation: 0.9999999999999998\n",
      "418: FEAT_A: 8d84fa81 FEAT_B: Bubble Bath_4010 - Correlation: 1.0\n",
      "419: FEAT_A: 1b54d27f FEAT_B: Watering Hole (Activity)_2010 - Correlation: 1.0\n",
      "420: FEAT_A: 91561152 FEAT_B: Cauldron Filler (Assessment)_4025 - Correlation: 1.0\n",
      "421: FEAT_A: d38c2fd7 FEAT_B: Bird Measurer (Assessment)_4035 - Correlation: 0.9999999999999999\n",
      "422: FEAT_A: a29c5338 FEAT_B: 7f0836bf - Correlation: 0.9986531654717626\n",
      "423: FEAT_A: a29c5338 FEAT_B: Dino Drink_3010 - Correlation: 0.9999999999999999\n",
      "424: FEAT_A: a29c5338 FEAT_B: Dino Drink_3110 - Correlation: 0.9986531654717626\n",
      "425: FEAT_A: f54238ee FEAT_B: Fireworks (Activity)_4090 - Correlation: 1.0\n",
      "426: FEAT_A: 47efca07 FEAT_B: Bottle Filler (Activity)_4090 - Correlation: 1.0\n",
      "427: FEAT_A: 15ba1109 FEAT_B: Air Show_2000 - Correlation: 1.0\n",
      "428: FEAT_A: 907a054b FEAT_B: c51d8688 - Correlation: 0.9999667370361688\n",
      "429: FEAT_A: 907a054b FEAT_B: Pan Balance_3120 - Correlation: 0.9999667370361688\n",
      "430: FEAT_A: 907a054b FEAT_B: Pan Balance_3020 - Correlation: 1.0\n",
      "431: FEAT_A: a7640a16 FEAT_B: Happy Camel_4070 - Correlation: 1.0\n",
      "432: FEAT_A: 38074c54 FEAT_B: 222660ff - Correlation: 0.9999999999999998\n",
      "433: FEAT_A: 38074c54 FEAT_B: Chest Sorter (Assessment)_2030 - Correlation: 0.9999999999999998\n",
      "434: FEAT_A: 38074c54 FEAT_B: Chest Sorter (Assessment)_2010 - Correlation: 0.9999999999999998\n",
      "435: FEAT_A: 2c4e6db0 FEAT_B: All Star Sorting_2020 - Correlation: 1.0\n",
      "436: FEAT_A: 1325467d FEAT_B: Sandcastle Builder (Activity)_4070 - Correlation: 1.0\n",
      "437: FEAT_A: 90ea0bac FEAT_B: 5859dfb6 - Correlation: 0.9981052472610569\n",
      "438: FEAT_A: 90ea0bac FEAT_B: Bubble Bath_3120 - Correlation: 0.9981052472610569\n",
      "439: FEAT_A: 90ea0bac FEAT_B: Bubble Bath_3020 - Correlation: 1.0\n",
      "440: FEAT_A: 1af8be29 FEAT_B: 3bf1cf26 - Correlation: 0.9998900847287077\n",
      "441: FEAT_A: 1af8be29 FEAT_B: Happy Camel_3120 - Correlation: 0.9998900847287077\n",
      "442: FEAT_A: 1af8be29 FEAT_B: Happy Camel_3020 - Correlation: 0.9999999999999999\n",
      "443: FEAT_A: 6aeafed4 FEAT_B: Bubble Bath_4090 - Correlation: 1.0\n",
      "444: FEAT_A: 9ee1c98c FEAT_B: Sandcastle Builder (Activity)_4021 - Correlation: 0.9999999999999999\n",
      "445: FEAT_A: 06372577 FEAT_B: Air Show_2060 - Correlation: 1.0\n",
      "446: FEAT_A: 7da34a02 FEAT_B: Mushroom Sorter (Assessment)_4070 - Correlation: 0.9999999999999998\n",
      "447: FEAT_A: bd612267 FEAT_B: Chest Sorter (Assessment)_4070 - Correlation: 0.9999999999999999\n",
      "448: FEAT_A: c58186bf FEAT_B: Sandcastle Builder (Activity)_4035 - Correlation: 1.0\n",
      "449: FEAT_A: 13f56524 FEAT_B: Mushroom Sorter (Assessment)_4080 - Correlation: 1.0\n",
      "450: FEAT_A: a5be6304 FEAT_B: Mushroom Sorter (Assessment)_2010 - Correlation: 1.0\n",
      "451: FEAT_A: 731c0cbe FEAT_B: Bird Measurer (Assessment)_4090 - Correlation: 1.0\n",
      "452: FEAT_A: b88f38da FEAT_B: beb0a7b9 - Correlation: 0.9999125179829755\n",
      "453: FEAT_A: b88f38da FEAT_B: Fireworks (Activity)_3110 - Correlation: 1.0\n",
      "454: FEAT_A: b88f38da FEAT_B: Fireworks (Activity)_3010 - Correlation: 0.9999125179829755\n",
      "455: FEAT_A: e4d32835 FEAT_B: Pan Balance_4080 - Correlation: 0.9999999999999998\n",
      "456: FEAT_A: 1575e76c FEAT_B: Air Show_2020 - Correlation: 1.0\n",
      "457: FEAT_A: 53c6e11a FEAT_B: Leaf Leader_2075 - Correlation: 0.9999999999999999\n",
      "458: FEAT_A: d185d3ea FEAT_B: Chow Time_4035 - Correlation: 1.0\n",
      "459: FEAT_A: 6f445b57 FEAT_B: Chow Time_4080 - Correlation: 1.0\n",
      "460: FEAT_A: 392e14df FEAT_B: Cauldron Filler (Assessment)_4100 - Correlation: 1.0\n",
      "461: FEAT_A: 74e5f8a7 FEAT_B: Dino Drink_4020 - Correlation: 1.0\n",
      "462: FEAT_A: 363c86c9 FEAT_B: Bug Measurer (Activity)_4035 - Correlation: 1.0\n",
      "463: FEAT_A: a2df0760 FEAT_B: Happy Camel_4035 - Correlation: 0.9999999999999999\n",
      "464: FEAT_A: 86c924c4 FEAT_B: Crystals Rule_4020 - Correlation: 1.0\n",
      "465: FEAT_A: c7128948 FEAT_B: Mushroom Sorter (Assessment)_4040 - Correlation: 1.0\n",
      "466: FEAT_A: c952eb01 FEAT_B: Watering Hole (Activity)_4070 - Correlation: 1.0\n",
      "467: FEAT_A: 7d5c30a2 FEAT_B: Dino Dive_2060 - Correlation: 1.0\n",
      "468: FEAT_A: 6f8106d9 FEAT_B: Dino Drink_4090 - Correlation: 1.0\n",
      "469: FEAT_A: 51311d7a FEAT_B: Dino Drink_2000 - Correlation: 1.0\n",
      "470: FEAT_A: 93edfe2e FEAT_B: Crystals Rule_4090 - Correlation: 1.0\n",
      "471: FEAT_A: 499edb7c FEAT_B: Chicken Balancer (Activity)_4020 - Correlation: 1.0\n",
      "472: FEAT_A: 76babcde FEAT_B: Dino Dive_4070 - Correlation: 1.0\n",
      "473: FEAT_A: 93b353f2 FEAT_B: Chest Sorter (Assessment)_4100 - Correlation: 0.9999999999999998\n",
      "474: FEAT_A: 5348fd84 FEAT_B: Cauldron Filler (Assessment)_4040 - Correlation: 1.0\n",
      "475: FEAT_A: 792530f8 FEAT_B: Dino Drink_4030 - Correlation: 1.0\n",
      "476: FEAT_A: 8fee50e2 FEAT_B: Bird Measurer (Assessment)_4020 - Correlation: 1.0\n",
      "477: FEAT_A: ea321fb1 FEAT_B: 84b0e0c8 - Correlation: 0.9993007600205108\n",
      "478: FEAT_A: ea321fb1 FEAT_B: Chicken Balancer (Activity)_3110 - Correlation: 0.9993007600205108\n",
      "479: FEAT_A: ea321fb1 FEAT_B: Chicken Balancer (Activity)_3010 - Correlation: 1.0\n",
      "480: FEAT_A: ec138c1c FEAT_B: Bird Measurer (Assessment)_2020 - Correlation: 0.9999999999999999\n",
      "481: FEAT_A: 1beb320a FEAT_B: Bubble Bath_2020 - Correlation: 1.0\n",
      "482: FEAT_A: bd701df8 FEAT_B: 49ed92e9 - Correlation: 0.9993109138888533\n",
      "483: FEAT_A: bd701df8 FEAT_B: Watering Hole (Activity)_3110 - Correlation: 1.0\n",
      "484: FEAT_A: bd701df8 FEAT_B: Watering Hole (Activity)_3010 - Correlation: 0.9993109138888533\n",
      "485: FEAT_A: 9ce586dd FEAT_B: Chest Sorter (Assessment)_4035 - Correlation: 1.0\n",
      "486: FEAT_A: d02b7a8e FEAT_B: All Star Sorting_4035 - Correlation: 1.0\n",
      "487: FEAT_A: 14de4c5d FEAT_B: Air Show_4100 - Correlation: 1.0\n",
      "488: FEAT_A: 05ad839b FEAT_B: Happy Camel_4090 - Correlation: 1.0\n",
      "489: FEAT_A: 6c517a88 FEAT_B: Dino Drink_4070 - Correlation: 1.0\n",
      "490: FEAT_A: 29bdd9ba FEAT_B: Dino Dive_2000 - Correlation: 1.0\n",
      "491: FEAT_A: 46cd75b4 FEAT_B: Chicken Balancer (Activity)_4022 - Correlation: 0.9999999999999998\n",
      "492: FEAT_A: 86ba578b FEAT_B: Leaf Leader_2070 - Correlation: 1.0\n",
      "493: FEAT_A: 29f54413 FEAT_B: Leaf Leader_2060 - Correlation: 0.9999999999999998\n",
      "494: FEAT_A: fcfdffb6 FEAT_B: Flower Waterer (Activity)_4022 - Correlation: 1.0\n",
      "495: FEAT_A: ab3136ba FEAT_B: 832735e1 - Correlation: 0.9998637945770242\n",
      "496: FEAT_A: ab3136ba FEAT_B: Dino Dive_3010 - Correlation: 0.9998637945770242\n",
      "497: FEAT_A: ab3136ba FEAT_B: Dino Dive_3110 - Correlation: 1.0\n",
      "498: FEAT_A: d9c005dd FEAT_B: Happy Camel_2000 - Correlation: 1.0\n",
      "499: FEAT_A: 8af75982 FEAT_B: Happy Camel_4020 - Correlation: 1.0\n",
      "500: FEAT_A: 562cec5f FEAT_B: Chest Sorter (Assessment)_4025 - Correlation: 1.0\n",
      "501: FEAT_A: ecc36b7f FEAT_B: Bubble Bath_4095 - Correlation: 1.0\n",
      "502: FEAT_A: 30df3273 FEAT_B: Sandcastle Builder (Activity)_4080 - Correlation: 1.0\n",
      "503: FEAT_A: 5f0eb72c FEAT_B: Mushroom Sorter (Assessment)_4020 - Correlation: 0.9999999999999998\n",
      "504: FEAT_A: f3cd5473 FEAT_B: Pan Balance_4070 - Correlation: 1.0\n",
      "505: FEAT_A: cdd22e43 FEAT_B: Chicken Balancer (Activity)_4035 - Correlation: 1.0\n",
      "506: FEAT_A: 28f975ea FEAT_B: Air Show_4020 - Correlation: 1.0\n",
      "507: FEAT_A: 884228c8 FEAT_B: Fireworks (Activity)_4070 - Correlation: 1.0\n",
      "508: FEAT_A: d51b1749 FEAT_B: Happy Camel_2080 - Correlation: 1.0\n",
      "509: FEAT_A: 3a4be871 FEAT_B: Flower Waterer (Activity)_4080 - Correlation: 1.0\n",
      "510: FEAT_A: e79f3763 FEAT_B: Bug Measurer (Activity)_4030 - Correlation: 1.0\n",
      "511: FEAT_A: a44b10dc FEAT_B: Flower Waterer (Activity)_4070 - Correlation: 1.0\n",
      "512: FEAT_A: 598f4598 FEAT_B: Flower Waterer (Activity)_4025 - Correlation: 0.9999999999999998\n",
      "513: FEAT_A: 9d4e7b25 FEAT_B: Cart Balancer (Assessment)_4040 - Correlation: 1.0\n",
      "514: FEAT_A: b1d5101d FEAT_B: All Star Sorting_4095 - Correlation: 1.0\n"
     ]
    }
   ],
   "source": [
    "to_remove = remove_correlated_features(reduce_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 426 features in LGBM\n",
      "Training with 425 features in LR\n",
      "Training with 425 features in NN\n"
     ]
    }
   ],
   "source": [
    "features = [col for col in features if col not in to_remove]\n",
    "features = [col for col in features if col not in ['Heavy, Heavier, Heaviest_2000', 'Heavy, Heavier, Heaviest']]\n",
    "features.append('installation_id')\n",
    "print('Training with {} features in LGBM'.format(len(features)))\n",
    "\n",
    "lr_features = [col for col in lr_features if col not in to_remove]\n",
    "lr_features = [col for col in lr_features if col not in ['Heavy, Heavier, Heaviest_2000', 'Heavy, Heavier, Heaviest', \"session_title\"]]\n",
    "nn_features = [col for col in nn_features if col not in to_remove]\n",
    "nn_features = [col for col in nn_features if col not in ['Heavy, Heavier, Heaviest_2000', 'Heavy, Heavier, Heaviest', \"session_title\"]]\n",
    "lr_features.append('installation_id')\n",
    "nn_features.append('installation_id')\n",
    "print('Training with {} features in LR'.format(len(lr_features)))\n",
    "print('Training with {} features in NN'.format(len(nn_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_Cart Balancer (Assessment)\n",
      "misses\n",
      "01ca3a3c\n",
      "eb2c19cd\n",
      "bfc77bd6\n",
      "2ec694de\n",
      "5dc079d8\n",
      "ab4ec3a4\n",
      "003cd2ee\n",
      "29a42aea\n",
      "611485c5\n",
      "0ce40006\n",
      "7fd1ac25\n",
      "119b5b02\n",
      "17ca3959\n",
      "ecc6157f\n",
      "1b54d27f\n",
      "6aeafed4\n",
      "a8cc6fec\n",
      "13f56524\n",
      "e4d32835\n",
      "dcb1663e\n",
      "4074bac2\n",
      "Scrub-A-Dub_4080\n",
      "Air Show_4080\n",
      "Sandcastle Builder (Activity)_2010\n",
      "Pan Balance_2010\n",
      "Crystals Rule_2010\n",
      "Bottle Filler (Activity)_2010\n"
     ]
    }
   ],
   "source": [
    "to_exclude, ajusted_test = exclude(reduce_train, reduce_test, features)\n",
    "features = [col for col in features if col not in to_exclude]\n",
    "nn_features = [col for col in nn_features if col not in to_exclude]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modelling and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "used validation data:  721\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.869328\tvalid_1's rmse: 1.06761\n",
      "[200]\ttraining's rmse: 0.790692\tvalid_1's rmse: 1.06911\n",
      "Early stopping, best iteration is:\n",
      "[109]\ttraining's rmse: 0.860926\tvalid_1's rmse: 1.0653\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870642\tvalid_1's rmse: 1.00835\n",
      "[200]\ttraining's rmse: 0.792988\tvalid_1's rmse: 1.0075\n",
      "Early stopping, best iteration is:\n",
      "[150]\ttraining's rmse: 0.828418\tvalid_1's rmse: 1.00555\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.87078\tvalid_1's rmse: 0.991765\n",
      "[200]\ttraining's rmse: 0.792649\tvalid_1's rmse: 0.998321\n",
      "Early stopping, best iteration is:\n",
      "[109]\ttraining's rmse: 0.862737\tvalid_1's rmse: 0.990664\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.864995\tvalid_1's rmse: 1.0521\n",
      "[200]\ttraining's rmse: 0.786264\tvalid_1's rmse: 1.05048\n",
      "[300]\ttraining's rmse: 0.725495\tvalid_1's rmse: 1.05366\n",
      "Early stopping, best iteration is:\n",
      "[201]\ttraining's rmse: 0.785592\tvalid_1's rmse: 1.05018\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.86024\tvalid_1's rmse: 1.07766\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttraining's rmse: 0.891447\tvalid_1's rmse: 1.0742\n",
      "Our oof rmse score is: 1.0377147247774114\n",
      "Our oof cohen kappa score is: 0.5349913164213201\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.869328\tvalid_1's rmse: 1.06088\n",
      "[200]\ttraining's rmse: 0.790692\tvalid_1's rmse: 1.05779\n",
      "[300]\ttraining's rmse: 0.730451\tvalid_1's rmse: 1.05844\n",
      "Early stopping, best iteration is:\n",
      "[226]\ttraining's rmse: 0.773452\tvalid_1's rmse: 1.05592\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870642\tvalid_1's rmse: 1.00426\n",
      "[200]\ttraining's rmse: 0.792988\tvalid_1's rmse: 1.00332\n",
      "Early stopping, best iteration is:\n",
      "[128]\ttraining's rmse: 0.845881\tvalid_1's rmse: 1.00136\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.87078\tvalid_1's rmse: 0.99868\n",
      "[200]\ttraining's rmse: 0.792649\tvalid_1's rmse: 1.00485\n",
      "Early stopping, best iteration is:\n",
      "[109]\ttraining's rmse: 0.862737\tvalid_1's rmse: 0.997543\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.864995\tvalid_1's rmse: 1.09101\n",
      "[200]\ttraining's rmse: 0.786264\tvalid_1's rmse: 1.08632\n",
      "[300]\ttraining's rmse: 0.725495\tvalid_1's rmse: 1.08585\n",
      "Early stopping, best iteration is:\n",
      "[247]\ttraining's rmse: 0.756227\tvalid_1's rmse: 1.08445\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.86024\tvalid_1's rmse: 1.03193\n",
      "[200]\ttraining's rmse: 0.781234\tvalid_1's rmse: 1.03102\n",
      "Early stopping, best iteration is:\n",
      "[188]\ttraining's rmse: 0.789574\tvalid_1's rmse: 1.02982\n",
      "Our oof rmse score is: 1.0343557955270308\n",
      "Our oof cohen kappa score is: 0.5561744209422714\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.869328\tvalid_1's rmse: 1.09117\n",
      "[200]\ttraining's rmse: 0.790692\tvalid_1's rmse: 1.08974\n",
      "Early stopping, best iteration is:\n",
      "[185]\ttraining's rmse: 0.800855\tvalid_1's rmse: 1.08801\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870642\tvalid_1's rmse: 1.01669\n",
      "[200]\ttraining's rmse: 0.792988\tvalid_1's rmse: 1.0166\n",
      "Early stopping, best iteration is:\n",
      "[107]\ttraining's rmse: 0.864254\tvalid_1's rmse: 1.01581\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.87078\tvalid_1's rmse: 1.00202\n",
      "[200]\ttraining's rmse: 0.792649\tvalid_1's rmse: 1.00617\n",
      "Early stopping, best iteration is:\n",
      "[109]\ttraining's rmse: 0.862737\tvalid_1's rmse: 1.00031\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.864995\tvalid_1's rmse: 1.05758\n",
      "[200]\ttraining's rmse: 0.786264\tvalid_1's rmse: 1.05411\n",
      "[300]\ttraining's rmse: 0.725495\tvalid_1's rmse: 1.05062\n",
      "[400]\ttraining's rmse: 0.674934\tvalid_1's rmse: 1.05168\n",
      "Early stopping, best iteration is:\n",
      "[312]\ttraining's rmse: 0.719222\tvalid_1's rmse: 1.05003\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.86024\tvalid_1's rmse: 1.07062\n",
      "[200]\ttraining's rmse: 0.781234\tvalid_1's rmse: 1.07167\n",
      "Early stopping, best iteration is:\n",
      "[132]\ttraining's rmse: 0.831831\tvalid_1's rmse: 1.06971\n",
      "Our oof rmse score is: 1.0452773379401237\n",
      "Our oof cohen kappa score is: 0.5288745093594474\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.869328\tvalid_1's rmse: 1.04908\n",
      "Early stopping, best iteration is:\n",
      "[75]\ttraining's rmse: 0.894756\tvalid_1's rmse: 1.04761\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870642\tvalid_1's rmse: 1.01583\n",
      "[200]\ttraining's rmse: 0.792988\tvalid_1's rmse: 1.01946\n",
      "Early stopping, best iteration is:\n",
      "[110]\ttraining's rmse: 0.861693\tvalid_1's rmse: 1.01447\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.87078\tvalid_1's rmse: 1.04646\n",
      "[200]\ttraining's rmse: 0.792649\tvalid_1's rmse: 1.05378\n",
      "Early stopping, best iteration is:\n",
      "[108]\ttraining's rmse: 0.863441\tvalid_1's rmse: 1.04541\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.864995\tvalid_1's rmse: 1.07215\n",
      "[200]\ttraining's rmse: 0.786264\tvalid_1's rmse: 1.07233\n",
      "Early stopping, best iteration is:\n",
      "[151]\ttraining's rmse: 0.822082\tvalid_1's rmse: 1.07015\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.86024\tvalid_1's rmse: 1.06672\n",
      "Early stopping, best iteration is:\n",
      "[80]\ttraining's rmse: 0.880329\tvalid_1's rmse: 1.06444\n",
      "Our oof rmse score is: 1.0486161797967861\n",
      "Our oof cohen kappa score is: 0.5333471065233271\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.869328\tvalid_1's rmse: 1.02773\n",
      "[200]\ttraining's rmse: 0.790692\tvalid_1's rmse: 1.02705\n",
      "Early stopping, best iteration is:\n",
      "[131]\ttraining's rmse: 0.841701\tvalid_1's rmse: 1.02502\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.870642\tvalid_1's rmse: 1.0427\n",
      "[200]\ttraining's rmse: 0.792988\tvalid_1's rmse: 1.04655\n",
      "Early stopping, best iteration is:\n",
      "[110]\ttraining's rmse: 0.861693\tvalid_1's rmse: 1.04097\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.87078\tvalid_1's rmse: 1.00936\n",
      "[200]\ttraining's rmse: 0.792649\tvalid_1's rmse: 1.01257\n",
      "Early stopping, best iteration is:\n",
      "[108]\ttraining's rmse: 0.863441\tvalid_1's rmse: 1.00744\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.864995\tvalid_1's rmse: 1.06479\n",
      "[200]\ttraining's rmse: 0.786264\tvalid_1's rmse: 1.06309\n",
      "[300]\ttraining's rmse: 0.725495\tvalid_1's rmse: 1.05969\n",
      "[400]\ttraining's rmse: 0.674934\tvalid_1's rmse: 1.06194\n",
      "Early stopping, best iteration is:\n",
      "[314]\ttraining's rmse: 0.717927\tvalid_1's rmse: 1.05881\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.86024\tvalid_1's rmse: 1.05805\n",
      "Early stopping, best iteration is:\n",
      "[76]\ttraining's rmse: 0.884445\tvalid_1's rmse: 1.05609\n",
      "Our oof rmse score is: 1.037864396118481\n",
      "Our oof cohen kappa score is: 0.5419736248926257\n",
      "Our mean rmse score is:  1.0407656868319666\n",
      "Our mean cohen kappa score is:  0.5390721956277983\n"
     ]
    }
   ],
   "source": [
    "# train 5 times because the evaluation and training data change with the randomness\n",
    "y_pred_1, oof_rmse_score_1, oof_cohen_score_1 = run_lgb(reduce_train, ajusted_test, features)\n",
    "y_pred_2, oof_rmse_score_2, oof_cohen_score_2 = run_lgb(reduce_train, ajusted_test, features)\n",
    "y_pred_3, oof_rmse_score_3, oof_cohen_score_3 = run_lgb(reduce_train, ajusted_test, features)\n",
    "y_pred_4, oof_rmse_score_4, oof_cohen_score_4 = run_lgb(reduce_train, ajusted_test, features)\n",
    "y_pred_5, oof_rmse_score_5, oof_cohen_score_5 = run_lgb(reduce_train, ajusted_test, features)\n",
    "mean_rmse = (oof_rmse_score_1 + oof_rmse_score_2 + oof_rmse_score_3 + oof_rmse_score_4 + oof_rmse_score_5) / 5\n",
    "mean_cohen_kappa = (oof_cohen_score_1 + oof_cohen_score_2 + oof_cohen_score_3 + oof_cohen_score_4 + oof_cohen_score_5) / 5\n",
    "print('Our mean rmse score is: ', mean_rmse)\n",
    "print('Our mean cohen kappa score is: ', mean_cohen_kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression and neutal network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_std, test_std = standardize_data(reduce_train, ajusted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "used validation data:  721\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Our oof rmse score is: 1.1228520149459407\n",
      "Our oof cohen kappa score is: 0.49040254600952593\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Our oof rmse score is: 1.115791944553628\n",
      "Our oof cohen kappa score is: 0.48660052902676554\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Our oof rmse score is: 1.1057264044813024\n",
      "Our oof cohen kappa score is: 0.5101893696433124\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Our oof rmse score is: 1.1077952237396185\n",
      "Our oof cohen kappa score is: 0.5133393649518427\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Our oof rmse score is: 1.114891446431042\n",
      "Our oof cohen kappa score is: 0.5035140967052028\n",
      "Our mean rmse score is:  1.1134114068303063\n",
      "Our mean cohen kappa score is:  0.50080918126733\n"
     ]
    }
   ],
   "source": [
    "for i in train_std.columns:\n",
    "    if \"session_title\" in str(i):\n",
    "        lr_features.append(i)\n",
    "y_pred_1_lr, oof_rmse_score_1_lr, oof_cohen_score_1_lr = run_lr(train_std, test_std, lr_features)\n",
    "y_pred_2_lr, oof_rmse_score_2_lr, oof_cohen_score_2_lr = run_lr(train_std, test_std, lr_features)\n",
    "y_pred_3_lr, oof_rmse_score_3_lr, oof_cohen_score_3_lr = run_lr(train_std, test_std, lr_features)\n",
    "y_pred_4_lr, oof_rmse_score_4_lr, oof_cohen_score_4_lr = run_lr(train_std, test_std, lr_features)\n",
    "y_pred_5_lr, oof_rmse_score_5_lr, oof_cohen_score_5_lr = run_lr(train_std, test_std, lr_features)\n",
    "mean_rmse_lr = (oof_rmse_score_1_lr + oof_rmse_score_2_lr + oof_rmse_score_3_lr + oof_rmse_score_4_lr + oof_rmse_score_5_lr) / 5\n",
    "mean_cohen_kappa_lr = (oof_cohen_score_1_lr + oof_cohen_score_2_lr + oof_cohen_score_3_lr + oof_cohen_score_4_lr + oof_cohen_score_5_lr) / 5\n",
    "print('Our mean rmse score is: ', mean_rmse_lr)\n",
    "print('Our mean cohen kappa score is: ', mean_cohen_kappa_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "used validation data:  721\n",
      "Train on 14152 samples, validate on 721 samples\n",
      "Epoch 1/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.3104\n",
      "Epoch 00001: val_loss improved from inf to 1.20714, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 214us/sample - loss: 1.3072 - val_loss: 1.2071\n",
      "Epoch 2/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.1383\n",
      "Epoch 00002: val_loss did not improve from 1.20714\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.1397 - val_loss: 1.2460\n",
      "Epoch 3/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.1221\n",
      "Epoch 00003: val_loss did not improve from 1.20714\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.1222 - val_loss: 1.2073\n",
      "Epoch 4/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0988\n",
      "Epoch 00004: val_loss improved from 1.20714 to 1.19789, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0985 - val_loss: 1.1979\n",
      "Epoch 5/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0813\n",
      "Epoch 00005: val_loss did not improve from 1.19789\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0794 - val_loss: 1.2306\n",
      "Epoch 6/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0546\n",
      "Epoch 00006: val_loss improved from 1.19789 to 1.19326, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0563 - val_loss: 1.1933\n",
      "Epoch 7/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0629\n",
      "Epoch 00007: val_loss did not improve from 1.19326\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0602 - val_loss: 1.2121\n",
      "Epoch 8/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0448\n",
      "Epoch 00008: val_loss did not improve from 1.19326\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0432 - val_loss: 1.2348\n",
      "Epoch 9/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0374\n",
      "Epoch 00009: val_loss did not improve from 1.19326\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0365 - val_loss: 1.2595\n",
      "Epoch 10/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0358\n",
      "Epoch 00010: val_loss did not improve from 1.19326\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0341 - val_loss: 1.2404\n",
      "Epoch 11/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0400\n",
      "Epoch 00011: val_loss did not improve from 1.19326\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0425 - val_loss: 1.2342\n",
      "Epoch 12/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0292\n",
      "Epoch 00012: val_loss did not improve from 1.19326\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0283 - val_loss: 1.2682\n",
      "Epoch 13/100\n",
      "13760/14152 [============================>.] - ETA: 0s - loss: 1.0149\n",
      "Epoch 00013: val_loss did not improve from 1.19326\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0160 - val_loss: 1.2417\n",
      "Epoch 14/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0142\n",
      "Epoch 00014: val_loss improved from 1.19326 to 1.18114, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0184 - val_loss: 1.1811\n",
      "Epoch 15/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0054\n",
      "Epoch 00015: val_loss improved from 1.18114 to 1.16805, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0035 - val_loss: 1.1680\n",
      "Epoch 16/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0063\n",
      "Epoch 00016: val_loss improved from 1.16805 to 1.14702, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0061 - val_loss: 1.1470\n",
      "Epoch 17/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9981\n",
      "Epoch 00017: val_loss did not improve from 1.14702\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0005 - val_loss: 1.1996\n",
      "Epoch 18/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0068\n",
      "Epoch 00018: val_loss did not improve from 1.14702\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0085 - val_loss: 1.1775\n",
      "Epoch 19/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9990\n",
      "Epoch 00019: val_loss did not improve from 1.14702\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0004 - val_loss: 1.1935\n",
      "Epoch 20/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9902\n",
      "Epoch 00020: val_loss did not improve from 1.14702\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9905 - val_loss: 1.1554\n",
      "Epoch 21/100\n",
      "13760/14152 [============================>.] - ETA: 0s - loss: 0.9835\n",
      "Epoch 00021: val_loss did not improve from 1.14702\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9816 - val_loss: 1.1669\n",
      "Epoch 22/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9753\n",
      "Epoch 00022: val_loss did not improve from 1.14702\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9764 - val_loss: 1.2136\n",
      "Epoch 23/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9955\n",
      "Epoch 00023: val_loss did not improve from 1.14702\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 0.9945 - val_loss: 1.1923\n",
      "Epoch 24/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9923\n",
      "Epoch 00024: val_loss did not improve from 1.14702\n",
      "14152/14152 [==============================] - 2s 141us/sample - loss: 0.9919 - val_loss: 1.2009\n",
      "Epoch 25/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9775\n",
      "Epoch 00025: val_loss did not improve from 1.14702\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 0.9784 - val_loss: 1.1621\n",
      "Epoch 26/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9730\n",
      "Epoch 00026: val_loss did not improve from 1.14702\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9744 - val_loss: 1.1804\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Train on 14152 samples, validate on 722 samples\n",
      "Epoch 1/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.2918\n",
      "Epoch 00001: val_loss improved from inf to 1.08805, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 222us/sample - loss: 1.2913 - val_loss: 1.0881\n",
      "Epoch 2/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.1414\n",
      "Epoch 00002: val_loss did not improve from 1.08805\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.1424 - val_loss: 1.1103\n",
      "Epoch 3/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.1120\n",
      "Epoch 00003: val_loss improved from 1.08805 to 1.08580, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.1121 - val_loss: 1.0858\n",
      "Epoch 4/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0785\n",
      "Epoch 00004: val_loss improved from 1.08580 to 1.08105, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0806 - val_loss: 1.0811\n",
      "Epoch 5/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0571\n",
      "Epoch 00005: val_loss did not improve from 1.08105\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0595 - val_loss: 1.0990\n",
      "Epoch 6/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0451\n",
      "Epoch 00006: val_loss improved from 1.08105 to 1.05847, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0456 - val_loss: 1.0585\n",
      "Epoch 7/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0525\n",
      "Epoch 00007: val_loss did not improve from 1.05847\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0510 - val_loss: 1.0798\n",
      "Epoch 8/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0297\n",
      "Epoch 00008: val_loss did not improve from 1.05847\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0295 - val_loss: 1.0728\n",
      "Epoch 9/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0306\n",
      "Epoch 00009: val_loss did not improve from 1.05847\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0302 - val_loss: 1.0940\n",
      "Epoch 10/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0227\n",
      "Epoch 00010: val_loss did not improve from 1.05847\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0232 - val_loss: 1.0927\n",
      "Epoch 11/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0173\n",
      "Epoch 00011: val_loss did not improve from 1.05847\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0172 - val_loss: 1.0945\n",
      "Epoch 12/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0156\n",
      "Epoch 00012: val_loss did not improve from 1.05847\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0148 - val_loss: 1.1164\n",
      "Epoch 13/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0135\n",
      "Epoch 00013: val_loss did not improve from 1.05847\n",
      "14152/14152 [==============================] - 2s 142us/sample - loss: 1.0152 - val_loss: 1.1131\n",
      "Epoch 14/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0056\n",
      "Epoch 00014: val_loss improved from 1.05847 to 1.04936, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 144us/sample - loss: 1.0051 - val_loss: 1.0494\n",
      "Epoch 15/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0019\n",
      "Epoch 00015: val_loss did not improve from 1.04936\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0015 - val_loss: 1.0540\n",
      "Epoch 16/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0153\n",
      "Epoch 00016: val_loss did not improve from 1.04936\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0149 - val_loss: 1.1062\n",
      "Epoch 17/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9880\n",
      "Epoch 00017: val_loss did not improve from 1.04936\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9888 - val_loss: 1.0604\n",
      "Epoch 18/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9937\n",
      "Epoch 00018: val_loss did not improve from 1.04936\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 0.9939 - val_loss: 1.0605\n",
      "Epoch 19/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9858\n",
      "Epoch 00019: val_loss did not improve from 1.04936\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9857 - val_loss: 1.1388\n",
      "Epoch 20/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9879\n",
      "Epoch 00020: val_loss did not improve from 1.04936\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9891 - val_loss: 1.0727\n",
      "Epoch 21/100\n",
      "13760/14152 [============================>.] - ETA: 0s - loss: 0.9866\n",
      "Epoch 00021: val_loss did not improve from 1.04936\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 0.9874 - val_loss: 1.0678\n",
      "Epoch 22/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9828\n",
      "Epoch 00022: val_loss did not improve from 1.04936\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9818 - val_loss: 1.0786\n",
      "Epoch 23/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9688\n",
      "Epoch 00023: val_loss did not improve from 1.04936\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9689 - val_loss: 1.0851\n",
      "Epoch 24/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9744\n",
      "Epoch 00024: val_loss did not improve from 1.04936\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 0.9711 - val_loss: 1.0731\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Train on 14152 samples, validate on 723 samples\n",
      "Epoch 1/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.3218\n",
      "Epoch 00001: val_loss improved from inf to 1.12413, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 199us/sample - loss: 1.3183 - val_loss: 1.1241\n",
      "Epoch 2/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1499\n",
      "Epoch 00002: val_loss improved from 1.12413 to 1.11778, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.1489 - val_loss: 1.1178\n",
      "Epoch 3/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.1139\n",
      "Epoch 00003: val_loss did not improve from 1.11778\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.1158 - val_loss: 1.1395\n",
      "Epoch 4/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0928\n",
      "Epoch 00004: val_loss improved from 1.11778 to 1.10415, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0932 - val_loss: 1.1042\n",
      "Epoch 5/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0612\n",
      "Epoch 00005: val_loss did not improve from 1.10415\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0617 - val_loss: 1.1289\n",
      "Epoch 6/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0576\n",
      "Epoch 00006: val_loss did not improve from 1.10415\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0571 - val_loss: 1.1319\n",
      "Epoch 7/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0463\n",
      "Epoch 00007: val_loss did not improve from 1.10415\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0458 - val_loss: 1.1182\n",
      "Epoch 8/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0438\n",
      "Epoch 00008: val_loss improved from 1.10415 to 1.09682, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0434 - val_loss: 1.0968\n",
      "Epoch 9/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0314\n",
      "Epoch 00009: val_loss did not improve from 1.09682\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0310 - val_loss: 1.1083\n",
      "Epoch 10/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0343\n",
      "Epoch 00010: val_loss did not improve from 1.09682\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0345 - val_loss: 1.0997\n",
      "Epoch 11/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0331\n",
      "Epoch 00011: val_loss did not improve from 1.09682\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0332 - val_loss: 1.1631\n",
      "Epoch 12/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0177\n",
      "Epoch 00012: val_loss improved from 1.09682 to 1.09652, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0174 - val_loss: 1.0965\n",
      "Epoch 13/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0057\n",
      "Epoch 00013: val_loss did not improve from 1.09652\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0074 - val_loss: 1.1102\n",
      "Epoch 14/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0153\n",
      "Epoch 00014: val_loss did not improve from 1.09652\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0156 - val_loss: 1.1040\n",
      "Epoch 15/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9974\n",
      "Epoch 00015: val_loss improved from 1.09652 to 1.08998, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 0.9972 - val_loss: 1.0900\n",
      "Epoch 16/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0078\n",
      "Epoch 00016: val_loss improved from 1.08998 to 1.08381, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0101 - val_loss: 1.0838\n",
      "Epoch 17/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9970\n",
      "Epoch 00017: val_loss did not improve from 1.08381\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9995 - val_loss: 1.0918\n",
      "Epoch 18/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9912\n",
      "Epoch 00018: val_loss did not improve from 1.08381\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9913 - val_loss: 1.1120\n",
      "Epoch 19/100\n",
      "13760/14152 [============================>.] - ETA: 0s - loss: 0.9859\n",
      "Epoch 00019: val_loss did not improve from 1.08381\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9846 - val_loss: 1.1073\n",
      "Epoch 20/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9848\n",
      "Epoch 00020: val_loss did not improve from 1.08381\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9854 - val_loss: 1.0992\n",
      "Epoch 21/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9806\n",
      "Epoch 00021: val_loss did not improve from 1.08381\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9849 - val_loss: 1.0967\n",
      "Epoch 22/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9808\n",
      "Epoch 00022: val_loss did not improve from 1.08381\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9814 - val_loss: 1.0965\n",
      "Epoch 23/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9706\n",
      "Epoch 00023: val_loss did not improve from 1.08381\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9734 - val_loss: 1.0848\n",
      "Epoch 24/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9707\n",
      "Epoch 00024: val_loss did not improve from 1.08381\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9705 - val_loss: 1.0942\n",
      "Epoch 25/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9690\n",
      "Epoch 00025: val_loss did not improve from 1.08381\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9691 - val_loss: 1.1031\n",
      "Epoch 26/100\n",
      "13760/14152 [============================>.] - ETA: 0s - loss: 0.9680\n",
      "Epoch 00026: val_loss improved from 1.08381 to 1.08282, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9688 - val_loss: 1.0828\n",
      "Epoch 27/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9636\n",
      "Epoch 00027: val_loss did not improve from 1.08282\n",
      "14152/14152 [==============================] - 2s 144us/sample - loss: 0.9631 - val_loss: 1.1299\n",
      "Epoch 28/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9562\n",
      "Epoch 00028: val_loss improved from 1.08282 to 1.07557, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 148us/sample - loss: 0.9569 - val_loss: 1.0756\n",
      "Epoch 29/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9613\n",
      "Epoch 00029: val_loss did not improve from 1.07557\n",
      "14152/14152 [==============================] - 2s 147us/sample - loss: 0.9617 - val_loss: 1.1091\n",
      "Epoch 30/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9629\n",
      "Epoch 00030: val_loss improved from 1.07557 to 1.06885, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 147us/sample - loss: 0.9625 - val_loss: 1.0688\n",
      "Epoch 31/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9512\n",
      "Epoch 00031: val_loss did not improve from 1.06885\n",
      "14152/14152 [==============================] - 2s 147us/sample - loss: 0.9499 - val_loss: 1.1202\n",
      "Epoch 32/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9510\n",
      "Epoch 00032: val_loss did not improve from 1.06885\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 0.9504 - val_loss: 1.0913\n",
      "Epoch 33/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9611\n",
      "Epoch 00033: val_loss did not improve from 1.06885\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9599 - val_loss: 1.1363\n",
      "Epoch 34/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9477\n",
      "Epoch 00034: val_loss did not improve from 1.06885\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 0.9484 - val_loss: 1.0944\n",
      "Epoch 35/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9435\n",
      "Epoch 00035: val_loss did not improve from 1.06885\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9436 - val_loss: 1.0916\n",
      "Epoch 36/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9437\n",
      "Epoch 00036: val_loss did not improve from 1.06885\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9437 - val_loss: 1.1642\n",
      "Epoch 37/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9454\n",
      "Epoch 00037: val_loss did not improve from 1.06885\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 0.9435 - val_loss: 1.0834\n",
      "Epoch 38/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9328\n",
      "Epoch 00038: val_loss did not improve from 1.06885\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9341 - val_loss: 1.1252\n",
      "Epoch 39/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9419\n",
      "Epoch 00039: val_loss did not improve from 1.06885\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9413 - val_loss: 1.0928\n",
      "Epoch 40/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9329\n",
      "Epoch 00040: val_loss did not improve from 1.06885\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9330 - val_loss: 1.0977\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Train on 14152 samples, validate on 724 samples\n",
      "Epoch 1/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.3431\n",
      "Epoch 00001: val_loss improved from inf to 1.25259, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 197us/sample - loss: 1.3424 - val_loss: 1.2526\n",
      "Epoch 2/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.1520\n",
      "Epoch 00002: val_loss improved from 1.25259 to 1.23964, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.1527 - val_loss: 1.2396\n",
      "Epoch 3/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.1155\n",
      "Epoch 00003: val_loss did not improve from 1.23964\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.1139 - val_loss: 1.3346\n",
      "Epoch 4/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0946\n",
      "Epoch 00004: val_loss improved from 1.23964 to 1.23526, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0930 - val_loss: 1.2353\n",
      "Epoch 5/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0617\n",
      "Epoch 00005: val_loss improved from 1.23526 to 1.20216, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0631 - val_loss: 1.2022\n",
      "Epoch 6/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0574\n",
      "Epoch 00006: val_loss did not improve from 1.20216\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0571 - val_loss: 1.2045\n",
      "Epoch 7/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0422\n",
      "Epoch 00007: val_loss did not improve from 1.20216\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0408 - val_loss: 1.2625\n",
      "Epoch 8/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0346\n",
      "Epoch 00008: val_loss did not improve from 1.20216\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0319 - val_loss: 1.2285\n",
      "Epoch 9/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0302\n",
      "Epoch 00009: val_loss did not improve from 1.20216\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0301 - val_loss: 1.2073\n",
      "Epoch 10/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0214\n",
      "Epoch 00010: val_loss did not improve from 1.20216\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0223 - val_loss: 1.2361\n",
      "Epoch 11/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0191\n",
      "Epoch 00011: val_loss did not improve from 1.20216\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0189 - val_loss: 1.2305\n",
      "Epoch 12/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0157\n",
      "Epoch 00012: val_loss did not improve from 1.20216\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0154 - val_loss: 1.2118\n",
      "Epoch 13/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0002\n",
      "Epoch 00013: val_loss did not improve from 1.20216\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0000 - val_loss: 1.2597\n",
      "Epoch 14/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9973\n",
      "Epoch 00014: val_loss did not improve from 1.20216\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9948 - val_loss: 1.2879\n",
      "Epoch 15/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9975\n",
      "Epoch 00015: val_loss did not improve from 1.20216\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9993 - val_loss: 1.2187\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Train on 14152 samples, validate on 724 samples\n",
      "Epoch 1/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.3745\n",
      "Epoch 00001: val_loss improved from inf to 1.20755, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 196us/sample - loss: 1.3744 - val_loss: 1.2076\n",
      "Epoch 2/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.1390\n",
      "Epoch 00002: val_loss did not improve from 1.20755\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.1392 - val_loss: 1.2431\n",
      "Epoch 3/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0935\n",
      "Epoch 00003: val_loss improved from 1.20755 to 1.18850, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0947 - val_loss: 1.1885\n",
      "Epoch 4/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0672\n",
      "Epoch 00004: val_loss did not improve from 1.18850\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0698 - val_loss: 1.2197\n",
      "Epoch 5/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0549\n",
      "Epoch 00005: val_loss improved from 1.18850 to 1.16248, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0539 - val_loss: 1.1625\n",
      "Epoch 6/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0567\n",
      "Epoch 00006: val_loss did not improve from 1.16248\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0559 - val_loss: 1.1817\n",
      "Epoch 7/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0340\n",
      "Epoch 00007: val_loss did not improve from 1.16248\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0341 - val_loss: 1.1984\n",
      "Epoch 8/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0242\n",
      "Epoch 00008: val_loss did not improve from 1.16248\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0224 - val_loss: 1.2142\n",
      "Epoch 9/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0132\n",
      "Epoch 00009: val_loss did not improve from 1.16248\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0134 - val_loss: 1.2062\n",
      "Epoch 10/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0152\n",
      "Epoch 00010: val_loss did not improve from 1.16248\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0151 - val_loss: 1.1970\n",
      "Epoch 11/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0131\n",
      "Epoch 00011: val_loss did not improve from 1.16248\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0135 - val_loss: 1.1629\n",
      "Epoch 12/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0023\n",
      "Epoch 00012: val_loss did not improve from 1.16248\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0018 - val_loss: 1.1907\n",
      "Epoch 13/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9985\n",
      "Epoch 00013: val_loss did not improve from 1.16248\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9979 - val_loss: 1.2416\n",
      "Epoch 14/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9955\n",
      "Epoch 00014: val_loss did not improve from 1.16248\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9950 - val_loss: 1.1863\n",
      "Epoch 15/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9848\n",
      "Epoch 00015: val_loss improved from 1.16248 to 1.16234, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9856 - val_loss: 1.1623\n",
      "Epoch 16/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9818\n",
      "Epoch 00016: val_loss did not improve from 1.16234\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9820 - val_loss: 1.2234\n",
      "Epoch 17/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9762\n",
      "Epoch 00017: val_loss did not improve from 1.16234\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 0.9763 - val_loss: 1.1984\n",
      "Epoch 18/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9737\n",
      "Epoch 00018: val_loss improved from 1.16234 to 1.16198, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9757 - val_loss: 1.1620\n",
      "Epoch 19/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9611\n",
      "Epoch 00019: val_loss did not improve from 1.16198\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9617 - val_loss: 1.1872\n",
      "Epoch 20/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9621\n",
      "Epoch 00020: val_loss did not improve from 1.16198\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9632 - val_loss: 1.1802\n",
      "Epoch 21/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9710\n",
      "Epoch 00021: val_loss did not improve from 1.16198\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9721 - val_loss: 1.2080\n",
      "Epoch 22/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9720\n",
      "Epoch 00022: val_loss did not improve from 1.16198\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9718 - val_loss: 1.1904\n",
      "Epoch 23/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9550\n",
      "Epoch 00023: val_loss did not improve from 1.16198\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 0.9555 - val_loss: 1.2199\n",
      "Epoch 24/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9467\n",
      "Epoch 00024: val_loss did not improve from 1.16198\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9479 - val_loss: 1.2085\n",
      "Epoch 25/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9534\n",
      "Epoch 00025: val_loss did not improve from 1.16198\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9533 - val_loss: 1.2264\n",
      "Epoch 26/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9530\n",
      "Epoch 00026: val_loss did not improve from 1.16198\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9516 - val_loss: 1.2332\n",
      "Epoch 27/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9520\n",
      "Epoch 00027: val_loss did not improve from 1.16198\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9503 - val_loss: 1.2377\n",
      "Epoch 28/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9515\n",
      "Epoch 00028: val_loss did not improve from 1.16198\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9514 - val_loss: 1.1762\n",
      "Our oof rmse score is: 1.0610904394652227\n",
      "Our oof cohen kappa score is: 0.5197792826186214\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Train on 14152 samples, validate on 721 samples\n",
      "Epoch 1/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.3756\n",
      "Epoch 00001: val_loss improved from inf to 1.29950, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 220us/sample - loss: 1.3753 - val_loss: 1.2995\n",
      "Epoch 2/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1631\n",
      "Epoch 00002: val_loss improved from 1.29950 to 1.27343, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.1661 - val_loss: 1.2734\n",
      "Epoch 3/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.1282\n",
      "Epoch 00003: val_loss improved from 1.27343 to 1.27237, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.1288 - val_loss: 1.2724\n",
      "Epoch 4/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0862\n",
      "Epoch 00004: val_loss improved from 1.27237 to 1.25368, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0858 - val_loss: 1.2537\n",
      "Epoch 5/100\n",
      "13760/14152 [============================>.] - ETA: 0s - loss: 1.0835\n",
      "Epoch 00005: val_loss did not improve from 1.25368\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0814 - val_loss: 1.2926\n",
      "Epoch 6/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0610\n",
      "Epoch 00006: val_loss did not improve from 1.25368\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0611 - val_loss: 1.2586\n",
      "Epoch 7/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0558\n",
      "Epoch 00007: val_loss improved from 1.25368 to 1.21085, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0558 - val_loss: 1.2108\n",
      "Epoch 8/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0461\n",
      "Epoch 00008: val_loss did not improve from 1.21085\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0443 - val_loss: 1.2515\n",
      "Epoch 9/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0452\n",
      "Epoch 00009: val_loss did not improve from 1.21085\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0432 - val_loss: 1.2285\n",
      "Epoch 10/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0377\n",
      "Epoch 00010: val_loss did not improve from 1.21085\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0379 - val_loss: 1.2457\n",
      "Epoch 11/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0332\n",
      "Epoch 00011: val_loss did not improve from 1.21085\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0337 - val_loss: 1.2125\n",
      "Epoch 12/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0276\n",
      "Epoch 00012: val_loss improved from 1.21085 to 1.20425, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0276 - val_loss: 1.2043\n",
      "Epoch 13/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0190\n",
      "Epoch 00013: val_loss did not improve from 1.20425\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0210 - val_loss: 1.2342\n",
      "Epoch 14/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0102\n",
      "Epoch 00014: val_loss did not improve from 1.20425\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0101 - val_loss: 1.2096\n",
      "Epoch 15/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0120\n",
      "Epoch 00015: val_loss did not improve from 1.20425\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0146 - val_loss: 1.2351\n",
      "Epoch 16/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9986\n",
      "Epoch 00016: val_loss did not improve from 1.20425\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0005 - val_loss: 1.2559\n",
      "Epoch 17/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0046\n",
      "Epoch 00017: val_loss did not improve from 1.20425\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0045 - val_loss: 1.2496\n",
      "Epoch 18/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0057\n",
      "Epoch 00018: val_loss did not improve from 1.20425\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0043 - val_loss: 1.2103\n",
      "Epoch 19/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9958\n",
      "Epoch 00019: val_loss did not improve from 1.20425\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9986 - val_loss: 1.2060\n",
      "Epoch 20/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9837\n",
      "Epoch 00020: val_loss did not improve from 1.20425\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9845 - val_loss: 1.2049\n",
      "Epoch 21/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9854\n",
      "Epoch 00021: val_loss improved from 1.20425 to 1.19668, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 0.9857 - val_loss: 1.1967\n",
      "Epoch 22/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9883\n",
      "Epoch 00022: val_loss did not improve from 1.19668\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 0.9863 - val_loss: 1.2083\n",
      "Epoch 23/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9769\n",
      "Epoch 00023: val_loss did not improve from 1.19668\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9785 - val_loss: 1.2069\n",
      "Epoch 24/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9654\n",
      "Epoch 00024: val_loss improved from 1.19668 to 1.19501, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 0.9637 - val_loss: 1.1950\n",
      "Epoch 25/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9768\n",
      "Epoch 00025: val_loss did not improve from 1.19501\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 0.9765 - val_loss: 1.2104\n",
      "Epoch 26/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9705\n",
      "Epoch 00026: val_loss improved from 1.19501 to 1.19199, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 0.9696 - val_loss: 1.1920\n",
      "Epoch 27/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9665\n",
      "Epoch 00027: val_loss did not improve from 1.19199\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 0.9669 - val_loss: 1.2341\n",
      "Epoch 28/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9538\n",
      "Epoch 00028: val_loss did not improve from 1.19199\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9544 - val_loss: 1.2212\n",
      "Epoch 29/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9614\n",
      "Epoch 00029: val_loss did not improve from 1.19199\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 0.9614 - val_loss: 1.1978\n",
      "Epoch 30/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9634\n",
      "Epoch 00030: val_loss did not improve from 1.19199\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 0.9633 - val_loss: 1.1927\n",
      "Epoch 31/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9497\n",
      "Epoch 00031: val_loss did not improve from 1.19199\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 0.9501 - val_loss: 1.2130\n",
      "Epoch 32/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9514\n",
      "Epoch 00032: val_loss did not improve from 1.19199\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 0.9528 - val_loss: 1.2173\n",
      "Epoch 33/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9473\n",
      "Epoch 00033: val_loss did not improve from 1.19199\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 0.9491 - val_loss: 1.2035\n",
      "Epoch 34/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9450\n",
      "Epoch 00034: val_loss did not improve from 1.19199\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9442 - val_loss: 1.2542\n",
      "Epoch 35/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9395\n",
      "Epoch 00035: val_loss did not improve from 1.19199\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9395 - val_loss: 1.1986\n",
      "Epoch 36/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9506\n",
      "Epoch 00036: val_loss did not improve from 1.19199\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9493 - val_loss: 1.2109\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Train on 14152 samples, validate on 722 samples\n",
      "Epoch 1/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.3594\n",
      "Epoch 00001: val_loss improved from inf to 1.20486, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 198us/sample - loss: 1.3559 - val_loss: 1.2049\n",
      "Epoch 2/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.1422\n",
      "Epoch 00002: val_loss did not improve from 1.20486\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.1388 - val_loss: 1.2332\n",
      "Epoch 3/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.1022\n",
      "Epoch 00003: val_loss improved from 1.20486 to 1.10252, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 1.1021 - val_loss: 1.1025\n",
      "Epoch 4/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0805\n",
      "Epoch 00004: val_loss improved from 1.10252 to 1.09437, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 142us/sample - loss: 1.0792 - val_loss: 1.0944\n",
      "Epoch 5/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0578\n",
      "Epoch 00005: val_loss did not improve from 1.09437\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0577 - val_loss: 1.1356\n",
      "Epoch 6/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0541\n",
      "Epoch 00006: val_loss did not improve from 1.09437\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0536 - val_loss: 1.1045\n",
      "Epoch 7/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0413\n",
      "Epoch 00007: val_loss did not improve from 1.09437\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0408 - val_loss: 1.1089\n",
      "Epoch 8/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0285\n",
      "Epoch 00008: val_loss did not improve from 1.09437\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0296 - val_loss: 1.1153\n",
      "Epoch 9/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0135\n",
      "Epoch 00009: val_loss did not improve from 1.09437\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0141 - val_loss: 1.0984\n",
      "Epoch 10/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0139\n",
      "Epoch 00010: val_loss did not improve from 1.09437\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0141 - val_loss: 1.1177\n",
      "Epoch 11/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0114\n",
      "Epoch 00011: val_loss did not improve from 1.09437\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0116 - val_loss: 1.1348\n",
      "Epoch 12/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0015\n",
      "Epoch 00012: val_loss did not improve from 1.09437\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0016 - val_loss: 1.1206\n",
      "Epoch 13/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9978\n",
      "Epoch 00013: val_loss did not improve from 1.09437\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9973 - val_loss: 1.2442\n",
      "Epoch 14/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9986\n",
      "Epoch 00014: val_loss did not improve from 1.09437\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 0.9995 - val_loss: 1.1241\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Train on 14152 samples, validate on 723 samples\n",
      "Epoch 1/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.3541\n",
      "Epoch 00001: val_loss improved from inf to 1.12560, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 198us/sample - loss: 1.3526 - val_loss: 1.1256\n",
      "Epoch 2/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.1551\n",
      "Epoch 00002: val_loss improved from 1.12560 to 1.07584, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.1547 - val_loss: 1.0758\n",
      "Epoch 3/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1086\n",
      "Epoch 00003: val_loss improved from 1.07584 to 1.04831, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.1089 - val_loss: 1.0483\n",
      "Epoch 4/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0831\n",
      "Epoch 00004: val_loss improved from 1.04831 to 1.02303, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0844 - val_loss: 1.0230\n",
      "Epoch 5/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0670\n",
      "Epoch 00005: val_loss did not improve from 1.02303\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.0673 - val_loss: 1.0427\n",
      "Epoch 6/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0594\n",
      "Epoch 00006: val_loss improved from 1.02303 to 1.01984, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 142us/sample - loss: 1.0593 - val_loss: 1.0198\n",
      "Epoch 7/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0510\n",
      "Epoch 00007: val_loss did not improve from 1.01984\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.0543 - val_loss: 1.0515\n",
      "Epoch 8/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0410\n",
      "Epoch 00008: val_loss did not improve from 1.01984\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0414 - val_loss: 1.0241\n",
      "Epoch 9/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0333\n",
      "Epoch 00009: val_loss did not improve from 1.01984\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0345 - val_loss: 1.0274\n",
      "Epoch 10/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0249\n",
      "Epoch 00010: val_loss did not improve from 1.01984\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0248 - val_loss: 1.0249\n",
      "Epoch 11/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0228\n",
      "Epoch 00011: val_loss did not improve from 1.01984\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0232 - val_loss: 1.0463\n",
      "Epoch 12/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0315\n",
      "Epoch 00012: val_loss did not improve from 1.01984\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0322 - val_loss: 1.0239\n",
      "Epoch 13/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0119\n",
      "Epoch 00013: val_loss did not improve from 1.01984\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0142 - val_loss: 1.0783\n",
      "Epoch 14/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0109\n",
      "Epoch 00014: val_loss did not improve from 1.01984\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0112 - val_loss: 1.0422\n",
      "Epoch 15/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9972\n",
      "Epoch 00015: val_loss did not improve from 1.01984\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9969 - val_loss: 1.0406\n",
      "Epoch 16/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0015\n",
      "Epoch 00016: val_loss did not improve from 1.01984\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0016 - val_loss: 1.0801\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Train on 14152 samples, validate on 724 samples\n",
      "Epoch 1/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.3278\n",
      "Epoch 00001: val_loss improved from inf to 1.26504, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 196us/sample - loss: 1.3275 - val_loss: 1.2650\n",
      "Epoch 2/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1506\n",
      "Epoch 00002: val_loss did not improve from 1.26504\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.1511 - val_loss: 1.2721\n",
      "Epoch 3/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.1051\n",
      "Epoch 00003: val_loss improved from 1.26504 to 1.20375, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.1049 - val_loss: 1.2037\n",
      "Epoch 4/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0799\n",
      "Epoch 00004: val_loss did not improve from 1.20375\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0812 - val_loss: 1.2111\n",
      "Epoch 5/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0542\n",
      "Epoch 00005: val_loss improved from 1.20375 to 1.19864, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0534 - val_loss: 1.1986\n",
      "Epoch 6/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0440\n",
      "Epoch 00006: val_loss improved from 1.19864 to 1.19449, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0465 - val_loss: 1.1945\n",
      "Epoch 7/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0390\n",
      "Epoch 00007: val_loss improved from 1.19449 to 1.17200, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0395 - val_loss: 1.1720\n",
      "Epoch 8/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0264\n",
      "Epoch 00008: val_loss did not improve from 1.17200\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0260 - val_loss: 1.2366\n",
      "Epoch 9/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0211\n",
      "Epoch 00009: val_loss did not improve from 1.17200\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0225 - val_loss: 1.1791\n",
      "Epoch 10/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0238\n",
      "Epoch 00010: val_loss did not improve from 1.17200\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0220 - val_loss: 1.1963\n",
      "Epoch 11/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0137\n",
      "Epoch 00011: val_loss did not improve from 1.17200\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0131 - val_loss: 1.1911\n",
      "Epoch 12/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0109\n",
      "Epoch 00012: val_loss did not improve from 1.17200\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0107 - val_loss: 1.1980\n",
      "Epoch 13/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0042\n",
      "Epoch 00013: val_loss did not improve from 1.17200\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0042 - val_loss: 1.1896\n",
      "Epoch 14/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0033\n",
      "Epoch 00014: val_loss did not improve from 1.17200\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0024 - val_loss: 1.2244\n",
      "Epoch 15/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9969\n",
      "Epoch 00015: val_loss did not improve from 1.17200\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9963 - val_loss: 1.2143\n",
      "Epoch 16/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9881\n",
      "Epoch 00016: val_loss did not improve from 1.17200\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9874 - val_loss: 1.2643\n",
      "Epoch 17/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9863\n",
      "Epoch 00017: val_loss did not improve from 1.17200\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 0.9849 - val_loss: 1.1907\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Train on 14152 samples, validate on 724 samples\n",
      "Epoch 1/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.3504\n",
      "Epoch 00001: val_loss improved from inf to 1.20730, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 197us/sample - loss: 1.3500 - val_loss: 1.2073\n",
      "Epoch 2/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.1394\n",
      "Epoch 00002: val_loss improved from 1.20730 to 1.16549, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.1375 - val_loss: 1.1655\n",
      "Epoch 3/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0876\n",
      "Epoch 00003: val_loss did not improve from 1.16549\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0893 - val_loss: 1.2035\n",
      "Epoch 4/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0800\n",
      "Epoch 00004: val_loss did not improve from 1.16549\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0775 - val_loss: 1.2323\n",
      "Epoch 5/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0507\n",
      "Epoch 00005: val_loss improved from 1.16549 to 1.15578, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0518 - val_loss: 1.1558\n",
      "Epoch 6/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0296\n",
      "Epoch 00006: val_loss did not improve from 1.15578\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0291 - val_loss: 1.1719\n",
      "Epoch 7/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0393\n",
      "Epoch 00007: val_loss did not improve from 1.15578\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0403 - val_loss: 1.1722\n",
      "Epoch 8/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0237\n",
      "Epoch 00008: val_loss improved from 1.15578 to 1.14661, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0232 - val_loss: 1.1466\n",
      "Epoch 9/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0233\n",
      "Epoch 00009: val_loss did not improve from 1.14661\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0214 - val_loss: 1.1576\n",
      "Epoch 10/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0136\n",
      "Epoch 00010: val_loss did not improve from 1.14661\n",
      "14152/14152 [==============================] - 2s 132us/sample - loss: 1.0144 - val_loss: 1.1632\n",
      "Epoch 11/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0050\n",
      "Epoch 00011: val_loss did not improve from 1.14661\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0046 - val_loss: 1.1513\n",
      "Epoch 12/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0045\n",
      "Epoch 00012: val_loss did not improve from 1.14661\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0045 - val_loss: 1.1755\n",
      "Epoch 13/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9970\n",
      "Epoch 00013: val_loss did not improve from 1.14661\n",
      "14152/14152 [==============================] - 2s 146us/sample - loss: 0.9966 - val_loss: 1.1641\n",
      "Epoch 14/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9887\n",
      "Epoch 00014: val_loss did not improve from 1.14661\n",
      "14152/14152 [==============================] - 2s 147us/sample - loss: 0.9882 - val_loss: 1.2135\n",
      "Epoch 15/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9762\n",
      "Epoch 00015: val_loss improved from 1.14661 to 1.13834, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 150us/sample - loss: 0.9762 - val_loss: 1.1383\n",
      "Epoch 16/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9786\n",
      "Epoch 00016: val_loss did not improve from 1.13834\n",
      "14152/14152 [==============================] - 2s 146us/sample - loss: 0.9784 - val_loss: 1.2007\n",
      "Epoch 17/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9787\n",
      "Epoch 00017: val_loss did not improve from 1.13834\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 0.9782 - val_loss: 1.2526\n",
      "Epoch 18/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9736\n",
      "Epoch 00018: val_loss did not improve from 1.13834\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9736 - val_loss: 1.1976\n",
      "Epoch 19/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9563\n",
      "Epoch 00019: val_loss did not improve from 1.13834\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9581 - val_loss: 1.1865\n",
      "Epoch 20/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9686\n",
      "Epoch 00020: val_loss did not improve from 1.13834\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9683 - val_loss: 1.2080\n",
      "Epoch 21/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9745\n",
      "Epoch 00021: val_loss did not improve from 1.13834\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 0.9734 - val_loss: 1.1915\n",
      "Epoch 22/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9561\n",
      "Epoch 00022: val_loss did not improve from 1.13834\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9558 - val_loss: 1.1850\n",
      "Epoch 23/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9525\n",
      "Epoch 00023: val_loss did not improve from 1.13834\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9506 - val_loss: 1.1718\n",
      "Epoch 24/100\n",
      "13760/14152 [============================>.] - ETA: 0s - loss: 0.9504\n",
      "Epoch 00024: val_loss did not improve from 1.13834\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9517 - val_loss: 1.1583\n",
      "Epoch 25/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9480\n",
      "Epoch 00025: val_loss did not improve from 1.13834\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 0.9486 - val_loss: 1.1453\n",
      "Our oof rmse score is: 1.0598572040356298\n",
      "Our oof cohen kappa score is: 0.5261717257049279\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Train on 14152 samples, validate on 721 samples\n",
      "Epoch 1/100\n",
      "13760/14152 [============================>.] - ETA: 0s - loss: 1.3103\n",
      "Epoch 00001: val_loss improved from inf to 1.28568, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 195us/sample - loss: 1.3092 - val_loss: 1.2857\n",
      "Epoch 2/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.1691\n",
      "Epoch 00002: val_loss improved from 1.28568 to 1.25712, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.1680 - val_loss: 1.2571\n",
      "Epoch 3/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.1201\n",
      "Epoch 00003: val_loss did not improve from 1.25712\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.1197 - val_loss: 1.2688\n",
      "Epoch 4/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0961\n",
      "Epoch 00004: val_loss did not improve from 1.25712\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0979 - val_loss: 1.2642\n",
      "Epoch 5/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0798\n",
      "Epoch 00005: val_loss did not improve from 1.25712\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0836 - val_loss: 1.2664\n",
      "Epoch 6/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0653\n",
      "Epoch 00006: val_loss improved from 1.25712 to 1.23835, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0664 - val_loss: 1.2383\n",
      "Epoch 7/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0653\n",
      "Epoch 00007: val_loss did not improve from 1.23835\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0620 - val_loss: 1.3110\n",
      "Epoch 8/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0461\n",
      "Epoch 00008: val_loss did not improve from 1.23835\n",
      "14152/14152 [==============================] - 2s 132us/sample - loss: 1.0465 - val_loss: 1.2500\n",
      "Epoch 9/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0432\n",
      "Epoch 00009: val_loss improved from 1.23835 to 1.22544, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0427 - val_loss: 1.2254\n",
      "Epoch 10/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0362\n",
      "Epoch 00010: val_loss did not improve from 1.22544\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0363 - val_loss: 1.2441\n",
      "Epoch 11/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0341\n",
      "Epoch 00011: val_loss did not improve from 1.22544\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0331 - val_loss: 1.2558\n",
      "Epoch 12/100\n",
      "13760/14152 [============================>.] - ETA: 0s - loss: 1.0253\n",
      "Epoch 00012: val_loss did not improve from 1.22544\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0240 - val_loss: 1.2514\n",
      "Epoch 13/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0083\n",
      "Epoch 00013: val_loss improved from 1.22544 to 1.21893, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0076 - val_loss: 1.2189\n",
      "Epoch 14/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0060\n",
      "Epoch 00014: val_loss did not improve from 1.21893\n",
      "14152/14152 [==============================] - 2s 132us/sample - loss: 1.0056 - val_loss: 1.2340\n",
      "Epoch 15/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9973\n",
      "Epoch 00015: val_loss did not improve from 1.21893\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 0.9979 - val_loss: 1.2292\n",
      "Epoch 16/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9948\n",
      "Epoch 00016: val_loss improved from 1.21893 to 1.20552, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9979 - val_loss: 1.2055\n",
      "Epoch 17/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0028\n",
      "Epoch 00017: val_loss did not improve from 1.20552\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.0024 - val_loss: 1.2447\n",
      "Epoch 18/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9911\n",
      "Epoch 00018: val_loss improved from 1.20552 to 1.19894, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9891 - val_loss: 1.1989\n",
      "Epoch 19/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9880\n",
      "Epoch 00019: val_loss did not improve from 1.19894\n",
      "14152/14152 [==============================] - 2s 132us/sample - loss: 0.9884 - val_loss: 1.2031\n",
      "Epoch 20/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9798\n",
      "Epoch 00020: val_loss did not improve from 1.19894\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 0.9786 - val_loss: 1.2061\n",
      "Epoch 21/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9769\n",
      "Epoch 00021: val_loss did not improve from 1.19894\n",
      "14152/14152 [==============================] - 2s 132us/sample - loss: 0.9762 - val_loss: 1.2137\n",
      "Epoch 22/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9757\n",
      "Epoch 00022: val_loss did not improve from 1.19894\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 0.9738 - val_loss: 1.2263\n",
      "Epoch 23/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9729\n",
      "Epoch 00023: val_loss did not improve from 1.19894\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 0.9741 - val_loss: 1.2014\n",
      "Epoch 24/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9635\n",
      "Epoch 00024: val_loss did not improve from 1.19894\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 0.9657 - val_loss: 1.2140\n",
      "Epoch 25/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9806\n",
      "Epoch 00025: val_loss did not improve from 1.19894\n",
      "14152/14152 [==============================] - 2s 132us/sample - loss: 0.9802 - val_loss: 1.2108\n",
      "Epoch 26/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9638\n",
      "Epoch 00026: val_loss improved from 1.19894 to 1.18926, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9642 - val_loss: 1.1893\n",
      "Epoch 27/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9676\n",
      "Epoch 00027: val_loss did not improve from 1.18926\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9677 - val_loss: 1.2331\n",
      "Epoch 28/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9546\n",
      "Epoch 00028: val_loss did not improve from 1.18926\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9539 - val_loss: 1.1998\n",
      "Epoch 29/100\n",
      "13760/14152 [============================>.] - ETA: 0s - loss: 0.9453\n",
      "Epoch 00029: val_loss did not improve from 1.18926\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 0.9492 - val_loss: 1.2288\n",
      "Epoch 30/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9507\n",
      "Epoch 00030: val_loss did not improve from 1.18926\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 0.9513 - val_loss: 1.2244\n",
      "Epoch 31/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9578\n",
      "Epoch 00031: val_loss did not improve from 1.18926\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 0.9570 - val_loss: 1.2077\n",
      "Epoch 32/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9548\n",
      "Epoch 00032: val_loss did not improve from 1.18926\n",
      "14152/14152 [==============================] - 2s 132us/sample - loss: 0.9551 - val_loss: 1.1955\n",
      "Epoch 33/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9429\n",
      "Epoch 00033: val_loss did not improve from 1.18926\n",
      "14152/14152 [==============================] - 2s 131us/sample - loss: 0.9424 - val_loss: 1.2795\n",
      "Epoch 34/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9418\n",
      "Epoch 00034: val_loss did not improve from 1.18926\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 0.9435 - val_loss: 1.2276\n",
      "Epoch 35/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9299\n",
      "Epoch 00035: val_loss did not improve from 1.18926\n",
      "14152/14152 [==============================] - 2s 132us/sample - loss: 0.9296 - val_loss: 1.2031\n",
      "Epoch 36/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9422\n",
      "Epoch 00036: val_loss did not improve from 1.18926\n",
      "14152/14152 [==============================] - 2s 132us/sample - loss: 0.9429 - val_loss: 1.2162\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Train on 14152 samples, validate on 722 samples\n",
      "Epoch 1/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.3482\n",
      "Epoch 00001: val_loss improved from inf to 1.11058, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 199us/sample - loss: 1.3490 - val_loss: 1.1106\n",
      "Epoch 2/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.1405\n",
      "Epoch 00002: val_loss did not improve from 1.11058\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 1.1408 - val_loss: 1.1273\n",
      "Epoch 3/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.1090\n",
      "Epoch 00003: val_loss improved from 1.11058 to 1.10393, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.1074 - val_loss: 1.1039\n",
      "Epoch 4/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0872\n",
      "Epoch 00004: val_loss did not improve from 1.10393\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0879 - val_loss: 1.1315\n",
      "Epoch 5/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0704\n",
      "Epoch 00005: val_loss improved from 1.10393 to 1.06739, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0708 - val_loss: 1.0674\n",
      "Epoch 6/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0478\n",
      "Epoch 00006: val_loss did not improve from 1.06739\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0476 - val_loss: 1.1149\n",
      "Epoch 7/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0417\n",
      "Epoch 00007: val_loss did not improve from 1.06739\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0399 - val_loss: 1.0926\n",
      "Epoch 8/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0317\n",
      "Epoch 00008: val_loss did not improve from 1.06739\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0304 - val_loss: 1.0873\n",
      "Epoch 9/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0251\n",
      "Epoch 00009: val_loss did not improve from 1.06739\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0272 - val_loss: 1.1217\n",
      "Epoch 10/100\n",
      "13760/14152 [============================>.] - ETA: 0s - loss: 1.0280\n",
      "Epoch 00010: val_loss did not improve from 1.06739\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0279 - val_loss: 1.1429\n",
      "Epoch 11/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0277\n",
      "Epoch 00011: val_loss did not improve from 1.06739\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0245 - val_loss: 1.0966\n",
      "Epoch 12/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0237\n",
      "Epoch 00012: val_loss improved from 1.06739 to 1.06731, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0212 - val_loss: 1.0673\n",
      "Epoch 13/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0099\n",
      "Epoch 00013: val_loss did not improve from 1.06731\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0101 - val_loss: 1.1170\n",
      "Epoch 14/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0041\n",
      "Epoch 00014: val_loss did not improve from 1.06731\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0067 - val_loss: 1.0746\n",
      "Epoch 15/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9970\n",
      "Epoch 00015: val_loss did not improve from 1.06731\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9981 - val_loss: 1.0710\n",
      "Epoch 16/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9903\n",
      "Epoch 00016: val_loss did not improve from 1.06731\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 0.9938 - val_loss: 1.1126\n",
      "Epoch 17/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9986\n",
      "Epoch 00017: val_loss improved from 1.06731 to 1.05735, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 0.9974 - val_loss: 1.0574\n",
      "Epoch 18/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9930\n",
      "Epoch 00018: val_loss did not improve from 1.05735\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9919 - val_loss: 1.0715\n",
      "Epoch 19/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9732\n",
      "Epoch 00019: val_loss did not improve from 1.05735\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 0.9728 - val_loss: 1.0699\n",
      "Epoch 20/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9790\n",
      "Epoch 00020: val_loss did not improve from 1.05735\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 0.9806 - val_loss: 1.0832\n",
      "Epoch 21/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9654\n",
      "Epoch 00021: val_loss did not improve from 1.05735\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9665 - val_loss: 1.2036\n",
      "Epoch 22/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9765\n",
      "Epoch 00022: val_loss did not improve from 1.05735\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9766 - val_loss: 1.0806\n",
      "Epoch 23/100\n",
      "13760/14152 [============================>.] - ETA: 0s - loss: 0.9733\n",
      "Epoch 00023: val_loss did not improve from 1.05735\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9752 - val_loss: 1.1446\n",
      "Epoch 24/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9717\n",
      "Epoch 00024: val_loss improved from 1.05735 to 1.04185, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 141us/sample - loss: 0.9742 - val_loss: 1.0418\n",
      "Epoch 25/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9575\n",
      "Epoch 00025: val_loss improved from 1.04185 to 1.04027, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 0.9569 - val_loss: 1.0403\n",
      "Epoch 26/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9630\n",
      "Epoch 00026: val_loss did not improve from 1.04027\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 0.9636 - val_loss: 1.0871\n",
      "Epoch 27/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9496\n",
      "Epoch 00027: val_loss did not improve from 1.04027\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 0.9504 - val_loss: 1.0750\n",
      "Epoch 28/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9565\n",
      "Epoch 00028: val_loss did not improve from 1.04027\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9592 - val_loss: 1.0874\n",
      "Epoch 29/100\n",
      "13760/14152 [============================>.] - ETA: 0s - loss: 0.9540\n",
      "Epoch 00029: val_loss did not improve from 1.04027\n",
      "14152/14152 [==============================] - 2s 133us/sample - loss: 0.9557 - val_loss: 1.0689\n",
      "Epoch 30/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9471\n",
      "Epoch 00030: val_loss did not improve from 1.04027\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 0.9487 - val_loss: 1.0906\n",
      "Epoch 31/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9384\n",
      "Epoch 00031: val_loss did not improve from 1.04027\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9398 - val_loss: 1.0768\n",
      "Epoch 32/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9459\n",
      "Epoch 00032: val_loss did not improve from 1.04027\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 0.9469 - val_loss: 1.1040\n",
      "Epoch 33/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9319\n",
      "Epoch 00033: val_loss did not improve from 1.04027\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9326 - val_loss: 1.0719\n",
      "Epoch 34/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9260\n",
      "Epoch 00034: val_loss did not improve from 1.04027\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9269 - val_loss: 1.0783\n",
      "Epoch 35/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9319\n",
      "Epoch 00035: val_loss did not improve from 1.04027\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9311 - val_loss: 1.0810\n",
      "Fold: 3\n",
      "used validation data:  723\n",
      "Train on 14152 samples, validate on 723 samples\n",
      "Epoch 1/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.3037\n",
      "Epoch 00001: val_loss improved from inf to 1.10138, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 219us/sample - loss: 1.3004 - val_loss: 1.1014\n",
      "Epoch 2/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1421\n",
      "Epoch 00002: val_loss did not improve from 1.10138\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.1437 - val_loss: 1.1298\n",
      "Epoch 3/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1001\n",
      "Epoch 00003: val_loss improved from 1.10138 to 1.09246, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.1014 - val_loss: 1.0925\n",
      "Epoch 4/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0796\n",
      "Epoch 00004: val_loss did not improve from 1.09246\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0781 - val_loss: 1.0928\n",
      "Epoch 5/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0568\n",
      "Epoch 00005: val_loss did not improve from 1.09246\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0583 - val_loss: 1.0999\n",
      "Epoch 6/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0558\n",
      "Epoch 00006: val_loss improved from 1.09246 to 1.06824, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.0534 - val_loss: 1.0682\n",
      "Epoch 7/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0455\n",
      "Epoch 00007: val_loss improved from 1.06824 to 1.06014, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0451 - val_loss: 1.0601\n",
      "Epoch 8/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0433\n",
      "Epoch 00008: val_loss did not improve from 1.06014\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0435 - val_loss: 1.0685\n",
      "Epoch 9/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0311\n",
      "Epoch 00009: val_loss did not improve from 1.06014\n",
      "14152/14152 [==============================] - 2s 159us/sample - loss: 1.0309 - val_loss: 1.0692\n",
      "Epoch 10/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0349\n",
      "Epoch 00010: val_loss did not improve from 1.06014\n",
      "14152/14152 [==============================] - 2s 143us/sample - loss: 1.0345 - val_loss: 1.0917\n",
      "Epoch 11/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0152\n",
      "Epoch 00011: val_loss did not improve from 1.06014\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0181 - val_loss: 1.1249\n",
      "Epoch 12/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0238\n",
      "Epoch 00012: val_loss did not improve from 1.06014\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0237 - val_loss: 1.0628\n",
      "Epoch 13/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9972\n",
      "Epoch 00013: val_loss did not improve from 1.06014\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9984 - val_loss: 1.0950\n",
      "Epoch 14/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9968\n",
      "Epoch 00014: val_loss improved from 1.06014 to 1.05700, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0001 - val_loss: 1.0570\n",
      "Epoch 15/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0097\n",
      "Epoch 00015: val_loss improved from 1.05700 to 1.05244, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.0083 - val_loss: 1.0524\n",
      "Epoch 16/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0025\n",
      "Epoch 00016: val_loss did not improve from 1.05244\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0014 - val_loss: 1.1341\n",
      "Epoch 17/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9946\n",
      "Epoch 00017: val_loss did not improve from 1.05244\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9932 - val_loss: 1.0723\n",
      "Epoch 18/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9836\n",
      "Epoch 00018: val_loss did not improve from 1.05244\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9836 - val_loss: 1.0756\n",
      "Epoch 19/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9871\n",
      "Epoch 00019: val_loss did not improve from 1.05244\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 0.9881 - val_loss: 1.0597\n",
      "Epoch 20/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9716\n",
      "Epoch 00020: val_loss did not improve from 1.05244\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 0.9731 - val_loss: 1.0707\n",
      "Epoch 21/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9768\n",
      "Epoch 00021: val_loss did not improve from 1.05244\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 0.9771 - val_loss: 1.1057\n",
      "Epoch 22/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9709\n",
      "Epoch 00022: val_loss did not improve from 1.05244\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 0.9715 - val_loss: 1.0787\n",
      "Epoch 23/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9755\n",
      "Epoch 00023: val_loss did not improve from 1.05244\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9764 - val_loss: 1.0998\n",
      "Epoch 24/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9711\n",
      "Epoch 00024: val_loss did not improve from 1.05244\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 0.9708 - val_loss: 1.0863\n",
      "Epoch 25/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9699\n",
      "Epoch 00025: val_loss did not improve from 1.05244\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 0.9688 - val_loss: 1.0965\n",
      "Fold: 4\n",
      "used validation data:  724\n",
      "Train on 14152 samples, validate on 724 samples\n",
      "Epoch 1/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.3492\n",
      "Epoch 00001: val_loss improved from inf to 1.22295, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 201us/sample - loss: 1.3459 - val_loss: 1.2229\n",
      "Epoch 2/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1407\n",
      "Epoch 00002: val_loss improved from 1.22295 to 1.18925, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 144us/sample - loss: 1.1395 - val_loss: 1.1892\n",
      "Epoch 3/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.1005\n",
      "Epoch 00003: val_loss did not improve from 1.18925\n",
      "14152/14152 [==============================] - 2s 145us/sample - loss: 1.0990 - val_loss: 1.2274\n",
      "Epoch 4/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0789\n",
      "Epoch 00004: val_loss did not improve from 1.18925\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0764 - val_loss: 1.2253\n",
      "Epoch 5/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0617\n",
      "Epoch 00005: val_loss improved from 1.18925 to 1.17640, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 1.0616 - val_loss: 1.1764\n",
      "Epoch 6/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0467\n",
      "Epoch 00006: val_loss improved from 1.17640 to 1.16726, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.0472 - val_loss: 1.1673\n",
      "Epoch 7/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0421\n",
      "Epoch 00007: val_loss did not improve from 1.16726\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0417 - val_loss: 1.2103\n",
      "Epoch 8/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0399\n",
      "Epoch 00008: val_loss improved from 1.16726 to 1.15369, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0408 - val_loss: 1.1537\n",
      "Epoch 9/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0243\n",
      "Epoch 00009: val_loss did not improve from 1.15369\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0258 - val_loss: 1.1863\n",
      "Epoch 10/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0222\n",
      "Epoch 00010: val_loss did not improve from 1.15369\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0220 - val_loss: 1.1809\n",
      "Epoch 11/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0164\n",
      "Epoch 00011: val_loss did not improve from 1.15369\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0194 - val_loss: 1.1607\n",
      "Epoch 12/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0096\n",
      "Epoch 00012: val_loss did not improve from 1.15369\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0097 - val_loss: 1.2528\n",
      "Epoch 13/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0108\n",
      "Epoch 00013: val_loss did not improve from 1.15369\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0100 - val_loss: 1.2941\n",
      "Epoch 14/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9998\n",
      "Epoch 00014: val_loss did not improve from 1.15369\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0017 - val_loss: 1.2101\n",
      "Epoch 15/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9981\n",
      "Epoch 00015: val_loss did not improve from 1.15369\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 0.9981 - val_loss: 1.1945\n",
      "Epoch 16/100\n",
      "13760/14152 [============================>.] - ETA: 0s - loss: 0.9958\n",
      "Epoch 00016: val_loss did not improve from 1.15369\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9946 - val_loss: 1.1865\n",
      "Epoch 17/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9902\n",
      "Epoch 00017: val_loss did not improve from 1.15369\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 0.9924 - val_loss: 1.1728\n",
      "Epoch 18/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9897\n",
      "Epoch 00018: val_loss did not improve from 1.15369\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 0.9906 - val_loss: 1.1597\n",
      "Fold: 5\n",
      "used validation data:  724\n",
      "Train on 14152 samples, validate on 724 samples\n",
      "Epoch 1/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.3322\n",
      "Epoch 00001: val_loss improved from inf to 1.19087, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 199us/sample - loss: 1.3273 - val_loss: 1.1909\n",
      "Epoch 2/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.1353\n",
      "Epoch 00002: val_loss did not improve from 1.19087\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.1341 - val_loss: 1.2209\n",
      "Epoch 3/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0850\n",
      "Epoch 00003: val_loss did not improve from 1.19087\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0858 - val_loss: 1.2250\n",
      "Epoch 4/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0715\n",
      "Epoch 00004: val_loss did not improve from 1.19087\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0730 - val_loss: 1.2120\n",
      "Epoch 5/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0479\n",
      "Epoch 00005: val_loss did not improve from 1.19087\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0489 - val_loss: 1.2026\n",
      "Epoch 6/100\n",
      "13760/14152 [============================>.] - ETA: 0s - loss: 1.0349\n",
      "Epoch 00006: val_loss did not improve from 1.19087\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0345 - val_loss: 1.1993\n",
      "Epoch 7/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0248\n",
      "Epoch 00007: val_loss improved from 1.19087 to 1.18085, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0257 - val_loss: 1.1808\n",
      "Epoch 8/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0278\n",
      "Epoch 00008: val_loss improved from 1.18085 to 1.17467, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0276 - val_loss: 1.1747\n",
      "Epoch 9/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0168\n",
      "Epoch 00009: val_loss did not improve from 1.17467\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0142 - val_loss: 1.1773\n",
      "Epoch 10/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0079\n",
      "Epoch 00010: val_loss did not improve from 1.17467\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0062 - val_loss: 1.2078\n",
      "Epoch 11/100\n",
      "13760/14152 [============================>.] - ETA: 0s - loss: 1.0007\n",
      "Epoch 00011: val_loss did not improve from 1.17467\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 0.9987 - val_loss: 1.1959\n",
      "Epoch 12/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0015\n",
      "Epoch 00012: val_loss improved from 1.17467 to 1.17352, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0019 - val_loss: 1.1735\n",
      "Epoch 13/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9847\n",
      "Epoch 00013: val_loss improved from 1.17352 to 1.16840, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 0.9853 - val_loss: 1.1684\n",
      "Epoch 14/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9898\n",
      "Epoch 00014: val_loss did not improve from 1.16840\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 0.9895 - val_loss: 1.1714\n",
      "Epoch 15/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9769\n",
      "Epoch 00015: val_loss did not improve from 1.16840\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 0.9768 - val_loss: 1.2369\n",
      "Epoch 16/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9790\n",
      "Epoch 00016: val_loss did not improve from 1.16840\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 0.9784 - val_loss: 1.1873\n",
      "Epoch 17/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9580\n",
      "Epoch 00017: val_loss did not improve from 1.16840\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 0.9595 - val_loss: 1.1987\n",
      "Epoch 18/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9688\n",
      "Epoch 00018: val_loss did not improve from 1.16840\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 0.9705 - val_loss: 1.1816\n",
      "Epoch 19/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9639\n",
      "Epoch 00019: val_loss did not improve from 1.16840\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9638 - val_loss: 1.1937\n",
      "Epoch 20/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9536\n",
      "Epoch 00020: val_loss did not improve from 1.16840\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9526 - val_loss: 1.2914\n",
      "Epoch 21/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9524\n",
      "Epoch 00021: val_loss did not improve from 1.16840\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 0.9535 - val_loss: 1.2359\n",
      "Epoch 22/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9517\n",
      "Epoch 00022: val_loss did not improve from 1.16840\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 0.9506 - val_loss: 1.2331\n",
      "Epoch 23/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9555\n",
      "Epoch 00023: val_loss did not improve from 1.16840\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 0.9536 - val_loss: 1.2348\n",
      "Our oof rmse score is: 1.058686844872214\n",
      "Our oof cohen kappa score is: 0.5304593762404448\n",
      "Fold: 1\n",
      "used validation data:  721\n",
      "Train on 14152 samples, validate on 721 samples\n",
      "Epoch 1/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.3235\n",
      "Epoch 00001: val_loss improved from inf to 1.22928, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 197us/sample - loss: 1.3222 - val_loss: 1.2293\n",
      "Epoch 2/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.1628\n",
      "Epoch 00002: val_loss improved from 1.22928 to 1.15178, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.1643 - val_loss: 1.1518\n",
      "Epoch 3/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.1284\n",
      "Epoch 00003: val_loss improved from 1.15178 to 1.10931, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 148us/sample - loss: 1.1264 - val_loss: 1.1093\n",
      "Epoch 4/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0967\n",
      "Epoch 00004: val_loss improved from 1.10931 to 1.08466, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0954 - val_loss: 1.0847\n",
      "Epoch 5/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0703\n",
      "Epoch 00005: val_loss did not improve from 1.08466\n",
      "14152/14152 [==============================] - 3s 183us/sample - loss: 1.0711 - val_loss: 1.1603\n",
      "Epoch 6/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0751\n",
      "Epoch 00006: val_loss did not improve from 1.08466\n",
      "14152/14152 [==============================] - 2s 150us/sample - loss: 1.0733 - val_loss: 1.1101\n",
      "Epoch 7/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0621\n",
      "Epoch 00007: val_loss did not improve from 1.08466\n",
      "14152/14152 [==============================] - 2s 144us/sample - loss: 1.0620 - val_loss: 1.0926\n",
      "Epoch 8/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0482\n",
      "Epoch 00008: val_loss improved from 1.08466 to 1.05996, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0472 - val_loss: 1.0600\n",
      "Epoch 9/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0445\n",
      "Epoch 00009: val_loss did not improve from 1.05996\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0455 - val_loss: 1.0865\n",
      "Epoch 10/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0370\n",
      "Epoch 00010: val_loss did not improve from 1.05996\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0364 - val_loss: 1.1235\n",
      "Epoch 11/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0326\n",
      "Epoch 00011: val_loss did not improve from 1.05996\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0322 - val_loss: 1.0704\n",
      "Epoch 12/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0149\n",
      "Epoch 00012: val_loss did not improve from 1.05996\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0145 - val_loss: 1.0915\n",
      "Epoch 13/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0251\n",
      "Epoch 00013: val_loss did not improve from 1.05996\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0252 - val_loss: 1.1099\n",
      "Epoch 14/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0185\n",
      "Epoch 00014: val_loss did not improve from 1.05996\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0187 - val_loss: 1.0964\n",
      "Epoch 15/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0048\n",
      "Epoch 00015: val_loss did not improve from 1.05996\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0066 - val_loss: 1.0808\n",
      "Epoch 16/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0008\n",
      "Epoch 00016: val_loss did not improve from 1.05996\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 1.0045 - val_loss: 1.0774\n",
      "Epoch 17/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0156\n",
      "Epoch 00017: val_loss did not improve from 1.05996\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0157 - val_loss: 1.0845\n",
      "Epoch 18/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9985\n",
      "Epoch 00018: val_loss did not improve from 1.05996\n",
      "14152/14152 [==============================] - 2s 134us/sample - loss: 0.9997 - val_loss: 1.0642\n",
      "Fold: 2\n",
      "used validation data:  722\n",
      "Train on 14152 samples, validate on 722 samples\n",
      "Epoch 1/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.2988\n",
      "Epoch 00001: val_loss improved from inf to 1.23892, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 197us/sample - loss: 1.2978 - val_loss: 1.2389\n",
      "Epoch 2/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1312\n",
      "Epoch 00002: val_loss improved from 1.23892 to 1.14576, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.1312 - val_loss: 1.1458\n",
      "Epoch 3/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0953\n",
      "Epoch 00003: val_loss did not improve from 1.14576\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0967 - val_loss: 1.2062\n",
      "Epoch 4/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0699\n",
      "Epoch 00004: val_loss improved from 1.14576 to 1.14112, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0707 - val_loss: 1.1411\n",
      "Epoch 5/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0553\n",
      "Epoch 00005: val_loss did not improve from 1.14112\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0561 - val_loss: 1.1945\n",
      "Epoch 6/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0569\n",
      "Epoch 00006: val_loss did not improve from 1.14112\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0549 - val_loss: 1.1711\n",
      "Epoch 7/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0466\n",
      "Epoch 00007: val_loss improved from 1.14112 to 1.11602, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0466 - val_loss: 1.1160\n",
      "Epoch 8/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0465\n",
      "Epoch 00008: val_loss did not improve from 1.11602\n",
      "14152/14152 [==============================] - 2s 136us/sample - loss: 1.0470 - val_loss: 1.1483\n",
      "Epoch 9/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0292\n",
      "Epoch 00009: val_loss did not improve from 1.11602\n",
      "14152/14152 [==============================] - 2s 137us/sample - loss: 1.0289 - val_loss: 1.1622\n",
      "Epoch 10/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0330\n",
      "Epoch 00010: val_loss did not improve from 1.11602\n",
      "14152/14152 [==============================] - 2s 135us/sample - loss: 1.0329 - val_loss: 1.1487\n",
      "Epoch 11/100\n",
      " 3584/14152 [======>.......................] - ETA: 1s - loss: 1.0323"
     ]
    }
   ],
   "source": [
    "for i in train_std.columns:\n",
    "    if \"session_title\" in str(i):\n",
    "        nn_features.append(i)\n",
    "y_pred_1_nn, oof_rmse_score_1_nn, oof_cohen_score_1_nn = run_nn(train_std, test_std, nn_features)\n",
    "y_pred_2_nn, oof_rmse_score_2_nn, oof_cohen_score_2_nn = run_nn(train_std, test_std, nn_features)\n",
    "y_pred_3_nn, oof_rmse_score_3_nn, oof_cohen_score_3_nn = run_nn(train_std, test_std, nn_features)\n",
    "y_pred_4_nn, oof_rmse_score_4_nn, oof_cohen_score_4_nn = run_nn(train_std, test_std, nn_features)\n",
    "y_pred_5_nn, oof_rmse_score_5_nn, oof_cohen_score_5_nn = run_nn(train_std, test_std, nn_features)\n",
    "mean_rmse_nn = (oof_rmse_score_1_nn + oof_rmse_score_2_nn + oof_rmse_score_3_nn + oof_rmse_score_4_nn + oof_rmse_score_5_nn) / 5\n",
    "mean_cohen_kappa_nn = (oof_cohen_score_1_nn + oof_cohen_score_2_nn + oof_cohen_score_3_nn + oof_cohen_score_4_nn + oof_cohen_score_5_nn) / 5\n",
    "print('Our mean rmse score is: ', mean_rmse_nn)\n",
    "print('Our mean cohen kappa score is: ', mean_cohen_kappa_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    0.499\n",
      "0    0.239\n",
      "1    0.137\n",
      "2    0.125\n",
      "Name: accuracy_group, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "y_final_lgb = (y_pred_1 + y_pred_2 + y_pred_3 + y_pred_4 + y_pred_5) / 5\n",
    "y_final_lr = (y_pred_1_lr + y_pred_2_lr + y_pred_3_lr + y_pred_4_lr + y_pred_5_lr) / 5\n",
    "y_final_nn = (y_pred_1_nn + y_pred_2_nn + y_pred_3_nn + y_pred_4_nn + y_pred_5_nn) / 5\n",
    "y_final = y_final_lgb * 0.85 + y_final_nn * 0.1 + y_final_lr * 0.05\n",
    "y_final = eval_qwk_lgb_regr(y_final, reduce_train)\n",
    "predict(sample_submission, y_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "05abdde115284f729eacd4e076edded9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "08d39160c82e449195f0d0108f2cf9c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0d5dac8ad9db43b4888e76d23f5ebfce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "16589192bb394ba299545f08063f06ce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "399c875b45a947d6a918b1f0895ec7ce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3a6a21df2e6b40e9ba1f6d098d7ec199": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "571d197fb0eb4c36a747e8544e15fef4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cc0ca6782bb7404d9a16b445077d75c0",
       "placeholder": "",
       "style": "IPY_MODEL_9328a624ed3246bf9f6948a70f72f9c3",
       "value": " 17000/17000 [11:48&lt;00:00, 24.00it/s]"
      }
     },
     "5b45ccabb6d14ce8806594c8f33a819f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "67434a3e107645f799e1a640236c711e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c055511a198440839b99a07c372646a8",
       "max": 17000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d985bc318e4746849c016c7339950c73",
       "value": 17000
      }
     },
     "6efb6874a4c24ea0ab2fa34bf51f59f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "7b675fbdef854311a825c134f0e199d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3a6a21df2e6b40e9ba1f6d098d7ec199",
       "placeholder": "",
       "style": "IPY_MODEL_5b45ccabb6d14ce8806594c8f33a819f",
       "value": " 1000/1000 [01:42&lt;00:00,  9.77it/s]"
      }
     },
     "8244870fff5b4388acd50a8dd51e066f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_05abdde115284f729eacd4e076edded9",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6efb6874a4c24ea0ab2fa34bf51f59f4",
       "value": 1000
      }
     },
     "86cd03dee06046b5a5f700ca1600aa9d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "9328a624ed3246bf9f6948a70f72f9c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9ac056ca5a6c405da752321a04b6a9e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8244870fff5b4388acd50a8dd51e066f",
        "IPY_MODEL_a5fff538aca14132bafc0f86f5567483"
       ],
       "layout": "IPY_MODEL_0d5dac8ad9db43b4888e76d23f5ebfce"
      }
     },
     "a5fff538aca14132bafc0f86f5567483": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_399c875b45a947d6a918b1f0895ec7ce",
       "placeholder": "",
       "style": "IPY_MODEL_eb5ac27df0254505859010a4e625db2e",
       "value": " 1000/1000 [11:17&lt;00:00,  1.48it/s]"
      }
     },
     "b0c785cb370e4cf3b63940ade355b325": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b55a601fdc9e4a36b3904e9770df02c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_16589192bb394ba299545f08063f06ce",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_86cd03dee06046b5a5f700ca1600aa9d",
       "value": 1000
      }
     },
     "c022f6d5941546109d7a7727ae8454ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_67434a3e107645f799e1a640236c711e",
        "IPY_MODEL_571d197fb0eb4c36a747e8544e15fef4"
       ],
       "layout": "IPY_MODEL_b0c785cb370e4cf3b63940ade355b325"
      }
     },
     "c055511a198440839b99a07c372646a8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cc0ca6782bb7404d9a16b445077d75c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d985bc318e4746849c016c7339950c73": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "e86ce07eabda48109e4500a92c2ba05a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b55a601fdc9e4a36b3904e9770df02c1",
        "IPY_MODEL_7b675fbdef854311a825c134f0e199d4"
       ],
       "layout": "IPY_MODEL_08d39160c82e449195f0d0108f2cf9c0"
      }
     },
     "eb5ac27df0254505859010a4e625db2e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
