{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024317,
     "end_time": "2020-11-11T23:40:51.852683",
     "exception": false,
     "start_time": "2020-11-11T23:40:51.828366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- 1st grownet\n",
    "https://www.kaggle.com/tmhrkt/grownet-gradient-boosting-neural-networks/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-11-11T23:40:51.906753Z",
     "iopub.status.busy": "2020-11-11T23:40:51.905887Z",
     "iopub.status.idle": "2020-11-11T23:40:59.614428Z",
     "shell.execute_reply": "2020-11-11T23:40:59.613248Z"
    },
    "papermill": {
     "duration": 7.740486,
     "end_time": "2020-11-11T23:40:59.614563",
     "exception": false,
     "start_time": "2020-11-11T23:40:51.874077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss,roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "sys.path.append('../input/multilabelstraifier/')\n",
    "from ml_stratifiers import MultilabelStratifiedKFold\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "from torch.nn.modules.loss import _WeightedLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022736,
     "end_time": "2020-11-11T23:40:59.660346",
     "exception": false,
     "start_time": "2020-11-11T23:40:59.637610",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-11-11T23:40:59.719601Z",
     "iopub.status.busy": "2020-11-11T23:40:59.718444Z",
     "iopub.status.idle": "2020-11-11T23:41:07.181529Z",
     "shell.execute_reply": "2020-11-11T23:41:07.182767Z"
    },
    "papermill": {
     "duration": 7.499696,
     "end_time": "2020-11-11T23:41:07.182973",
     "exception": false,
     "start_time": "2020-11-11T23:40:59.683277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/kaggle/input/lish-moa/'\n",
    "train = pd.read_csv(DATA_DIR + 'train_features.csv')\n",
    "targets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "non_targets = pd.read_csv(DATA_DIR + 'train_targets_nonscored.csv')\n",
    "test = pd.read_csv(DATA_DIR + 'test_features.csv')\n",
    "sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "drug = pd.read_csv(DATA_DIR + 'train_drug.csv')\n",
    "\n",
    "target_feats = [ i for i in targets.columns if i != \"sig_id\"]\n",
    "g_feats = [i for i in train.columns if \"g-\" in i]\n",
    "c_feats = [i for i in train.columns if \"c-\" in i]\n",
    "\n",
    "noncons_train_index = train[train.cp_type==\"ctl_vehicle\"].index\n",
    "cons_train_index = train[train.cp_type!=\"ctl_vehicle\"].index\n",
    "noncons_test_index = test[test.cp_type==\"ctl_vehicle\"].index\n",
    "cons_test_index = test[test.cp_type!=\"ctl_vehicle\"].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.064251,
     "end_time": "2020-11-11T23:41:07.281135",
     "exception": false,
     "start_time": "2020-11-11T23:41:07.216884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:07.361709Z",
     "iopub.status.busy": "2020-11-11T23:41:07.360760Z",
     "iopub.status.idle": "2020-11-11T23:41:07.672761Z",
     "shell.execute_reply": "2020-11-11T23:41:07.673608Z"
    },
    "papermill": {
     "duration": 0.360441,
     "end_time": "2020-11-11T23:41:07.673836",
     "exception": false,
     "start_time": "2020-11-11T23:41:07.313395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train[train.index.isin(cons_train_index)].copy().reset_index(drop=True)\n",
    "test = test[test.index.isin(cons_test_index)].copy().reset_index(drop=True)\n",
    "targets = targets[targets.index.isin(cons_train_index)].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:07.754516Z",
     "iopub.status.busy": "2020-11-11T23:41:07.753609Z",
     "iopub.status.idle": "2020-11-11T23:41:08.518142Z",
     "shell.execute_reply": "2020-11-11T23:41:08.517573Z"
    },
    "papermill": {
     "duration": 0.806017,
     "end_time": "2020-11-11T23:41:08.518285",
     "exception": false,
     "start_time": "2020-11-11T23:41:07.712268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 6, ..., 3, 4, 0]], dtype=int8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_SPLITS = 7\n",
    "seed = 34\n",
    "\n",
    "folds = []\n",
    "    \n",
    "# LOAD FILES\n",
    "train_score = targets.merge(drug, on='sig_id', how='left') \n",
    "\n",
    "# LOCATE DRUGS\n",
    "vc = train_score.drug_id.value_counts()\n",
    "vc1 = vc.loc[vc <= 18].index.sort_values()\n",
    "vc2 = vc.loc[vc > 18].index.sort_values()\n",
    "    \n",
    "# STRATIFY DRUGS 18X OR LESS\n",
    "dct1 = {}; dct2 = {}\n",
    "skf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, shuffle = True, random_state = seed)\n",
    "tmp = train_score.groupby('drug_id')[target_feats].mean().loc[vc1]\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_feats])):\n",
    "    dd = {k:fold for k in tmp.index[idxV].values}\n",
    "    dct1.update(dd)\n",
    "    \n",
    "# STRATIFY DRUGS MORE THAN 18X\n",
    "skf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, shuffle = True, random_state = seed)\n",
    "tmp = train_score.loc[train_score.drug_id.isin(vc2)].reset_index(drop = True)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_feats])):\n",
    "    dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "    dct2.update(dd)\n",
    "\n",
    "# ASSIGN FOLDS\n",
    "train_score['fold'] = train_score.drug_id.map(dct1)\n",
    "train_score.loc[train_score.fold.isna(),'fold'] = train_score.loc[train_score.fold.isna(),'sig_id'].map(dct2)\n",
    "train_score.fold = train_score.fold.astype('int8')\n",
    "folds.append(train_score.fold.values)\n",
    "    \n",
    "np.array(folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023649,
     "end_time": "2020-11-11T23:41:08.566130",
     "exception": false,
     "start_time": "2020-11-11T23:41:08.542481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# feature enginnering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:08.669485Z",
     "iopub.status.busy": "2020-11-11T23:41:08.667915Z",
     "iopub.status.idle": "2020-11-11T23:41:08.995595Z",
     "shell.execute_reply": "2020-11-11T23:41:08.996165Z"
    },
    "papermill": {
     "duration": 0.406515,
     "end_time": "2020-11-11T23:41:08.996338",
     "exception": false,
     "start_time": "2020-11-11T23:41:08.589823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "X = train.iloc[:,4:].copy().values\n",
    "select = VarianceThreshold(threshold=0.7)\n",
    "X_new = select.fit_transform(X)\n",
    "drop_feats = list(np.array(train.iloc[:,4:].columns)[select.get_support()==False])\n",
    "print(len(drop_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:09.100225Z",
     "iopub.status.busy": "2020-11-11T23:41:09.096740Z",
     "iopub.status.idle": "2020-11-11T23:41:09.110652Z",
     "shell.execute_reply": "2020-11-11T23:41:09.110028Z"
    },
    "papermill": {
     "duration": 0.090121,
     "end_time": "2020-11-11T23:41:09.110771",
     "exception": false,
     "start_time": "2020-11-11T23:41:09.020650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.drop(drop_feats, axis=1, inplace=True)\n",
    "test.drop(drop_feats, axis=1, inplace=True)\n",
    "\n",
    "g_feats = [i for i in train.columns if \"g-\" in i]\n",
    "c_feats = [i for i in train.columns if \"c-\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:09.170562Z",
     "iopub.status.busy": "2020-11-11T23:41:09.169619Z",
     "iopub.status.idle": "2020-11-11T23:41:21.517203Z",
     "shell.execute_reply": "2020-11-11T23:41:21.516421Z"
    },
    "papermill": {
     "duration": 12.380829,
     "end_time": "2020-11-11T23:41:21.517372",
     "exception": false,
     "start_time": "2020-11-11T23:41:09.136543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rank gauss\n",
    "for i in c_feats + g_feats:\n",
    "    ss = preprocessing.QuantileTransformer(n_quantiles=1000, random_state=0, output_distribution=\"normal\")\n",
    "    #    ss.fit(pd.concat([train[i], test[i]]).values.reshape(-1,1))\n",
    "    ss.fit(train[i].values.reshape(-1,1))\n",
    "    train[i] = ss.transform(train[i].values.reshape(-1,1))\n",
    "    test[i] = ss.transform(test[i].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:21.589005Z",
     "iopub.status.busy": "2020-11-11T23:41:21.587496Z",
     "iopub.status.idle": "2020-11-11T23:41:23.924978Z",
     "shell.execute_reply": "2020-11-11T23:41:23.923767Z"
    },
    "papermill": {
     "duration": 2.383306,
     "end_time": "2020-11-11T23:41:23.925125",
     "exception": false,
     "start_time": "2020-11-11T23:41:21.541819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_num = 10\n",
    "pca_c_cols = [\"pca-c\"+str(i+1) for i in range(c_num)]\n",
    "pca = PCA(n_components=c_num,random_state=42)\n",
    "c_train = pca.fit_transform(train[c_feats])\n",
    "c_test = pca.transform(test[c_feats])\n",
    "c_train = pd.DataFrame(c_train, columns=pca_c_cols)\n",
    "c_test = pd.DataFrame(c_test, columns=pca_c_cols)\n",
    "\n",
    "g_num = 60\n",
    "pca_g_cols = [\"pca-g\"+str(i+1) for i in range(g_num)]\n",
    "pca = PCA(n_components=g_num, random_state=42)\n",
    "g_train = pca.fit_transform(train[g_feats])\n",
    "g_test = pca.transform(test[g_feats])\n",
    "g_train = pd.DataFrame(g_train, columns=pca_g_cols)\n",
    "g_test = pd.DataFrame(g_test, columns=pca_g_cols)\n",
    "\n",
    "train = pd.concat([train, c_train],axis=1)\n",
    "test = pd.concat([test, c_test],axis=1)\n",
    "train = pd.concat([train, g_train],axis=1)\n",
    "test = pd.concat([test, g_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:23.989043Z",
     "iopub.status.busy": "2020-11-11T23:41:23.987702Z",
     "iopub.status.idle": "2020-11-11T23:41:25.996125Z",
     "shell.execute_reply": "2020-11-11T23:41:25.995306Z"
    },
    "papermill": {
     "duration": 2.044957,
     "end_time": "2020-11-11T23:41:25.996295",
     "exception": false,
     "start_time": "2020-11-11T23:41:23.951338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 919) (3624, 919)\n"
     ]
    }
   ],
   "source": [
    "def fe(df):\n",
    "    tmp = df.copy()\n",
    "    tmp['g_kurt'] = tmp[g_feats].kurtosis(axis = 1)\n",
    "    tmp['g_skew'] = tmp[g_feats].skew(axis = 1)\n",
    "    tmp['c_kurt'] = tmp[c_feats].kurtosis(axis = 1)\n",
    "    tmp['c_skew'] = tmp[c_feats].skew(axis = 1)\n",
    "    tmp = pd.get_dummies(tmp, columns=['cp_time','cp_dose'])\n",
    "    tmp.drop([\"cp_type\"], axis=1, inplace=True)\n",
    "    return tmp\n",
    "\n",
    "train = fe(train)\n",
    "test = fe(test)\n",
    "\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:26.052861Z",
     "iopub.status.busy": "2020-11-11T23:41:26.051841Z",
     "iopub.status.idle": "2020-11-11T23:41:26.054360Z",
     "shell.execute_reply": "2020-11-11T23:41:26.054849Z"
    },
    "papermill": {
     "duration": 0.032452,
     "end_time": "2020-11-11T23:41:26.054981",
     "exception": false,
     "start_time": "2020-11-11T23:41:26.022529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_cols = train.columns[1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:26.111682Z",
     "iopub.status.busy": "2020-11-11T23:41:26.110210Z",
     "iopub.status.idle": "2020-11-11T23:41:26.227411Z",
     "shell.execute_reply": "2020-11-11T23:41:26.227942Z"
    },
    "papermill": {
     "duration": 0.147629,
     "end_time": "2020-11-11T23:41:26.228105",
     "exception": false,
     "start_time": "2020-11-11T23:41:26.080476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.merge(targets, on =\"sig_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:26.286454Z",
     "iopub.status.busy": "2020-11-11T23:41:26.285479Z",
     "iopub.status.idle": "2020-11-11T23:41:26.288573Z",
     "shell.execute_reply": "2020-11-11T23:41:26.287991Z"
    },
    "papermill": {
     "duration": 0.03441,
     "end_time": "2020-11-11T23:41:26.288688",
     "exception": false,
     "start_time": "2020-11-11T23:41:26.254278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train[\"fold\"] = np.array(folds).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024208,
     "end_time": "2020-11-11T23:41:26.337809",
     "exception": false,
     "start_time": "2020-11-11T23:41:26.313601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:26.400857Z",
     "iopub.status.busy": "2020-11-11T23:41:26.400014Z",
     "iopub.status.idle": "2020-11-11T23:41:26.403963Z",
     "shell.execute_reply": "2020-11-11T23:41:26.403477Z"
    },
    "papermill": {
     "duration": 0.040505,
     "end_time": "2020-11-11T23:41:26.404062",
     "exception": false,
     "start_time": "2020-11-11T23:41:26.363557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MoADataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :] , dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:26.486253Z",
     "iopub.status.busy": "2020-11-11T23:41:26.472079Z",
     "iopub.status.idle": "2020-11-11T23:41:26.489345Z",
     "shell.execute_reply": "2020-11-11T23:41:26.488774Z"
    },
    "papermill": {
     "duration": 0.060188,
     "end_time": "2020-11-11T23:41:26.489450",
     "exception": false,
     "start_time": "2020-11-11T23:41:26.429262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class ForwardType(Enum):\n",
    "    SIMPLE = 0\n",
    "    STACKED = 1\n",
    "    CASCADE = 2\n",
    "    GRADIENT = 3\n",
    "\n",
    "class DynamicNet(object):\n",
    "    def __init__(self, c0, lr):\n",
    "        self.models = []\n",
    "        self.c0 = c0\n",
    "        self.lr = lr\n",
    "        self.boost_rate  = nn.Parameter(torch.tensor(lr, requires_grad=True, device=\"cuda\"))\n",
    "\n",
    "    def add(self, model):\n",
    "        self.models.append(model)\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for m in self.models:\n",
    "            params.extend(m.parameters())\n",
    "\n",
    "        params.append(self.boost_rate)\n",
    "        return params\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for m in self.models:\n",
    "            m.zero_grad()\n",
    "\n",
    "    def to_cuda(self):\n",
    "        for m in self.models:\n",
    "            m.cuda()\n",
    "\n",
    "    def to_eval(self):\n",
    "        for m in self.models:\n",
    "            m.eval()\n",
    "\n",
    "    def to_train(self):\n",
    "        for m in self.models:\n",
    "            m.train(True)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if len(self.models) == 0:\n",
    "            batch = x.shape[0]\n",
    "            c0 = np.repeat(self.c0.detach().cpu().numpy().reshape(1,-1), batch, axis=0)\n",
    "            return None, torch.Tensor(c0).cuda()\n",
    "        middle_feat_cum = None\n",
    "        prediction = None\n",
    "        with torch.no_grad():\n",
    "            for m in self.models:\n",
    "                if middle_feat_cum is None:\n",
    "                    middle_feat_cum, prediction = m(x, middle_feat_cum)\n",
    "                else:\n",
    "                    middle_feat_cum, pred = m(x, middle_feat_cum)\n",
    "                    prediction += pred\n",
    "        return middle_feat_cum, self.c0 + self.boost_rate * prediction\n",
    "    \n",
    "    def forward_grad(self, x):\n",
    "        if len(self.models) == 0:\n",
    "            batch = x.shape[0]\n",
    "            c0 = np.repeat(self.c0.detach().cpu().numpy().reshape(1, -1), batch, axis=0)\n",
    "            return None, torch.Tensor(c0).cuda()\n",
    "        # at least one model\n",
    "        middle_feat_cum = None\n",
    "        prediction = None\n",
    "        for m in self.models:\n",
    "            if middle_feat_cum is None:\n",
    "                middle_feat_cum, prediction = m(x, middle_feat_cum)\n",
    "            else:\n",
    "                middle_feat_cum, pred = m(x, middle_feat_cum)\n",
    "                prediction += pred\n",
    "        return middle_feat_cum, self.c0 + self.boost_rate * prediction\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, path, builder):\n",
    "        d = torch.load(path)\n",
    "        net = DynamicNet(d['c0'], d['lr'])\n",
    "        net.boost_rate = d['boost_rate']\n",
    "        for stage, m in enumerate(d['models']):\n",
    "            submod = builder(stage)\n",
    "            submod.load_state_dict(m)\n",
    "            net.add(submod)\n",
    "        return net\n",
    "    \n",
    "    def to_file(self, path):\n",
    "        models = [m.state_dict() for m in self.models]\n",
    "        d = {'models': models, 'c0': self.c0, 'lr': self.lr, 'boost_rate': self.boost_rate}\n",
    "        torch.save(d, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025171,
     "end_time": "2020-11-11T23:41:26.540316",
     "exception": false,
     "start_time": "2020-11-11T23:41:26.515145",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:26.599709Z",
     "iopub.status.busy": "2020-11-11T23:41:26.598841Z",
     "iopub.status.idle": "2020-11-11T23:41:26.604988Z",
     "shell.execute_reply": "2020-11-11T23:41:26.604487Z"
    },
    "papermill": {
     "duration": 0.039184,
     "end_time": "2020-11-11T23:41:26.605089",
     "exception": false,
     "start_time": "2020-11-11T23:41:26.565905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:26.660594Z",
     "iopub.status.busy": "2020-11-11T23:41:26.659562Z",
     "iopub.status.idle": "2020-11-11T23:41:26.685836Z",
     "shell.execute_reply": "2020-11-11T23:41:26.685314Z"
    },
    "papermill": {
     "duration": 0.054861,
     "end_time": "2020-11-11T23:41:26.685940",
     "exception": false,
     "start_time": "2020-11-11T23:41:26.631079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP_1HL(nn.Module):\n",
    "    def __init__(self, dim_in, dim_hidden1, dim_hidden2, sparse=False, bn=True):\n",
    "        super(MLP_1HL, self).__init__()\n",
    "#         self.in_layer = nn.Linear(dim_in, dim_hidden1)\n",
    "#         self.out_layer = nn.Linear(dim_hidden1, 206)\n",
    "#         self.lrelu = nn.LeakyReLU(0.1)\n",
    "#         self.relu = nn.ReLU()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                        nn.Dropout(0.2),\n",
    "                        nn.Linear(dim_in, dim_hidden1),\n",
    "                        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "                        nn.ReLU(),\n",
    "#                         nn.BatchNorm1d(dim_hidden1),\n",
    "#                         nn.Dropout(0.4),\n",
    "                        nn.Linear(dim_hidden1, 206))\n",
    "        if bn:\n",
    "            self.bn = nn.BatchNorm1d(dim_hidden1)\n",
    "            self.bn2 = nn.BatchNorm1d(dim_in)\n",
    "\n",
    "    def forward(self, x, lower_f):\n",
    "        if lower_f is not None:\n",
    "            x = torch.cat([x, lower_f], dim=1)\n",
    "            x = self.bn2(x)\n",
    "        out = self.layer1(x)\n",
    "        return out, self.layer2(out)\n",
    "\n",
    "    @classmethod\n",
    "    def get_model(cls, stage, params):\n",
    "        if stage == 0:\n",
    "            dim_in = params[\"feat_d\"]\n",
    "        else:\n",
    "            dim_in = params[\"feat_d\"] + params[\"hidden_size\"]\n",
    "        model = MLP_1HL(dim_in, params[\"hidden_size\"], params[\"hidden_size\"])\n",
    "        return model\n",
    "    \n",
    "class MLP_2HL(nn.Module):\n",
    "    def __init__(self, dim_in, dim_hidden1, dim_hidden2, sparse=False, bn=True):\n",
    "        super(MLP_2HL, self).__init__()\n",
    "        # self.in_layer = SpLinear(dim_in, dim_hidden1) if sparse else nn.Linear(dim_in, dim_hidden1)\n",
    "        # self.dropout_layer = nn.Dropout(0.4)\n",
    "        # self.lrelu = nn.LeakyReLU(0.1)\n",
    "        # self.relu = nn.ReLU()\n",
    "        # self.hidden_layer = nn.Linear(dim_hidden1, dim_hidden2)\n",
    "        # self.out_layer = nn.Linear(dim_hidden2, 206)\n",
    "        # self.bn = nn.BatchNorm1d(dim_hidden1)\n",
    "        self.bn2 = nn.BatchNorm1d(dim_in)\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(dim_in, dim_hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(dim_hidden1),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(dim_hidden1, dim_hidden2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.4),\n",
    "            nn.Linear(dim_hidden2, 206)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, lower_f):\n",
    "        if lower_f is not None:\n",
    "            x = torch.cat([x, lower_f], dim=1)\n",
    "            x = self.bn2(x)\n",
    "        # out = self.lrelu(self.in_layer(x))\n",
    "        # out = self.bn(out)\n",
    "        # out = self.hidden_layer(out)\n",
    "        middle_feat = self.layer1(x)\n",
    "        out = self.layer2(middle_feat)\n",
    "        return middle_feat, out\n",
    "\n",
    "    @classmethod\n",
    "    def get_model(cls, stage, params):\n",
    "        if stage == 0:\n",
    "            dim_in = params[\"feat_d\"]\n",
    "        else:\n",
    "            dim_in = params[\"feat_d\"] + params[\"hidden_size\"]\n",
    "        model = MLP_2HL(dim_in, params[\"hidden_size\"], params[\"hidden_size\"])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:26.748853Z",
     "iopub.status.busy": "2020-11-11T23:41:26.747884Z",
     "iopub.status.idle": "2020-11-11T23:41:26.751084Z",
     "shell.execute_reply": "2020-11-11T23:41:26.750531Z"
    },
    "papermill": {
     "duration": 0.039828,
     "end_time": "2020-11-11T23:41:26.751207",
     "exception": false,
     "start_time": "2020-11-11T23:41:26.711379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:27.193573Z",
     "iopub.status.busy": "2020-11-11T23:41:27.192608Z",
     "iopub.status.idle": "2020-11-11T23:41:27.196887Z",
     "shell.execute_reply": "2020-11-11T23:41:27.195791Z"
    },
    "papermill": {
     "duration": 0.421101,
     "end_time": "2020-11-11T23:41:27.197008",
     "exception": false,
     "start_time": "2020-11-11T23:41:26.775907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_optim(params, lr, weight_decay):\n",
    "    optimizer = optim.Adam(params, lr, weight_decay=weight_decay)\n",
    "    #optimizer = SGD(params, lr, weight_decay=weight_decay)\n",
    "    return optimizer\n",
    "\n",
    "def logloss(net_ensemble, test_loader):\n",
    "    loss = 0\n",
    "    total = 0\n",
    "    loss_f = nn.BCEWithLogitsLoss() # Binary cross entopy loss with logits, reduction=mean by default\n",
    "    for data in test_loader:\n",
    "        x = data[\"x\"].cuda()\n",
    "        y = data[\"y\"].cuda()\n",
    "        # y = (y + 1) / 2\n",
    "        with torch.no_grad():\n",
    "            _, out = net_ensemble.forward(x)\n",
    "        # out = torch.as_tensor(out, dtype=torch.float32).cuda().view(-1, 1)\n",
    "        loss += loss_f(out, y)\n",
    "        total += 1\n",
    "\n",
    "    return loss / total\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:27.257099Z",
     "iopub.status.busy": "2020-11-11T23:41:27.256264Z",
     "iopub.status.idle": "2020-11-11T23:41:27.259889Z",
     "shell.execute_reply": "2020-11-11T23:41:27.259374Z"
    },
    "papermill": {
     "duration": 0.035939,
     "end_time": "2020-11-11T23:41:27.259991",
     "exception": false,
     "start_time": "2020-11-11T23:41:27.224052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "          \"batch_size\": 256,\n",
    "          \"lr\": 1e-3,\n",
    "          \"weight_decay\": 1e-5,\n",
    "          \"n_folds\": 7,\n",
    "          \"early_stopping_steps\": 5,\n",
    "          \"hidden_size\": 512,\n",
    "          \"boost_rate\": 1.0,  # original: 1.0\n",
    "          \"num_nets\": 40,  # Number of weak NNs. original: 40\n",
    "          \"epochs_per_stage\": 1,  # Number of epochs to learn the Kth model. original: 1\n",
    "          \"correct_epoch\": 1,    #  Number of epochs to correct the whole week models original: 1\n",
    "          \"model_order\": \"second\"  # You could put \"first\" according to the original implemention, but error occurs. original: \"second\"\n",
    "          }\n",
    "params[\"feat_d\"] = len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:27.318401Z",
     "iopub.status.busy": "2020-11-11T23:41:27.317429Z",
     "iopub.status.idle": "2020-11-11T23:41:27.355797Z",
     "shell.execute_reply": "2020-11-11T23:41:27.356365Z"
    },
    "papermill": {
     "duration": 0.07184,
     "end_time": "2020-11-11T23:41:27.356532",
     "exception": false,
     "start_time": "2020-11-11T23:41:27.284692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>g-8</th>\n",
       "      <th>g-9</th>\n",
       "      <th>...</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>1.111702</td>\n",
       "      <td>-0.433285</td>\n",
       "      <td>-0.971725</td>\n",
       "      <td>-0.287840</td>\n",
       "      <td>-1.009546</td>\n",
       "      <td>-1.360720</td>\n",
       "      <td>-0.041833</td>\n",
       "      <td>0.718883</td>\n",
       "      <td>-0.299251</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>0.106780</td>\n",
       "      <td>0.257495</td>\n",
       "      <td>0.086766</td>\n",
       "      <td>1.199177</td>\n",
       "      <td>0.692749</td>\n",
       "      <td>0.353080</td>\n",
       "      <td>0.558392</td>\n",
       "      <td>-0.497809</td>\n",
       "      <td>0.847984</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>0.766810</td>\n",
       "      <td>1.412209</td>\n",
       "      <td>-0.124407</td>\n",
       "      <td>-0.028251</td>\n",
       "      <td>1.492135</td>\n",
       "      <td>0.271884</td>\n",
       "      <td>0.359279</td>\n",
       "      <td>0.038358</td>\n",
       "      <td>1.249253</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>-0.755782</td>\n",
       "      <td>-0.455069</td>\n",
       "      <td>0.765733</td>\n",
       "      <td>2.441521</td>\n",
       "      <td>-0.854377</td>\n",
       "      <td>-2.321686</td>\n",
       "      <td>0.301043</td>\n",
       "      <td>-0.151210</td>\n",
       "      <td>-1.361834</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>-0.469617</td>\n",
       "      <td>0.958032</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>1.445311</td>\n",
       "      <td>-0.864632</td>\n",
       "      <td>-0.346314</td>\n",
       "      <td>-0.225963</td>\n",
       "      <td>-1.026217</td>\n",
       "      <td>0.858427</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21943</th>\n",
       "      <td>id_fff8c2444</td>\n",
       "      <td>0.225925</td>\n",
       "      <td>0.202459</td>\n",
       "      <td>-0.361957</td>\n",
       "      <td>-0.364957</td>\n",
       "      <td>0.574001</td>\n",
       "      <td>-0.119274</td>\n",
       "      <td>-0.271844</td>\n",
       "      <td>-0.902164</td>\n",
       "      <td>-0.020179</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>id_fffb1ceed</td>\n",
       "      <td>0.196398</td>\n",
       "      <td>-0.255266</td>\n",
       "      <td>-0.799475</td>\n",
       "      <td>-0.720502</td>\n",
       "      <td>0.922380</td>\n",
       "      <td>0.780550</td>\n",
       "      <td>0.509202</td>\n",
       "      <td>-0.583784</td>\n",
       "      <td>-0.031221</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21945</th>\n",
       "      <td>id_fffb70c0c</td>\n",
       "      <td>-1.951184</td>\n",
       "      <td>-0.605409</td>\n",
       "      <td>1.299466</td>\n",
       "      <td>-1.059551</td>\n",
       "      <td>0.855859</td>\n",
       "      <td>-0.274979</td>\n",
       "      <td>-0.731313</td>\n",
       "      <td>0.670101</td>\n",
       "      <td>0.655237</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21946</th>\n",
       "      <td>id_fffcb9e7c</td>\n",
       "      <td>0.803756</td>\n",
       "      <td>0.416740</td>\n",
       "      <td>0.309263</td>\n",
       "      <td>1.069119</td>\n",
       "      <td>-0.020001</td>\n",
       "      <td>0.083853</td>\n",
       "      <td>0.087779</td>\n",
       "      <td>0.571281</td>\n",
       "      <td>-1.174457</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21947</th>\n",
       "      <td>id_ffffdd77b</td>\n",
       "      <td>-1.271339</td>\n",
       "      <td>-0.287592</td>\n",
       "      <td>1.089933</td>\n",
       "      <td>-0.553025</td>\n",
       "      <td>-2.107117</td>\n",
       "      <td>-1.630423</td>\n",
       "      <td>1.461379</td>\n",
       "      <td>2.256652</td>\n",
       "      <td>1.263243</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21948 rows Ã— 1126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sig_id       g-0       g-2       g-3       g-4       g-5  \\\n",
       "0      id_000644bb2  1.111702 -0.433285 -0.971725 -0.287840 -1.009546   \n",
       "1      id_000779bfc  0.106780  0.257495  0.086766  1.199177  0.692749   \n",
       "2      id_000a6266a  0.766810  1.412209 -0.124407 -0.028251  1.492135   \n",
       "3      id_0015fd391 -0.755782 -0.455069  0.765733  2.441521 -0.854377   \n",
       "4      id_001626bd3 -0.469617  0.958032  0.977273  1.445311 -0.864632   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "21943  id_fff8c2444  0.225925  0.202459 -0.361957 -0.364957  0.574001   \n",
       "21944  id_fffb1ceed  0.196398 -0.255266 -0.799475 -0.720502  0.922380   \n",
       "21945  id_fffb70c0c -1.951184 -0.605409  1.299466 -1.059551  0.855859   \n",
       "21946  id_fffcb9e7c  0.803756  0.416740  0.309263  1.069119 -0.020001   \n",
       "21947  id_ffffdd77b -1.271339 -0.287592  1.089933 -0.553025 -2.107117   \n",
       "\n",
       "            g-6       g-7       g-8       g-9  ...  trpv_agonist  \\\n",
       "0     -1.360720 -0.041833  0.718883 -0.299251  ...             0   \n",
       "1      0.353080  0.558392 -0.497809  0.847984  ...             0   \n",
       "2      0.271884  0.359279  0.038358  1.249253  ...             0   \n",
       "3     -2.321686  0.301043 -0.151210 -1.361834  ...             0   \n",
       "4     -0.346314 -0.225963 -1.026217  0.858427  ...             0   \n",
       "...         ...       ...       ...       ...  ...           ...   \n",
       "21943 -0.119274 -0.271844 -0.902164 -0.020179  ...             0   \n",
       "21944  0.780550  0.509202 -0.583784 -0.031221  ...             0   \n",
       "21945 -0.274979 -0.731313  0.670101  0.655237  ...             0   \n",
       "21946  0.083853  0.087779  0.571281 -1.174457  ...             0   \n",
       "21947 -1.630423  1.461379  2.256652  1.263243  ...             0   \n",
       "\n",
       "       trpv_antagonist  tubulin_inhibitor  tyrosine_kinase_inhibitor  \\\n",
       "0                    0                  0                          0   \n",
       "1                    0                  0                          0   \n",
       "2                    0                  0                          0   \n",
       "3                    0                  0                          0   \n",
       "4                    0                  0                          0   \n",
       "...                ...                ...                        ...   \n",
       "21943                0                  0                          0   \n",
       "21944                0                  0                          0   \n",
       "21945                0                  0                          0   \n",
       "21946                0                  0                          0   \n",
       "21947                0                  0                          0   \n",
       "\n",
       "       ubiquitin_specific_protease_inhibitor  vegfr_inhibitor  vitamin_b  \\\n",
       "0                                          0                0          0   \n",
       "1                                          0                0          0   \n",
       "2                                          0                0          0   \n",
       "3                                          0                0          0   \n",
       "4                                          0                0          0   \n",
       "...                                      ...              ...        ...   \n",
       "21943                                      0                0          0   \n",
       "21944                                      0                0          0   \n",
       "21945                                      0                0          0   \n",
       "21946                                      0                0          0   \n",
       "21947                                      0                0          0   \n",
       "\n",
       "       vitamin_d_receptor_agonist  wnt_inhibitor  fold  \n",
       "0                               0              0     1  \n",
       "1                               0              0     2  \n",
       "2                               0              0     6  \n",
       "3                               0              0     0  \n",
       "4                               0              0     2  \n",
       "...                           ...            ...   ...  \n",
       "21943                           0              0     0  \n",
       "21944                           0              0     5  \n",
       "21945                           0              0     3  \n",
       "21946                           0              0     4  \n",
       "21947                           0              0     0  \n",
       "\n",
       "[21948 rows x 1126 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:27.423027Z",
     "iopub.status.busy": "2020-11-11T23:41:27.422053Z",
     "iopub.status.idle": "2020-11-11T23:41:27.790572Z",
     "shell.execute_reply": "2020-11-11T23:41:27.789820Z"
    },
    "papermill": {
     "duration": 0.404214,
     "end_time": "2020-11-11T23:41:27.790729",
     "exception": false,
     "start_time": "2020-11-11T23:41:27.386515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:27.863537Z",
     "iopub.status.busy": "2020-11-11T23:41:27.858098Z",
     "iopub.status.idle": "2020-11-11T23:41:28.241779Z",
     "shell.execute_reply": "2020-11-11T23:41:28.241149Z"
    },
    "papermill": {
     "duration": 0.420452,
     "end_time": "2020-11-11T23:41:28.241896",
     "exception": false,
     "start_time": "2020-11-11T23:41:27.821444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "c0_ = np.log(np.mean(pd.read_csv(DATA_DIR + 'train_targets_scored.csv').drop(\"sig_id\", axis=1).values, axis=0))\n",
    "def train_fn(seed=0):\n",
    "    oof = np.zeros((len(train), len(target_feats)))\n",
    "    predictions = np.zeros((len(test), len(target_feats)))\n",
    "    \n",
    "    for fold in range(params[\"n_folds\"]):\n",
    "        seed_everything(seed)\n",
    "        \n",
    "        train_idx = train[train[\"fold\"] != fold].index\n",
    "        val_idx = train[train[\"fold\"] == fold].index\n",
    "        \n",
    "        train_df = train[train[\"fold\"] != fold].reset_index(drop=True)\n",
    "        val_df = train[train[\"fold\"] == fold].reset_index(drop=True)\n",
    "        \n",
    "        x_train = train_df[feature_cols].values\n",
    "        y_train = train_df[target_feats].values  #\n",
    "        \n",
    "        x_val = val_df[feature_cols].values  #\n",
    "        y_val = val_df[target_feats].values  #\n",
    "        \n",
    "        train_ds = MoADataset(x_train, y_train)\n",
    "        val_ds = MoADataset(x_val, y_val)\n",
    "        train_loader = torch.utils.data.DataLoader(train_ds, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_ds, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "        \n",
    "        best_score = np.inf\n",
    "        val_score = best_score\n",
    "        best_stage = params[\"num_nets\"] - 1\n",
    "\n",
    "        c0 = torch.tensor(c0_, dtype=torch.float).to(device)\n",
    "        net_ensemble = DynamicNet(c0, params[\"boost_rate\"])\n",
    "        \n",
    "        loss_f1 = nn.MSELoss(reduction='none')\n",
    "#         loss_f2 = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        loss_f2 = SmoothBCEwLogits(smoothing=0.001, reduction=\"none\")\n",
    "        loss_models = torch.zeros((params[\"num_nets\"], 3))\n",
    "\n",
    "        all_ensm_losses = []\n",
    "        all_ensm_losses_te = []\n",
    "        all_mdl_losses = []\n",
    "        dynamic_br = []\n",
    "\n",
    "        lr = params[\"lr\"]\n",
    "        L2 = params[\"weight_decay\"]        \n",
    "        \n",
    "        early_stop = 0\n",
    "        for stage in range(params[\"num_nets\"]):\n",
    "            t0 = time.time()\n",
    "        \n",
    "            model = MLP_2HL.get_model(stage, params)  # Initialize the model_k: f_k(x), multilayer perception v2\n",
    "            model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "            optimizer = get_optim(model.parameters(), lr, L2)\n",
    "            net_ensemble.to_train() # Set the models in ensemble net to train mode\n",
    "            stage_mdlloss = []\n",
    "            for epoch in range(params[\"epochs_per_stage\"]):\n",
    "                for i, data in enumerate(train_loader):\n",
    "                    x = data[\"x\"].to(device)\n",
    "                    y = data[\"y\"].to(device)\n",
    "                    middle_feat, out = net_ensemble.forward(x)\n",
    "                    # out = torch.as_tensor(out, dtype=torch.float32).cuda().view(-1, 1)\n",
    "                    if params[\"model_order\"] == 'first':\n",
    "                        grad_direction = y / (1.0 + torch.exp(y * out))\n",
    "                    else:\n",
    "                        h = 1 / ((1 + torch.exp(y * out)) * (1 + torch.exp(-y * out)))\n",
    "                        grad_direction = y * (1.0 + torch.exp(-y * out))\n",
    "                        # out = torch.as_tensor(out)\n",
    "                        nwtn_weights = (torch.exp(out) + torch.exp(-out)).abs()\n",
    "                    _, out = model(x, middle_feat)\n",
    "                    # out = torch.as_tensor(out, dtype=torch.float32).cuda().view(-1, 1)\n",
    "                    loss = loss_f1(net_ensemble.boost_rate * out, grad_direction)  # T\n",
    "                    loss = loss * h\n",
    "                    loss = loss.mean()\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    stage_mdlloss.append(loss.item()) \n",
    "                    \n",
    "            net_ensemble.add(model)\n",
    "            sml = np.mean(stage_mdlloss)\n",
    "\n",
    "\n",
    "            stage_loss = []\n",
    "            lr_scaler = 2\n",
    "            # fully-corrective step\n",
    "            if stage != 0:\n",
    "                # Adjusting corrective step learning rate \n",
    "                if stage % 3 == 0:\n",
    "                    #lr_scaler *= 2\n",
    "                    lr /= 2\n",
    "                    # L2 /= 2\n",
    "                optimizer = get_optim(net_ensemble.parameters(), lr / lr_scaler, L2)\n",
    "                for _ in range(params[\"correct_epoch\"]):\n",
    "                    for i, data in enumerate(train_loader):\n",
    "                        x = data[\"x\"].to(device)\n",
    "                        y = data[\"y\"].to(device)\n",
    "\n",
    "                        _, out = net_ensemble.forward_grad(x)\n",
    "                        # out = torch.as_tensor(out, dtype=torch.float32).cuda().view(-1, 1)\n",
    "                        # y = (y + 1.0) / 2.0\n",
    "                        loss = loss_f2(out, y).mean() \n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        stage_loss.append(loss.item())\n",
    "                        \n",
    "            sl_te = logloss(net_ensemble, val_loader)  # -----\n",
    "            # Store dynamic boost rate\n",
    "            dynamic_br.append(net_ensemble.boost_rate.item())\n",
    "            # store model\n",
    "            # net_ensemble.to_file(f\"./{fold}FOLD_{seed}_.pth\")\n",
    "            # net_ensemble = DynamicNet.from_file(f\"./{fold}FOLD_{seed}_.pth\", lambda stage: MLP_2HL.get_model(stage, self.params))\n",
    "\n",
    "            elapsed_tr = time.time()-t0\n",
    "            sl = 0\n",
    "            if stage_loss != []:\n",
    "                sl = np.mean(stage_loss)\n",
    "\n",
    "            \n",
    "\n",
    "            all_ensm_losses.append(sl)\n",
    "            all_ensm_losses_te.append(sl_te)\n",
    "            all_mdl_losses.append(sml)\n",
    "            print(f'Stage - {stage}, training time: {elapsed_tr: .1f} sec, boost rate: {net_ensemble.boost_rate: .4f}, Training Loss: {sl: .5f}, Val Loss: {sl_te: .5f}')\n",
    "\n",
    "\n",
    "            net_ensemble.to_cuda()\n",
    "            net_ensemble.to_eval() # Set the models in ensemble net to eval mode\n",
    "            \n",
    "            if sl_te < best_score:\n",
    "                best_score = sl_te\n",
    "                best_stage = stage\n",
    "                net_ensemble.to_file(f\"./{fold}FOLD_{seed}_.pth\")\n",
    "                early_stop = 0\n",
    "            else:\n",
    "                \n",
    "                early_stop += 1\n",
    "                \n",
    "\n",
    "#             test_score = auc_score(net_ensemble, val_loader)\n",
    "#             print(f'Stage: {stage}, AUC@Val: {val_score:.4f}, AUC@Test: {test_score:.4f}')\n",
    "\n",
    "#             loss_models[stage, 1], loss_models[stage, 2] = val_score, test_score\n",
    "            \n",
    "    \n",
    "            if early_stop > params[\"early_stopping_steps\"]:\n",
    "                print(\"early stopped!\")\n",
    "                break\n",
    "                \n",
    "        print(f'Best validation stage: {best_stage}')\n",
    "\n",
    "        net_ensemble = DynamicNet.from_file(f\"./{fold}FOLD_{seed}_.pth\", lambda stage: MLP_2HL.get_model(stage, params))\n",
    "        net_ensemble.to_cuda()\n",
    "        net_ensemble.to_eval()\n",
    "\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                x = data[\"x\"].to(device)\n",
    "                _, pred = net_ensemble.forward(x)\n",
    "                preds.append(pred.sigmoid().detach().cpu().numpy())\n",
    "        oof[val_idx, :] = np.concatenate(preds)\n",
    "\n",
    "        x_test = test[feature_cols].values\n",
    "        test_ds = TestDataset(x_test)\n",
    "        test_loader = torch.utils.data.DataLoader(test_ds, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                x = data[\"x\"].to(device)\n",
    "                _, pred = net_ensemble.forward(x)\n",
    "                preds.append(pred.sigmoid().detach().cpu().numpy())\n",
    "        predictions += np.concatenate(preds) / params[\"n_folds\"]\n",
    "        \n",
    "    oof = np.clip(oof, 1e-3, 1 - 1e-3)\n",
    "    predictions = np.clip(predictions, 1e-3, 1 - 1e-3)\n",
    "    \n",
    "    train[target_feats] = oof\n",
    "    test[target_feats] = predictions\n",
    "\n",
    "    val_results = train_targets_scored.drop(columns=target_feats).merge(train[[\"sig_id\"] + target_feats], on=\"sig_id\", how=\"left\").fillna(0)\n",
    "\n",
    "    y_true = train_targets_scored[target_feats].values\n",
    "    y_pred = val_results[target_feats].values\n",
    "\n",
    "    score = 0\n",
    "    for i in range(len(target_feats)):\n",
    "        score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "        score += score_ / len(target_feats)\n",
    "    print(\"CV log_loss \", score)\n",
    "\n",
    "    sub = pd.read_csv('../input/lish-moa/sample_submission.csv')\n",
    "    sub = sub.drop(columns=target_feats).merge(test[[\"sig_id\"]+target_feats], on=\"sig_id\", how=\"left\").fillna(0)\n",
    "\n",
    "#         sub = sub.drop(columns=self.target_cols).merge(test_[[\"sig_id\"]+self.target_cols+[\"cp_time_24\", \"cp_dose_D2\"]], on=\"sig_id\", how=\"left\").fillna(0)\n",
    "#         sub.loc[:, [\"atp-sensitive_potassium_channel_antagonist\", \"erbb2_inhibitor\"]] = 0.000012\n",
    "#         sub = sub.drop([\"cp_time_24\", \"cp_dose_D2\"], axis=1)\n",
    "    return sub, val_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:41:28.302796Z",
     "iopub.status.busy": "2020-11-11T23:41:28.301857Z",
     "iopub.status.idle": "2020-11-11T23:45:28.939615Z",
     "shell.execute_reply": "2020-11-11T23:45:28.940338Z"
    },
    "papermill": {
     "duration": 240.670313,
     "end_time": "2020-11-11T23:45:28.940525",
     "exception": false,
     "start_time": "2020-11-11T23:41:28.270212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage - 0, training time:  1.7 sec, boost rate:  1.0000, Training Loss:  0.00000, Val Loss:  0.02009\n",
      "Stage - 1, training time:  2.5 sec, boost rate:  1.0207, Training Loss:  0.02144, Val Loss:  0.01787\n",
      "Stage - 2, training time:  2.8 sec, boost rate:  1.0459, Training Loss:  0.02016, Val Loss:  0.01755\n",
      "Stage - 3, training time:  2.7 sec, boost rate:  1.0623, Training Loss:  0.01903, Val Loss:  0.01752\n",
      "Stage - 4, training time:  3.0 sec, boost rate:  1.0782, Training Loss:  0.01839, Val Loss:  0.01752\n",
      "Stage - 5, training time:  3.2 sec, boost rate:  1.0944, Training Loss:  0.01777, Val Loss:  0.01757\n",
      "Stage - 6, training time:  3.7 sec, boost rate:  1.1031, Training Loss:  0.01677, Val Loss:  0.01771\n",
      "Stage - 7, training time:  3.7 sec, boost rate:  1.1116, Training Loss:  0.01629, Val Loss:  0.01782\n",
      "Stage - 8, training time:  3.9 sec, boost rate:  1.1202, Training Loss:  0.01581, Val Loss:  0.01799\n",
      "Stage - 9, training time:  4.3 sec, boost rate:  1.1247, Training Loss:  0.01511, Val Loss:  0.01808\n",
      "early stopped!\n",
      "Best validation stage: 3\n",
      "Stage - 0, training time:  1.1 sec, boost rate:  1.0000, Training Loss:  0.00000, Val Loss:  0.01983\n",
      "Stage - 1, training time:  2.5 sec, boost rate:  1.0237, Training Loss:  0.02148, Val Loss:  0.01757\n",
      "Stage - 2, training time:  2.7 sec, boost rate:  1.0475, Training Loss:  0.02025, Val Loss:  0.01735\n",
      "Stage - 3, training time:  3.8 sec, boost rate:  1.0635, Training Loss:  0.01908, Val Loss:  0.01709\n",
      "Stage - 4, training time:  3.5 sec, boost rate:  1.0796, Training Loss:  0.01847, Val Loss:  0.01720\n",
      "Stage - 5, training time:  3.5 sec, boost rate:  1.0960, Training Loss:  0.01785, Val Loss:  0.01738\n",
      "Stage - 6, training time:  3.9 sec, boost rate:  1.1044, Training Loss:  0.01690, Val Loss:  0.01737\n",
      "Stage - 7, training time:  3.6 sec, boost rate:  1.1130, Training Loss:  0.01639, Val Loss:  0.01746\n",
      "Stage - 8, training time:  3.9 sec, boost rate:  1.1215, Training Loss:  0.01588, Val Loss:  0.01768\n",
      "Stage - 9, training time:  4.3 sec, boost rate:  1.1260, Training Loss:  0.01512, Val Loss:  0.01770\n",
      "early stopped!\n",
      "Best validation stage: 3\n",
      "Stage - 0, training time:  1.1 sec, boost rate:  1.0000, Training Loss:  0.00000, Val Loss:  0.02078\n",
      "Stage - 1, training time:  2.6 sec, boost rate:  1.0213, Training Loss:  0.02133, Val Loss:  0.01852\n",
      "Stage - 2, training time:  2.6 sec, boost rate:  1.0455, Training Loss:  0.02005, Val Loss:  0.01836\n",
      "Stage - 3, training time:  2.8 sec, boost rate:  1.0611, Training Loss:  0.01896, Val Loss:  0.01819\n",
      "Stage - 4, training time:  2.9 sec, boost rate:  1.0760, Training Loss:  0.01834, Val Loss:  0.01831\n",
      "Stage - 5, training time:  3.1 sec, boost rate:  1.0920, Training Loss:  0.01776, Val Loss:  0.01838\n",
      "Stage - 6, training time:  3.7 sec, boost rate:  1.1005, Training Loss:  0.01677, Val Loss:  0.01835\n",
      "Stage - 7, training time:  3.6 sec, boost rate:  1.1091, Training Loss:  0.01625, Val Loss:  0.01851\n",
      "Stage - 8, training time:  3.8 sec, boost rate:  1.1176, Training Loss:  0.01587, Val Loss:  0.01860\n",
      "Stage - 9, training time:  4.3 sec, boost rate:  1.1220, Training Loss:  0.01508, Val Loss:  0.01873\n",
      "early stopped!\n",
      "Best validation stage: 3\n",
      "Stage - 0, training time:  1.1 sec, boost rate:  1.0000, Training Loss:  0.00000, Val Loss:  0.02003\n",
      "Stage - 1, training time:  3.3 sec, boost rate:  1.0178, Training Loss:  0.02144, Val Loss:  0.01785\n",
      "Stage - 2, training time:  2.7 sec, boost rate:  1.0398, Training Loss:  0.02017, Val Loss:  0.01775\n",
      "Stage - 3, training time:  3.1 sec, boost rate:  1.0552, Training Loss:  0.01906, Val Loss:  0.01764\n",
      "Stage - 4, training time:  3.0 sec, boost rate:  1.0711, Training Loss:  0.01843, Val Loss:  0.01781\n",
      "Stage - 5, training time:  3.1 sec, boost rate:  1.0867, Training Loss:  0.01776, Val Loss:  0.01786\n",
      "Stage - 6, training time:  3.6 sec, boost rate:  1.0950, Training Loss:  0.01684, Val Loss:  0.01783\n",
      "Stage - 7, training time:  3.6 sec, boost rate:  1.1036, Training Loss:  0.01636, Val Loss:  0.01800\n",
      "Stage - 8, training time:  3.8 sec, boost rate:  1.1122, Training Loss:  0.01587, Val Loss:  0.01806\n",
      "Stage - 9, training time:  4.3 sec, boost rate:  1.1166, Training Loss:  0.01513, Val Loss:  0.01819\n",
      "early stopped!\n",
      "Best validation stage: 3\n",
      "Stage - 0, training time:  1.1 sec, boost rate:  1.0000, Training Loss:  0.00000, Val Loss:  0.01981\n",
      "Stage - 1, training time:  2.7 sec, boost rate:  1.0250, Training Loss:  0.02150, Val Loss:  0.01743\n",
      "Stage - 2, training time:  2.5 sec, boost rate:  1.0519, Training Loss:  0.02017, Val Loss:  0.01725\n",
      "Stage - 3, training time:  3.1 sec, boost rate:  1.0682, Training Loss:  0.01907, Val Loss:  0.01707\n",
      "Stage - 4, training time:  3.0 sec, boost rate:  1.0838, Training Loss:  0.01846, Val Loss:  0.01715\n",
      "Stage - 5, training time:  3.1 sec, boost rate:  1.0991, Training Loss:  0.01785, Val Loss:  0.01726\n",
      "Stage - 6, training time:  3.4 sec, boost rate:  1.1077, Training Loss:  0.01688, Val Loss:  0.01725\n",
      "Stage - 7, training time:  3.9 sec, boost rate:  1.1162, Training Loss:  0.01636, Val Loss:  0.01731\n",
      "Stage - 8, training time:  3.8 sec, boost rate:  1.1248, Training Loss:  0.01589, Val Loss:  0.01750\n",
      "Stage - 9, training time:  4.5 sec, boost rate:  1.1293, Training Loss:  0.01514, Val Loss:  0.01766\n",
      "early stopped!\n",
      "Best validation stage: 3\n",
      "Stage - 0, training time:  1.2 sec, boost rate:  1.0000, Training Loss:  0.00000, Val Loss:  0.02113\n",
      "Stage - 1, training time:  3.1 sec, boost rate:  1.0205, Training Loss:  0.02125, Val Loss:  0.01866\n",
      "Stage - 2, training time:  2.5 sec, boost rate:  1.0445, Training Loss:  0.02000, Val Loss:  0.01828\n",
      "Stage - 3, training time:  2.9 sec, boost rate:  1.0604, Training Loss:  0.01892, Val Loss:  0.01804\n",
      "Stage - 4, training time:  3.0 sec, boost rate:  1.0763, Training Loss:  0.01831, Val Loss:  0.01808\n",
      "Stage - 5, training time:  3.1 sec, boost rate:  1.0926, Training Loss:  0.01769, Val Loss:  0.01813\n",
      "Stage - 6, training time:  3.4 sec, boost rate:  1.1012, Training Loss:  0.01675, Val Loss:  0.01821\n",
      "Stage - 7, training time:  3.9 sec, boost rate:  1.1097, Training Loss:  0.01628, Val Loss:  0.01831\n",
      "Stage - 8, training time:  3.8 sec, boost rate:  1.1187, Training Loss:  0.01581, Val Loss:  0.01855\n",
      "Stage - 9, training time:  4.1 sec, boost rate:  1.1232, Training Loss:  0.01506, Val Loss:  0.01853\n",
      "early stopped!\n",
      "Best validation stage: 3\n",
      "Stage - 0, training time:  1.1 sec, boost rate:  1.0000, Training Loss:  0.00000, Val Loss:  0.01984\n",
      "Stage - 1, training time:  2.7 sec, boost rate:  1.0248, Training Loss:  0.02145, Val Loss:  0.01772\n",
      "Stage - 2, training time:  2.5 sec, boost rate:  1.0488, Training Loss:  0.02014, Val Loss:  0.01743\n",
      "Stage - 3, training time:  2.7 sec, boost rate:  1.0649, Training Loss:  0.01904, Val Loss:  0.01728\n",
      "Stage - 4, training time:  3.2 sec, boost rate:  1.0810, Training Loss:  0.01844, Val Loss:  0.01724\n",
      "Stage - 5, training time:  3.1 sec, boost rate:  1.0973, Training Loss:  0.01783, Val Loss:  0.01744\n",
      "Stage - 6, training time:  3.3 sec, boost rate:  1.1059, Training Loss:  0.01688, Val Loss:  0.01747\n",
      "Stage - 7, training time:  3.8 sec, boost rate:  1.1144, Training Loss:  0.01638, Val Loss:  0.01758\n",
      "Stage - 8, training time:  4.3 sec, boost rate:  1.1230, Training Loss:  0.01590, Val Loss:  0.01769\n",
      "Stage - 9, training time:  4.6 sec, boost rate:  1.1274, Training Loss:  0.01515, Val Loss:  0.01774\n",
      "Stage - 10, training time:  5.0 sec, boost rate:  1.1318, Training Loss:  0.01483, Val Loss:  0.01795\n",
      "early stopped!\n",
      "Best validation stage: 4\n",
      "CV log_loss  0.016180654380730478\n"
     ]
    }
   ],
   "source": [
    "sub, grow_oof = train_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T23:45:29.055416Z",
     "iopub.status.busy": "2020-11-11T23:45:29.054571Z",
     "iopub.status.idle": "2020-11-11T23:45:30.994309Z",
     "shell.execute_reply": "2020-11-11T23:45:30.990922Z"
    },
    "papermill": {
     "duration": 1.999038,
     "end_time": "2020-11-11T23:45:30.994484",
     "exception": false,
     "start_time": "2020-11-11T23:45:28.995446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 284.963332,
   "end_time": "2020-11-11T23:45:32.468854",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-11T23:40:47.505522",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
