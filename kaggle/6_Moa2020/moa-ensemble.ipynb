{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-10-10T09:50:17.219054Z",
     "iopub.status.busy": "2020-10-10T09:50:17.218345Z",
     "iopub.status.idle": "2020-10-10T09:50:26.786215Z",
     "shell.execute_reply": "2020-10-10T09:50:26.785156Z"
    },
    "papermill": {
     "duration": 9.599051,
     "end_time": "2020-10-10T09:50:26.786348",
     "exception": false,
     "start_time": "2020-10-10T09:50:17.187297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from category_encoders import CountEncoder\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import log_loss, mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "sys.path.append('../input/multilabelstraifier/')\n",
    "from ml_stratifiers import MultilabelStratifiedKFold\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020181,
     "end_time": "2020-10-10T09:50:26.826214",
     "exception": false,
     "start_time": "2020-10-10T09:50:26.806033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocess & Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-10T09:50:26.876670Z",
     "iopub.status.busy": "2020-10-10T09:50:26.875967Z",
     "iopub.status.idle": "2020-10-10T09:50:31.719121Z",
     "shell.execute_reply": "2020-10-10T09:50:31.718126Z"
    },
    "papermill": {
     "duration": 4.87413,
     "end_time": "2020-10-10T09:50:31.719246",
     "exception": false,
     "start_time": "2020-10-10T09:50:26.845116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/kaggle/input/lish-moa/'\n",
    "train = pd.read_csv(DATA_DIR + 'train_features.csv')\n",
    "targets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "#non_targets = pd.read_csv(DATA_DIR + 'train_targets_nonscored.csv')\n",
    "test = pd.read_csv(DATA_DIR + 'test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T09:50:31.765593Z",
     "iopub.status.busy": "2020-10-10T09:50:31.764624Z",
     "iopub.status.idle": "2020-10-10T09:50:31.767764Z",
     "shell.execute_reply": "2020-10-10T09:50:31.767206Z"
    },
    "papermill": {
     "duration": 0.028989,
     "end_time": "2020-10-10T09:50:31.767893",
     "exception": false,
     "start_time": "2020-10-10T09:50:31.738904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_feats = [ i for i in targets.columns if i != \"sig_id\"]\n",
    "g_feats = [i for i in train.columns if \"g-\" in i]\n",
    "c_feats = [i for i in train.columns if \"c-\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T09:50:31.822862Z",
     "iopub.status.busy": "2020-10-10T09:50:31.822217Z",
     "iopub.status.idle": "2020-10-10T09:50:31.910735Z",
     "shell.execute_reply": "2020-10-10T09:50:31.910131Z"
    },
    "papermill": {
     "duration": 0.122692,
     "end_time": "2020-10-10T09:50:31.910863",
     "exception": false,
     "start_time": "2020-10-10T09:50:31.788171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noncons_train_index = train[train.cp_type==\"ctl_vehicle\"].index\n",
    "cons_train_index = train[train.cp_type!=\"ctl_vehicle\"].index\n",
    "noncons_test_index = test[test.cp_type==\"ctl_vehicle\"].index\n",
    "cons_test_index = test[test.cp_type!=\"ctl_vehicle\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T09:50:31.956799Z",
     "iopub.status.busy": "2020-10-10T09:50:31.955851Z",
     "iopub.status.idle": "2020-10-10T09:50:31.971962Z",
     "shell.execute_reply": "2020-10-10T09:50:31.971473Z"
    },
    "papermill": {
     "duration": 0.040983,
     "end_time": "2020-10-10T09:50:31.972064",
     "exception": false,
     "start_time": "2020-10-10T09:50:31.931081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = test[test.index.isin(cons_test_index)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T09:50:32.029840Z",
     "iopub.status.busy": "2020-10-10T09:50:32.027935Z",
     "iopub.status.idle": "2020-10-10T09:50:32.047159Z",
     "shell.execute_reply": "2020-10-10T09:50:32.046696Z"
    },
    "papermill": {
     "duration": 0.052523,
     "end_time": "2020-10-10T09:50:32.047266",
     "exception": false,
     "start_time": "2020-10-10T09:50:31.994743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "categoricals = [\"cp_dose\"]\n",
    "\n",
    "def encoding(tr, te):\n",
    "    for f in categoricals:\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(tr[f]))\n",
    "        tr[f] = lbl.transform(list(tr[f]))\n",
    "        te[f] = lbl.transform(list(te[f])) \n",
    "        \n",
    "    return tr, te\n",
    "\n",
    "train, test = encoding(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T09:50:32.094922Z",
     "iopub.status.busy": "2020-10-10T09:50:32.093658Z",
     "iopub.status.idle": "2020-10-10T09:50:32.230348Z",
     "shell.execute_reply": "2020-10-10T09:50:32.229803Z"
    },
    "papermill": {
     "duration": 0.163107,
     "end_time": "2020-10-10T09:50:32.230449",
     "exception": false,
     "start_time": "2020-10-10T09:50:32.067342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23814, 874) (3624, 874)\n"
     ]
    }
   ],
   "source": [
    "def fe(df, remove_features):\n",
    "    tmp = df.copy()\n",
    "    tmp.drop(remove_features, axis=1, inplace=True)\n",
    "    return tmp\n",
    "\n",
    "remove_features = [\"cp_type\" , \"sig_id\"]\n",
    "        \n",
    "train = fe(train, remove_features)\n",
    "test = fe(test, remove_features)\n",
    "    \n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032582,
     "end_time": "2020-10-10T09:50:32.293756",
     "exception": false,
     "start_time": "2020-10-10T09:50:32.261174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T09:50:32.381057Z",
     "iopub.status.busy": "2020-10-10T09:50:32.380135Z",
     "iopub.status.idle": "2020-10-10T09:50:33.422565Z",
     "shell.execute_reply": "2020-10-10T09:50:33.423185Z"
    },
    "papermill": {
     "duration": 1.089573,
     "end_time": "2020-10-10T09:50:33.423365",
     "exception": false,
     "start_time": "2020-10-10T09:50:32.333792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# xgb --------------------------\n",
    "X = train.copy()\n",
    "y = targets.drop(\"sig_id\", axis=1).copy()\n",
    "X_test = test.copy()\n",
    "\n",
    "# pytorch and logistic regression-----------------------\n",
    "fn_train = train.copy() \n",
    "fn_test = test.copy() \n",
    "fn_targets = targets.drop(\"sig_id\", axis=1).copy()\n",
    "\n",
    "fn_train = fn_train[fn_train.index.isin(cons_train_index)].copy().reset_index(drop=True).to_numpy()\n",
    "fn_targets = fn_targets[fn_targets.index.isin(cons_train_index)].copy().reset_index(drop=True).to_numpy()\n",
    "\n",
    "ss = preprocessing.StandardScaler()\n",
    "fn_train= ss.fit_transform(fn_train)\n",
    "fn_test = ss.transform(fn_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032113,
     "end_time": "2020-10-10T09:50:33.484127",
     "exception": false,
     "start_time": "2020-10-10T09:50:33.452014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1st XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T09:50:33.556315Z",
     "iopub.status.busy": "2020-10-10T09:50:33.555592Z",
     "iopub.status.idle": "2020-10-10T09:50:33.586918Z",
     "shell.execute_reply": "2020-10-10T09:50:33.587392Z"
    },
    "papermill": {
     "duration": 0.071333,
     "end_time": "2020-10-10T09:50:33.587509",
     "exception": false,
     "start_time": "2020-10-10T09:50:33.516176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('classify',\n",
       "                 MultiOutputClassifier(estimator=XGBClassifier(base_score=None,\n",
       "                                                               booster=None,\n",
       "                                                               colsample_bylevel=None,\n",
       "                                                               colsample_bynode=None,\n",
       "                                                               colsample_bytree=None,\n",
       "                                                               gamma=3.6975,\n",
       "                                                               gpu_id=None,\n",
       "                                                               importance_type='gain',\n",
       "                                                               interaction_constraints=None,\n",
       "                                                               learning_rate=0.0703,\n",
       "                                                               max_delta_step=2.0706,\n",
       "                                                               max_depth=10,\n",
       "                                                               min_child_weight=31.58,\n",
       "                                                               missing=nan,\n",
       "                                                               monotone_constraints=None,\n",
       "                                                               n_estimators=166,\n",
       "                                                               n_jobs=None,\n",
       "                                                               num_parallel_tree=None,\n",
       "                                                               random_state=None,\n",
       "                                                               reg_alpha=None,\n",
       "                                                               reg_lambda=None,\n",
       "                                                               scale_pos_weight=None,\n",
       "                                                               subsample=None,\n",
       "                                                               tree_method='gpu_hist',\n",
       "                                                               validate_parameters=None,\n",
       "                                                               verbosity=None)))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultiOutputClassifier(XGBClassifier(tree_method='gpu_hist'))\n",
    "\n",
    "clf = Pipeline([('classify', classifier)\n",
    "               ])\n",
    "\n",
    "params = {'classify__estimator__gamma': 3.6975,\n",
    "          'classify__estimator__learning_rate': 0.0703,\n",
    "          'classify__estimator__max_delta_step': 2.0706,\n",
    "          'classify__estimator__max_depth': 10,\n",
    "          'classify__estimator__min_child_weight': 31.5800,\n",
    "          'classify__estimator__n_estimators': 166,\n",
    "         }\n",
    "\n",
    "clf.set_params(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T09:50:33.646611Z",
     "iopub.status.busy": "2020-10-10T09:50:33.645708Z",
     "iopub.status.idle": "2020-10-10T09:50:33.648658Z",
     "shell.execute_reply": "2020-10-10T09:50:33.648089Z"
    },
    "papermill": {
     "duration": 0.039208,
     "end_time": "2020-10-10T09:50:33.648768",
     "exception": false,
     "start_time": "2020-10-10T09:50:33.609560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modelling_xgb(X, y, X_test, seed):\n",
    "    NFOLDS=5\n",
    "    oof_preds = np.zeros(y.shape)\n",
    "    test_preds = np.zeros((X_test.shape[0], y.shape[1]))\n",
    "    oof_losses = []\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=NFOLDS, random_state=seed, shuffle=True)\n",
    "    for fn, (trn_idx, val_idx) in enumerate(mskf.split(X, y)):\n",
    "        print('Starting fold: ', fn)\n",
    "        X_train, X_val = X.iloc[trn_idx,:], X.iloc[val_idx,:].to_numpy()\n",
    "        y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx].to_numpy()\n",
    "    \n",
    "        X_train = X_train[X_train.index.isin(cons_train_index)].to_numpy()\n",
    "        y_train = y_train[y_train.index.isin(cons_train_index)].to_numpy()\n",
    "    \n",
    "        clf.fit(X_train, y_train)\n",
    "        val_preds = clf.predict_proba(X_val) # list of preds per class\n",
    "        val_preds = np.array(val_preds)[:,:,1].T # take the positive class\n",
    "        oof_preds[val_idx] = val_preds\n",
    "    \n",
    "        loss = log_loss(np.ravel(y_val), np.ravel(val_preds))\n",
    "        print(loss)\n",
    "        oof_losses.append(loss)\n",
    "        preds = clf.predict_proba(X_test)\n",
    "        preds = np.array(preds)[:,:,1].T # take the positive class\n",
    "        test_preds += preds / NFOLDS\n",
    "    \n",
    "    print(oof_losses)\n",
    "    print('Mean OOF loss across folds', np.mean(oof_losses))\n",
    "    print('STD OOF loss across folds', np.std(oof_losses))\n",
    "    return oof_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T09:50:33.701489Z",
     "iopub.status.busy": "2020-10-10T09:50:33.700027Z",
     "iopub.status.idle": "2020-10-10T10:56:55.030969Z",
     "shell.execute_reply": "2020-10-10T10:56:55.031564Z"
    },
    "papermill": {
     "duration": 3981.360966,
     "end_time": "2020-10-10T10:56:55.031793",
     "exception": false,
     "start_time": "2020-10-10T09:50:33.670827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold:  0\n",
      "0.016738832513000318\n",
      "Starting fold:  1\n",
      "0.016908957230028557\n",
      "Starting fold:  2\n",
      "0.016773734659711288\n",
      "Starting fold:  3\n",
      "0.016497753560878757\n",
      "Starting fold:  4\n",
      "0.01667905765085504\n",
      "[0.016738832513000318, 0.016908957230028557, 0.016773734659711288, 0.016497753560878757, 0.01667905765085504]\n",
      "Mean OOF loss across folds 0.01671966712289479\n",
      "STD OOF loss across folds 0.00013417608848077545\n",
      "Starting fold:  0\n",
      "0.01668812742202729\n",
      "Starting fold:  1\n",
      "0.01672661540135191\n",
      "Starting fold:  2\n",
      "0.01671405352510354\n",
      "Starting fold:  3\n",
      "0.016696373422081447\n",
      "Starting fold:  4\n",
      "0.016772998153547904\n",
      "[0.01668812742202729, 0.01672661540135191, 0.01671405352510354, 0.016696373422081447, 0.016772998153547904]\n",
      "Mean OOF loss across folds 0.016719633584822417\n",
      "STD OOF loss across folds 2.9870833562472123e-05\n",
      "Starting fold:  0\n",
      "0.016696372969820933\n",
      "Starting fold:  1\n",
      "0.01672151248200614\n",
      "Starting fold:  2\n",
      "0.016674213856001072\n",
      "Starting fold:  3\n",
      "0.016673970749857346\n",
      "Starting fold:  4\n",
      "0.016831564237686652\n",
      "[0.016696372969820933, 0.01672151248200614, 0.016674213856001072, 0.016673970749857346, 0.016831564237686652]\n",
      "Mean OOF loss across folds 0.016719526859074428\n",
      "STD OOF loss across folds 5.868738676549415e-05\n"
     ]
    }
   ],
   "source": [
    "seeds = [42,43,44]\n",
    "xgb1_oof = np.zeros(y.shape)\n",
    "xgb1_test = np.zeros((test.shape[0], y.shape[1]))\n",
    "for seed_ in seeds:\n",
    "    ind_preds, ind_test_preds = modelling_xgb(X, y, X_test, seed_)\n",
    "    xgb1_oof += ind_preds / len(seeds)\n",
    "    xgb1_test += ind_test_preds / len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T10:56:55.204600Z",
     "iopub.status.busy": "2020-10-10T10:56:55.203031Z",
     "iopub.status.idle": "2020-10-10T10:56:58.902782Z",
     "shell.execute_reply": "2020-10-10T10:56:58.903212Z"
    },
    "papermill": {
     "duration": 3.784788,
     "end_time": "2020-10-10T10:56:58.903357",
     "exception": false,
     "start_time": "2020-10-10T10:56:55.118569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.016352895291178133\n"
     ]
    }
   ],
   "source": [
    "check_xgb1 = targets.copy()\n",
    "check_xgb1.iloc[:,1:] = xgb1_oof\n",
    "check_xgb1.loc[check_xgb1.index.isin(noncons_train_index),target_feats] = 0\n",
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(np.array(check_xgb1.iloc[:,1:]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.063055,
     "end_time": "2020-10-10T10:56:59.030346",
     "exception": false,
     "start_time": "2020-10-10T10:56:58.967291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1st NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T10:56:59.183059Z",
     "iopub.status.busy": "2020-10-10T10:56:59.171188Z",
     "iopub.status.idle": "2020-10-10T10:56:59.224305Z",
     "shell.execute_reply": "2020-10-10T10:56:59.224808Z"
    },
    "papermill": {
     "duration": 0.131054,
     "end_time": "2020-10-10T10:56:59.224950",
     "exception": false,
     "start_time": "2020-10-10T10:56:59.093896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "batch_size = 128\n",
    "train_epochs = 40\n",
    "n_folds=5\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "\n",
    "def mean_log_loss(y_true, y_pred):\n",
    "    metrics = []\n",
    "    for i, target in enumerate(target_feats):\n",
    "        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n",
    "    return np.mean(metrics)\n",
    "\n",
    "def seed_everything(seed=1234): \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "class MoaModel(nn.Module):\n",
    "    def __init__(self, num_columns, last_columns_num):\n",
    "        super(MoaModel, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_columns)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_columns, 2048))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(2048)\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(2048, 1048))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(1048)\n",
    "        self.dropout3 = nn.Dropout(0.6)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(1048, last_columns_num))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def modelling_torch(tr, target, te, sample_seed, init_num, last_num, layer):\n",
    "    seed_everything(seed=sample_seed) \n",
    "    X_train = tr.copy()\n",
    "    y_train = target.copy()\n",
    "    X_test = te.copy()\n",
    "    test_len = X_test.shape[0]\n",
    "\n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=2)\n",
    "    models = []\n",
    "    \n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    X_test = torch.utils.data.TensorDataset(X_test) \n",
    "    test_loader = torch.utils.data.DataLoader(X_test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    pred_value = np.zeros([test_len, y_train.shape[1]])\n",
    "    scores = []\n",
    "    for fold, (train_index, valid_index) in enumerate(mskf.split(X_train, y_train)):\n",
    "        print(\"Fold \"+str(fold+1))\n",
    "        X_train2 = torch.tensor(X_train[train_index,:], dtype=torch.float32)\n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "\n",
    "        X_valid2 = torch.tensor(X_train[valid_index,:], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "            \n",
    "        clf = MoaModel(init_num, last_num)\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "        optimizer = optim.Adam(clf.parameters(), lr = 0.001, weight_decay=1e-5) \n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, eps=1e-4, verbose=True)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2, y_valid2)\n",
    "        \n",
    "        clf.to(device)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        stop_counts = 0\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            clf.train()\n",
    "            avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader) \n",
    "    \n",
    "            clf.eval()\n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        \n",
    "            elapsed_time = time.time() - start_time \n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                stop_counts = 0\n",
    "                best_val_loss = avg_val_loss\n",
    "                print('Best model: Epoch {} \\t loss={:.6f} \\t val_loss={:.6f} \\t time={:.2f}s'.format(\n",
    "                    epoch + 1, avg_loss, avg_val_loss, elapsed_time))\n",
    "                torch.save(clf.state_dict(), 'best-model-parameters.pt')\n",
    "            else:\n",
    "                stop_counts += 1\n",
    "         \n",
    "        pred_model = MoaModel(init_num, last_num)\n",
    "        pred_model.load_state_dict(torch.load('best-model-parameters.pt'))\n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                y_pred = pred_model(x_batch).sigmoid().detach()\n",
    "                oof_epoch[i * batch_size:(i+1) * batch_size,:] = y_pred.cpu().numpy()\n",
    "                target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        print(\"Fold {} log loss: {}\".format(fold+1, mean_log_loss(target_epoch, oof_epoch)))\n",
    "        scores.append(mean_log_loss(target_epoch, oof_epoch))\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "        # test predcition --------------\n",
    "        test_preds = np.zeros([test_len, y_train.shape[1]])\n",
    "        for i, (x_batch,) in enumerate(test_loader): \n",
    "            y_pred = pred_model(x_batch).sigmoid().detach()\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred.cpu().numpy()\n",
    "        pred_value += test_preds / n_folds\n",
    "        # ------------------------------\n",
    "        \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    for i, ele in enumerate(scores):\n",
    "        print(\"Fold {} log loss: {}\".format(i+1, scores[i]))\n",
    "    print(\"Std of log loss: {}\".format(np.std(scores)))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "    \n",
    "    return oof, oof_targets, pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T10:56:59.363762Z",
     "iopub.status.busy": "2020-10-10T10:56:59.362259Z",
     "iopub.status.idle": "2020-10-10T11:12:27.325870Z",
     "shell.execute_reply": "2020-10-10T11:12:27.325251Z"
    },
    "papermill": {
     "duration": 928.036263,
     "end_time": "2020-10-10T11:12:27.326022",
     "exception": false,
     "start_time": "2020-10-10T10:56:59.289759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.412591 \t val_loss=0.077221 \t time=1.70s\n",
      "Best model: Epoch 2 \t loss=0.048580 \t val_loss=0.028151 \t time=0.79s\n",
      "Best model: Epoch 3 \t loss=0.027003 \t val_loss=0.022851 \t time=0.81s\n",
      "Best model: Epoch 4 \t loss=0.023460 \t val_loss=0.021035 \t time=0.76s\n",
      "Best model: Epoch 5 \t loss=0.021372 \t val_loss=0.019930 \t time=0.76s\n",
      "Best model: Epoch 6 \t loss=0.020710 \t val_loss=0.019312 \t time=0.76s\n",
      "Best model: Epoch 7 \t loss=0.020091 \t val_loss=0.019111 \t time=0.75s\n",
      "Best model: Epoch 8 \t loss=0.019413 \t val_loss=0.018614 \t time=0.76s\n",
      "Best model: Epoch 9 \t loss=0.018994 \t val_loss=0.018114 \t time=0.95s\n",
      "Best model: Epoch 10 \t loss=0.018733 \t val_loss=0.017949 \t time=0.76s\n",
      "Best model: Epoch 11 \t loss=0.018251 \t val_loss=0.017597 \t time=0.77s\n",
      "Best model: Epoch 13 \t loss=0.017651 \t val_loss=0.017239 \t time=0.74s\n",
      "Best model: Epoch 14 \t loss=0.017350 \t val_loss=0.017176 \t time=0.75s\n",
      "Best model: Epoch 15 \t loss=0.017291 \t val_loss=0.017116 \t time=0.76s\n",
      "Best model: Epoch 16 \t loss=0.017107 \t val_loss=0.016993 \t time=0.75s\n",
      "Best model: Epoch 17 \t loss=0.016768 \t val_loss=0.016788 \t time=0.76s\n",
      "Best model: Epoch 18 \t loss=0.016622 \t val_loss=0.016715 \t time=0.75s\n",
      "Best model: Epoch 21 \t loss=0.016251 \t val_loss=0.016576 \t time=0.75s\n",
      "Best model: Epoch 22 \t loss=0.016066 \t val_loss=0.016500 \t time=0.87s\n",
      "Best model: Epoch 24 \t loss=0.015943 \t val_loss=0.016470 \t time=0.77s\n",
      "Best model: Epoch 25 \t loss=0.015734 \t val_loss=0.016424 \t time=0.77s\n",
      "Best model: Epoch 28 \t loss=0.015489 \t val_loss=0.016377 \t time=0.76s\n",
      "Best model: Epoch 31 \t loss=0.015327 \t val_loss=0.016313 \t time=1.07s\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 36 \t loss=0.014506 \t val_loss=0.016155 \t time=1.01s\n",
      "Best model: Epoch 37 \t loss=0.014171 \t val_loss=0.016128 \t time=0.75s\n",
      "Best model: Epoch 38 \t loss=0.014019 \t val_loss=0.016098 \t time=0.76s\n",
      "Best model: Epoch 39 \t loss=0.013838 \t val_loss=0.016042 \t time=0.76s\n",
      "Fold 1 log loss: 0.016144690589180097\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.414173 \t val_loss=0.076114 \t time=0.80s\n",
      "Best model: Epoch 2 \t loss=0.048296 \t val_loss=0.029272 \t time=0.75s\n",
      "Best model: Epoch 3 \t loss=0.027405 \t val_loss=0.022898 \t time=0.83s\n",
      "Best model: Epoch 4 \t loss=0.023185 \t val_loss=0.021045 \t time=0.90s\n",
      "Best model: Epoch 5 \t loss=0.021348 \t val_loss=0.019655 \t time=0.81s\n",
      "Best model: Epoch 6 \t loss=0.020497 \t val_loss=0.019208 \t time=1.00s\n",
      "Best model: Epoch 7 \t loss=0.019868 \t val_loss=0.018997 \t time=0.81s\n",
      "Best model: Epoch 8 \t loss=0.019279 \t val_loss=0.018520 \t time=0.77s\n",
      "Best model: Epoch 9 \t loss=0.018965 \t val_loss=0.017982 \t time=0.75s\n",
      "Best model: Epoch 10 \t loss=0.018675 \t val_loss=0.017759 \t time=0.84s\n",
      "Best model: Epoch 11 \t loss=0.018200 \t val_loss=0.017652 \t time=0.76s\n",
      "Best model: Epoch 12 \t loss=0.017911 \t val_loss=0.017377 \t time=0.76s\n",
      "Best model: Epoch 13 \t loss=0.017592 \t val_loss=0.017221 \t time=0.75s\n",
      "Best model: Epoch 15 \t loss=0.017180 \t val_loss=0.017090 \t time=0.78s\n",
      "Best model: Epoch 16 \t loss=0.017055 \t val_loss=0.016951 \t time=0.76s\n",
      "Best model: Epoch 17 \t loss=0.016813 \t val_loss=0.016760 \t time=0.79s\n",
      "Best model: Epoch 18 \t loss=0.016564 \t val_loss=0.016719 \t time=0.76s\n",
      "Best model: Epoch 19 \t loss=0.016340 \t val_loss=0.016593 \t time=0.96s\n",
      "Best model: Epoch 20 \t loss=0.016182 \t val_loss=0.016538 \t time=0.76s\n",
      "Best model: Epoch 21 \t loss=0.016051 \t val_loss=0.016499 \t time=0.76s\n",
      "Best model: Epoch 22 \t loss=0.016053 \t val_loss=0.016487 \t time=0.79s\n",
      "Best model: Epoch 24 \t loss=0.015813 \t val_loss=0.016389 \t time=0.75s\n",
      "Best model: Epoch 25 \t loss=0.015707 \t val_loss=0.016370 \t time=0.75s\n",
      "Best model: Epoch 26 \t loss=0.015672 \t val_loss=0.016358 \t time=0.75s\n",
      "Best model: Epoch 27 \t loss=0.015543 \t val_loss=0.016294 \t time=0.99s\n",
      "Best model: Epoch 28 \t loss=0.015430 \t val_loss=0.016279 \t time=0.86s\n",
      "Best model: Epoch 29 \t loss=0.015341 \t val_loss=0.016245 \t time=0.78s\n",
      "Best model: Epoch 30 \t loss=0.015306 \t val_loss=0.016228 \t time=0.75s\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 35 \t loss=0.014418 \t val_loss=0.016175 \t time=0.75s\n",
      "Best model: Epoch 36 \t loss=0.014187 \t val_loss=0.016067 \t time=0.75s\n",
      "Best model: Epoch 37 \t loss=0.013917 \t val_loss=0.016034 \t time=0.75s\n",
      "Best model: Epoch 38 \t loss=0.013762 \t val_loss=0.016030 \t time=0.76s\n",
      "Best model: Epoch 39 \t loss=0.013603 \t val_loss=0.015983 \t time=0.75s\n",
      "Fold 2 log loss: 0.016044396281569673\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.413986 \t val_loss=0.076896 \t time=0.75s\n",
      "Best model: Epoch 2 \t loss=0.049192 \t val_loss=0.028994 \t time=0.76s\n",
      "Best model: Epoch 3 \t loss=0.027106 \t val_loss=0.022515 \t time=0.95s\n",
      "Best model: Epoch 4 \t loss=0.023059 \t val_loss=0.020760 \t time=0.75s\n",
      "Best model: Epoch 5 \t loss=0.021690 \t val_loss=0.020538 \t time=0.76s\n",
      "Best model: Epoch 6 \t loss=0.020625 \t val_loss=0.019409 \t time=0.77s\n",
      "Best model: Epoch 7 \t loss=0.019750 \t val_loss=0.018819 \t time=0.75s\n",
      "Best model: Epoch 8 \t loss=0.019329 \t val_loss=0.018315 \t time=0.80s\n",
      "Best model: Epoch 9 \t loss=0.019057 \t val_loss=0.018125 \t time=0.77s\n",
      "Best model: Epoch 10 \t loss=0.018609 \t val_loss=0.017987 \t time=0.77s\n",
      "Best model: Epoch 11 \t loss=0.018311 \t val_loss=0.017812 \t time=0.77s\n",
      "Best model: Epoch 12 \t loss=0.018039 \t val_loss=0.017600 \t time=0.76s\n",
      "Best model: Epoch 13 \t loss=0.017822 \t val_loss=0.017359 \t time=0.82s\n",
      "Best model: Epoch 14 \t loss=0.017500 \t val_loss=0.017197 \t time=0.77s\n",
      "Best model: Epoch 15 \t loss=0.017385 \t val_loss=0.017085 \t time=0.82s\n",
      "Best model: Epoch 16 \t loss=0.017066 \t val_loss=0.016969 \t time=0.88s\n",
      "Best model: Epoch 17 \t loss=0.016860 \t val_loss=0.016758 \t time=0.87s\n",
      "Best model: Epoch 18 \t loss=0.016605 \t val_loss=0.016644 \t time=0.77s\n",
      "Best model: Epoch 19 \t loss=0.016521 \t val_loss=0.016627 \t time=0.76s\n",
      "Best model: Epoch 21 \t loss=0.016267 \t val_loss=0.016498 \t time=0.78s\n",
      "Best model: Epoch 23 \t loss=0.015995 \t val_loss=0.016465 \t time=0.77s\n",
      "Best model: Epoch 24 \t loss=0.015823 \t val_loss=0.016417 \t time=0.84s\n",
      "Best model: Epoch 25 \t loss=0.015703 \t val_loss=0.016356 \t time=1.01s\n",
      "Best model: Epoch 26 \t loss=0.015659 \t val_loss=0.016288 \t time=0.77s\n",
      "Best model: Epoch 30 \t loss=0.015393 \t val_loss=0.016223 \t time=0.79s\n",
      "Best model: Epoch 33 \t loss=0.015215 \t val_loss=0.016222 \t time=0.76s\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 35 \t loss=0.014525 \t val_loss=0.016054 \t time=0.77s\n",
      "Best model: Epoch 36 \t loss=0.014278 \t val_loss=0.015991 \t time=0.79s\n",
      "Best model: Epoch 37 \t loss=0.014126 \t val_loss=0.015968 \t time=0.76s\n",
      "Best model: Epoch 38 \t loss=0.013885 \t val_loss=0.015964 \t time=0.80s\n",
      "Best model: Epoch 39 \t loss=0.013728 \t val_loss=0.015933 \t time=0.82s\n",
      "Fold 3 log loss: 0.015932693904937074\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.414550 \t val_loss=0.077203 \t time=0.81s\n",
      "Best model: Epoch 2 \t loss=0.050157 \t val_loss=0.030820 \t time=0.80s\n",
      "Best model: Epoch 3 \t loss=0.028654 \t val_loss=0.022643 \t time=0.79s\n",
      "Best model: Epoch 4 \t loss=0.022900 \t val_loss=0.020759 \t time=0.76s\n",
      "Best model: Epoch 5 \t loss=0.021488 \t val_loss=0.019617 \t time=0.76s\n",
      "Best model: Epoch 6 \t loss=0.020599 \t val_loss=0.019256 \t time=0.81s\n",
      "Best model: Epoch 7 \t loss=0.020185 \t val_loss=0.018790 \t time=0.75s\n",
      "Best model: Epoch 8 \t loss=0.019538 \t val_loss=0.018186 \t time=0.77s\n",
      "Best model: Epoch 9 \t loss=0.019437 \t val_loss=0.017978 \t time=0.77s\n",
      "Best model: Epoch 10 \t loss=0.018881 \t val_loss=0.017777 \t time=0.77s\n",
      "Best model: Epoch 11 \t loss=0.018440 \t val_loss=0.017634 \t time=0.75s\n",
      "Best model: Epoch 12 \t loss=0.018149 \t val_loss=0.017331 \t time=0.78s\n",
      "Best model: Epoch 13 \t loss=0.017822 \t val_loss=0.017247 \t time=0.92s\n",
      "Best model: Epoch 14 \t loss=0.017593 \t val_loss=0.017178 \t time=0.78s\n",
      "Best model: Epoch 15 \t loss=0.017360 \t val_loss=0.017018 \t time=0.83s\n",
      "Best model: Epoch 16 \t loss=0.017128 \t val_loss=0.016771 \t time=0.76s\n",
      "Best model: Epoch 18 \t loss=0.016888 \t val_loss=0.016649 \t time=0.75s\n",
      "Best model: Epoch 19 \t loss=0.016572 \t val_loss=0.016569 \t time=0.75s\n",
      "Best model: Epoch 20 \t loss=0.016374 \t val_loss=0.016445 \t time=1.02s\n",
      "Best model: Epoch 22 \t loss=0.016243 \t val_loss=0.016368 \t time=0.75s\n",
      "Best model: Epoch 23 \t loss=0.016123 \t val_loss=0.016358 \t time=0.75s\n",
      "Best model: Epoch 25 \t loss=0.015849 \t val_loss=0.016256 \t time=0.75s\n",
      "Best model: Epoch 27 \t loss=0.015719 \t val_loss=0.016254 \t time=0.78s\n",
      "Best model: Epoch 28 \t loss=0.015601 \t val_loss=0.016216 \t time=0.75s\n",
      "Best model: Epoch 29 \t loss=0.015491 \t val_loss=0.016183 \t time=0.75s\n",
      "Best model: Epoch 30 \t loss=0.015505 \t val_loss=0.016177 \t time=0.75s\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 35 \t loss=0.014550 \t val_loss=0.016025 \t time=0.75s\n",
      "Best model: Epoch 36 \t loss=0.014275 \t val_loss=0.015951 \t time=0.76s\n",
      "Best model: Epoch 37 \t loss=0.014051 \t val_loss=0.015898 \t time=0.75s\n",
      "Best model: Epoch 39 \t loss=0.013728 \t val_loss=0.015897 \t time=0.76s\n",
      "Best model: Epoch 40 \t loss=0.013633 \t val_loss=0.015858 \t time=0.93s\n",
      "Fold 4 log loss: 0.01591978898403236\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.412187 \t val_loss=0.088095 \t time=0.77s\n",
      "Best model: Epoch 2 \t loss=0.049344 \t val_loss=0.028300 \t time=0.75s\n",
      "Best model: Epoch 3 \t loss=0.027639 \t val_loss=0.022942 \t time=0.82s\n",
      "Best model: Epoch 4 \t loss=0.023355 \t val_loss=0.020933 \t time=0.78s\n",
      "Best model: Epoch 5 \t loss=0.021415 \t val_loss=0.019995 \t time=0.77s\n",
      "Best model: Epoch 6 \t loss=0.020525 \t val_loss=0.019669 \t time=0.77s\n",
      "Best model: Epoch 7 \t loss=0.019796 \t val_loss=0.018820 \t time=0.78s\n",
      "Best model: Epoch 8 \t loss=0.019289 \t val_loss=0.018472 \t time=0.82s\n",
      "Best model: Epoch 9 \t loss=0.019185 \t val_loss=0.018166 \t time=0.81s\n",
      "Best model: Epoch 11 \t loss=0.018588 \t val_loss=0.017788 \t time=0.78s\n",
      "Best model: Epoch 12 \t loss=0.018107 \t val_loss=0.017631 \t time=0.78s\n",
      "Best model: Epoch 13 \t loss=0.017794 \t val_loss=0.017333 \t time=0.80s\n",
      "Best model: Epoch 14 \t loss=0.017391 \t val_loss=0.017204 \t time=1.17s\n",
      "Best model: Epoch 15 \t loss=0.017268 \t val_loss=0.017087 \t time=0.86s\n",
      "Best model: Epoch 16 \t loss=0.017022 \t val_loss=0.016977 \t time=0.75s\n",
      "Best model: Epoch 17 \t loss=0.016798 \t val_loss=0.016941 \t time=1.09s\n",
      "Best model: Epoch 18 \t loss=0.016580 \t val_loss=0.016778 \t time=0.79s\n",
      "Best model: Epoch 19 \t loss=0.016403 \t val_loss=0.016737 \t time=0.83s\n",
      "Best model: Epoch 21 \t loss=0.016202 \t val_loss=0.016591 \t time=0.81s\n",
      "Best model: Epoch 22 \t loss=0.016031 \t val_loss=0.016581 \t time=0.94s\n",
      "Best model: Epoch 23 \t loss=0.015834 \t val_loss=0.016542 \t time=0.78s\n",
      "Best model: Epoch 24 \t loss=0.015865 \t val_loss=0.016475 \t time=0.80s\n",
      "Best model: Epoch 27 \t loss=0.015609 \t val_loss=0.016414 \t time=0.77s\n",
      "Best model: Epoch 30 \t loss=0.015342 \t val_loss=0.016413 \t time=0.76s\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 32 \t loss=0.014680 \t val_loss=0.016273 \t time=0.79s\n",
      "Best model: Epoch 33 \t loss=0.014344 \t val_loss=0.016219 \t time=0.76s\n",
      "Best model: Epoch 34 \t loss=0.014130 \t val_loss=0.016189 \t time=0.76s\n",
      "Best model: Epoch 35 \t loss=0.014023 \t val_loss=0.016185 \t time=0.99s\n",
      "Best model: Epoch 36 \t loss=0.013851 \t val_loss=0.016143 \t time=0.83s\n",
      "Best model: Epoch 37 \t loss=0.013768 \t val_loss=0.016134 \t time=0.76s\n",
      "Best model: Epoch 40 \t loss=0.013329 \t val_loss=0.016125 \t time=0.74s\n",
      "Fold 5 log loss: 0.016124345038051664\n",
      "Seed 0\n",
      "Fold 1 log loss: 0.016144690589180097\n",
      "Fold 2 log loss: 0.016044396281569673\n",
      "Fold 3 log loss: 0.015932693904937074\n",
      "Fold 4 log loss: 0.01591978898403236\n",
      "Fold 5 log loss: 0.016124345038051664\n",
      "Std of log loss: 9.362313645021518e-05\n",
      "Total log loss: 0.016033178295101806\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.412054 \t val_loss=0.076325 \t time=0.76s\n",
      "Best model: Epoch 2 \t loss=0.048526 \t val_loss=0.028851 \t time=0.95s\n",
      "Best model: Epoch 3 \t loss=0.026873 \t val_loss=0.023145 \t time=0.84s\n",
      "Best model: Epoch 4 \t loss=0.023518 \t val_loss=0.020954 \t time=0.78s\n",
      "Best model: Epoch 5 \t loss=0.021069 \t val_loss=0.019802 \t time=0.79s\n",
      "Best model: Epoch 6 \t loss=0.020366 \t val_loss=0.019167 \t time=0.80s\n",
      "Best model: Epoch 7 \t loss=0.019715 \t val_loss=0.018802 \t time=0.80s\n",
      "Best model: Epoch 8 \t loss=0.019131 \t val_loss=0.018499 \t time=0.78s\n",
      "Best model: Epoch 9 \t loss=0.018863 \t val_loss=0.018232 \t time=0.95s\n",
      "Best model: Epoch 10 \t loss=0.018576 \t val_loss=0.017987 \t time=0.86s\n",
      "Best model: Epoch 11 \t loss=0.018388 \t val_loss=0.017902 \t time=0.76s\n",
      "Best model: Epoch 12 \t loss=0.018011 \t val_loss=0.017349 \t time=0.75s\n",
      "Best model: Epoch 13 \t loss=0.017622 \t val_loss=0.017302 \t time=0.75s\n",
      "Best model: Epoch 14 \t loss=0.017534 \t val_loss=0.017155 \t time=0.80s\n",
      "Best model: Epoch 16 \t loss=0.017059 \t val_loss=0.016986 \t time=0.75s\n",
      "Best model: Epoch 17 \t loss=0.016869 \t val_loss=0.016890 \t time=0.75s\n",
      "Best model: Epoch 18 \t loss=0.016664 \t val_loss=0.016821 \t time=0.74s\n",
      "Best model: Epoch 19 \t loss=0.016417 \t val_loss=0.016683 \t time=0.75s\n",
      "Best model: Epoch 20 \t loss=0.016225 \t val_loss=0.016591 \t time=0.79s\n",
      "Best model: Epoch 22 \t loss=0.015986 \t val_loss=0.016465 \t time=0.76s\n",
      "Best model: Epoch 24 \t loss=0.015784 \t val_loss=0.016374 \t time=0.77s\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 29 \t loss=0.014782 \t val_loss=0.016199 \t time=0.98s\n",
      "Best model: Epoch 30 \t loss=0.014521 \t val_loss=0.016131 \t time=0.75s\n",
      "Best model: Epoch 31 \t loss=0.014358 \t val_loss=0.016063 \t time=0.75s\n",
      "Best model: Epoch 32 \t loss=0.014177 \t val_loss=0.016035 \t time=0.75s\n",
      "Best model: Epoch 34 \t loss=0.013939 \t val_loss=0.016024 \t time=0.74s\n",
      "Best model: Epoch 35 \t loss=0.013725 \t val_loss=0.016020 \t time=0.75s\n",
      "Best model: Epoch 36 \t loss=0.013646 \t val_loss=0.016016 \t time=0.77s\n",
      "Best model: Epoch 40 \t loss=0.013190 \t val_loss=0.015981 \t time=1.16s\n",
      "Fold 1 log loss: 0.016079840868431054\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.414855 \t val_loss=0.075799 \t time=0.82s\n",
      "Best model: Epoch 2 \t loss=0.048749 \t val_loss=0.029421 \t time=0.75s\n",
      "Best model: Epoch 3 \t loss=0.027499 \t val_loss=0.023112 \t time=0.79s\n",
      "Best model: Epoch 4 \t loss=0.023323 \t val_loss=0.020927 \t time=0.79s\n",
      "Best model: Epoch 5 \t loss=0.021469 \t val_loss=0.019786 \t time=0.82s\n",
      "Best model: Epoch 6 \t loss=0.020582 \t val_loss=0.019286 \t time=1.08s\n",
      "Best model: Epoch 7 \t loss=0.020079 \t val_loss=0.018804 \t time=0.79s\n",
      "Best model: Epoch 8 \t loss=0.019293 \t val_loss=0.018348 \t time=0.81s\n",
      "Best model: Epoch 9 \t loss=0.019088 \t val_loss=0.018044 \t time=0.95s\n",
      "Best model: Epoch 10 \t loss=0.018684 \t val_loss=0.018009 \t time=0.79s\n",
      "Best model: Epoch 11 \t loss=0.018275 \t val_loss=0.017549 \t time=0.75s\n",
      "Best model: Epoch 12 \t loss=0.017908 \t val_loss=0.017357 \t time=0.75s\n",
      "Best model: Epoch 13 \t loss=0.017698 \t val_loss=0.017252 \t time=0.76s\n",
      "Best model: Epoch 14 \t loss=0.017397 \t val_loss=0.017156 \t time=0.79s\n",
      "Best model: Epoch 15 \t loss=0.017207 \t val_loss=0.016991 \t time=0.75s\n",
      "Best model: Epoch 16 \t loss=0.016897 \t val_loss=0.016768 \t time=0.76s\n",
      "Best model: Epoch 18 \t loss=0.016568 \t val_loss=0.016672 \t time=0.75s\n",
      "Best model: Epoch 19 \t loss=0.016429 \t val_loss=0.016570 \t time=0.77s\n",
      "Best model: Epoch 20 \t loss=0.016223 \t val_loss=0.016533 \t time=0.75s\n",
      "Best model: Epoch 21 \t loss=0.016095 \t val_loss=0.016416 \t time=0.77s\n",
      "Best model: Epoch 23 \t loss=0.015917 \t val_loss=0.016410 \t time=0.94s\n",
      "Best model: Epoch 24 \t loss=0.015726 \t val_loss=0.016333 \t time=0.76s\n",
      "Best model: Epoch 27 \t loss=0.015412 \t val_loss=0.016291 \t time=0.77s\n",
      "Best model: Epoch 29 \t loss=0.015315 \t val_loss=0.016222 \t time=0.76s\n",
      "Best model: Epoch 32 \t loss=0.015167 \t val_loss=0.016173 \t time=0.76s\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 37 \t loss=0.014376 \t val_loss=0.016120 \t time=0.87s\n",
      "Best model: Epoch 38 \t loss=0.014005 \t val_loss=0.016016 \t time=0.78s\n",
      "Best model: Epoch 40 \t loss=0.013573 \t val_loss=0.016004 \t time=0.80s\n",
      "Fold 2 log loss: 0.016064742658142592\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.415098 \t val_loss=0.080733 \t time=0.75s\n",
      "Best model: Epoch 2 \t loss=0.048767 \t val_loss=0.028509 \t time=1.02s\n",
      "Best model: Epoch 3 \t loss=0.026784 \t val_loss=0.023150 \t time=0.80s\n",
      "Best model: Epoch 4 \t loss=0.023161 \t val_loss=0.021295 \t time=0.80s\n",
      "Best model: Epoch 5 \t loss=0.021450 \t val_loss=0.019731 \t time=0.78s\n",
      "Best model: Epoch 6 \t loss=0.020675 \t val_loss=0.019469 \t time=0.98s\n",
      "Best model: Epoch 7 \t loss=0.020227 \t val_loss=0.019188 \t time=0.77s\n",
      "Best model: Epoch 8 \t loss=0.019444 \t val_loss=0.018675 \t time=0.78s\n",
      "Best model: Epoch 9 \t loss=0.019434 \t val_loss=0.018288 \t time=0.78s\n",
      "Best model: Epoch 10 \t loss=0.018643 \t val_loss=0.018101 \t time=0.80s\n",
      "Best model: Epoch 11 \t loss=0.018215 \t val_loss=0.017674 \t time=0.78s\n",
      "Best model: Epoch 12 \t loss=0.017846 \t val_loss=0.017324 \t time=0.75s\n",
      "Best model: Epoch 15 \t loss=0.017431 \t val_loss=0.017053 \t time=0.78s\n",
      "Best model: Epoch 16 \t loss=0.017095 \t val_loss=0.016751 \t time=0.88s\n",
      "Best model: Epoch 19 \t loss=0.016675 \t val_loss=0.016733 \t time=0.83s\n",
      "Best model: Epoch 21 \t loss=0.016288 \t val_loss=0.016438 \t time=0.75s\n",
      "Best model: Epoch 22 \t loss=0.016149 \t val_loss=0.016371 \t time=0.76s\n",
      "Best model: Epoch 24 \t loss=0.015809 \t val_loss=0.016315 \t time=1.17s\n",
      "Best model: Epoch 25 \t loss=0.015760 \t val_loss=0.016313 \t time=0.82s\n",
      "Best model: Epoch 26 \t loss=0.015703 \t val_loss=0.016287 \t time=0.76s\n",
      "Best model: Epoch 27 \t loss=0.015558 \t val_loss=0.016223 \t time=0.78s\n",
      "Best model: Epoch 29 \t loss=0.015499 \t val_loss=0.016220 \t time=0.75s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014648 \t val_loss=0.016064 \t time=0.79s\n",
      "Best model: Epoch 35 \t loss=0.014381 \t val_loss=0.016009 \t time=0.77s\n",
      "Best model: Epoch 36 \t loss=0.014148 \t val_loss=0.015964 \t time=0.78s\n",
      "Best model: Epoch 37 \t loss=0.014036 \t val_loss=0.015958 \t time=0.79s\n",
      "Best model: Epoch 38 \t loss=0.013845 \t val_loss=0.015945 \t time=0.76s\n",
      "Best model: Epoch 39 \t loss=0.013695 \t val_loss=0.015936 \t time=0.76s\n",
      "Best model: Epoch 40 \t loss=0.013596 \t val_loss=0.015928 \t time=0.87s\n",
      "Fold 3 log loss: 0.01592049020117655\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.413201 \t val_loss=0.077351 \t time=0.76s\n",
      "Best model: Epoch 2 \t loss=0.047845 \t val_loss=0.027291 \t time=0.98s\n",
      "Best model: Epoch 3 \t loss=0.027879 \t val_loss=0.022880 \t time=0.76s\n",
      "Best model: Epoch 4 \t loss=0.023083 \t val_loss=0.021099 \t time=0.75s\n",
      "Best model: Epoch 5 \t loss=0.021983 \t val_loss=0.020159 \t time=0.76s\n",
      "Best model: Epoch 6 \t loss=0.020838 \t val_loss=0.019249 \t time=0.75s\n",
      "Best model: Epoch 7 \t loss=0.019902 \t val_loss=0.018613 \t time=0.76s\n",
      "Best model: Epoch 8 \t loss=0.019397 \t val_loss=0.018178 \t time=0.76s\n",
      "Best model: Epoch 9 \t loss=0.018991 \t val_loss=0.017956 \t time=0.86s\n",
      "Best model: Epoch 10 \t loss=0.018744 \t val_loss=0.017936 \t time=0.81s\n",
      "Best model: Epoch 11 \t loss=0.018373 \t val_loss=0.017435 \t time=0.76s\n",
      "Best model: Epoch 12 \t loss=0.017988 \t val_loss=0.017253 \t time=0.76s\n",
      "Best model: Epoch 13 \t loss=0.017706 \t val_loss=0.017092 \t time=0.75s\n",
      "Best model: Epoch 14 \t loss=0.017475 \t val_loss=0.016959 \t time=0.76s\n",
      "Best model: Epoch 15 \t loss=0.017304 \t val_loss=0.016845 \t time=0.92s\n",
      "Best model: Epoch 16 \t loss=0.016985 \t val_loss=0.016721 \t time=0.84s\n",
      "Best model: Epoch 17 \t loss=0.016821 \t val_loss=0.016602 \t time=0.76s\n",
      "Best model: Epoch 18 \t loss=0.016629 \t val_loss=0.016580 \t time=0.75s\n",
      "Best model: Epoch 19 \t loss=0.016474 \t val_loss=0.016482 \t time=0.76s\n",
      "Best model: Epoch 20 \t loss=0.016306 \t val_loss=0.016427 \t time=0.75s\n",
      "Best model: Epoch 21 \t loss=0.016230 \t val_loss=0.016412 \t time=0.76s\n",
      "Best model: Epoch 22 \t loss=0.016012 \t val_loss=0.016284 \t time=0.76s\n",
      "Best model: Epoch 23 \t loss=0.015916 \t val_loss=0.016250 \t time=0.78s\n",
      "Best model: Epoch 26 \t loss=0.015605 \t val_loss=0.016187 \t time=0.74s\n",
      "Best model: Epoch 29 \t loss=0.015414 \t val_loss=0.016160 \t time=0.97s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014523 \t val_loss=0.016007 \t time=0.77s\n",
      "Best model: Epoch 35 \t loss=0.014326 \t val_loss=0.015953 \t time=0.77s\n",
      "Best model: Epoch 36 \t loss=0.014128 \t val_loss=0.015927 \t time=0.76s\n",
      "Best model: Epoch 37 \t loss=0.013961 \t val_loss=0.015913 \t time=1.01s\n",
      "Best model: Epoch 38 \t loss=0.013800 \t val_loss=0.015902 \t time=0.88s\n",
      "Best model: Epoch 39 \t loss=0.013707 \t val_loss=0.015887 \t time=0.80s\n",
      "Best model: Epoch 40 \t loss=0.013494 \t val_loss=0.015874 \t time=0.79s\n",
      "Fold 4 log loss: 0.01592111042134359\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.414129 \t val_loss=0.077634 \t time=0.82s\n",
      "Best model: Epoch 2 \t loss=0.048715 \t val_loss=0.028425 \t time=0.79s\n",
      "Best model: Epoch 3 \t loss=0.027408 \t val_loss=0.022374 \t time=0.94s\n",
      "Best model: Epoch 4 \t loss=0.023334 \t val_loss=0.020878 \t time=0.76s\n",
      "Best model: Epoch 5 \t loss=0.021927 \t val_loss=0.020042 \t time=0.75s\n",
      "Best model: Epoch 6 \t loss=0.020496 \t val_loss=0.019306 \t time=0.76s\n",
      "Best model: Epoch 7 \t loss=0.019769 \t val_loss=0.018887 \t time=0.76s\n",
      "Best model: Epoch 8 \t loss=0.019723 \t val_loss=0.018739 \t time=0.80s\n",
      "Best model: Epoch 9 \t loss=0.018921 \t val_loss=0.018396 \t time=1.10s\n",
      "Best model: Epoch 10 \t loss=0.018596 \t val_loss=0.017923 \t time=0.79s\n",
      "Best model: Epoch 11 \t loss=0.018246 \t val_loss=0.017664 \t time=0.82s\n",
      "Best model: Epoch 12 \t loss=0.018002 \t val_loss=0.017398 \t time=0.92s\n",
      "Best model: Epoch 13 \t loss=0.017603 \t val_loss=0.017307 \t time=1.02s\n",
      "Best model: Epoch 15 \t loss=0.017409 \t val_loss=0.017181 \t time=0.82s\n",
      "Best model: Epoch 16 \t loss=0.017084 \t val_loss=0.017008 \t time=0.82s\n",
      "Best model: Epoch 18 \t loss=0.016757 \t val_loss=0.016826 \t time=0.76s\n",
      "Best model: Epoch 19 \t loss=0.016483 \t val_loss=0.016648 \t time=0.78s\n",
      "Best model: Epoch 20 \t loss=0.016282 \t val_loss=0.016632 \t time=0.84s\n",
      "Best model: Epoch 22 \t loss=0.016100 \t val_loss=0.016561 \t time=0.77s\n",
      "Best model: Epoch 23 \t loss=0.015977 \t val_loss=0.016516 \t time=0.77s\n",
      "Best model: Epoch 24 \t loss=0.015867 \t val_loss=0.016485 \t time=0.95s\n",
      "Best model: Epoch 25 \t loss=0.015713 \t val_loss=0.016440 \t time=0.77s\n",
      "Best model: Epoch 28 \t loss=0.015600 \t val_loss=0.016433 \t time=0.79s\n",
      "Best model: Epoch 29 \t loss=0.015441 \t val_loss=0.016363 \t time=0.79s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014609 \t val_loss=0.016205 \t time=0.75s\n",
      "Best model: Epoch 35 \t loss=0.014249 \t val_loss=0.016198 \t time=0.75s\n",
      "Best model: Epoch 36 \t loss=0.014074 \t val_loss=0.016103 \t time=0.79s\n",
      "Best model: Epoch 37 \t loss=0.013971 \t val_loss=0.016069 \t time=0.97s\n",
      "Best model: Epoch 38 \t loss=0.013754 \t val_loss=0.016059 \t time=0.84s\n",
      "Fold 5 log loss: 0.016063688369179374\n",
      "Seed 1\n",
      "Fold 1 log loss: 0.016079840868431054\n",
      "Fold 2 log loss: 0.016064742658142592\n",
      "Fold 3 log loss: 0.01592049020117655\n",
      "Fold 4 log loss: 0.01592111042134359\n",
      "Fold 5 log loss: 0.016063688369179374\n",
      "Std of log loss: 7.303465552417108e-05\n",
      "Total log loss: 0.01600996956097101\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.414243 \t val_loss=0.075401 \t time=0.77s\n",
      "Best model: Epoch 2 \t loss=0.049196 \t val_loss=0.028064 \t time=0.75s\n",
      "Best model: Epoch 3 \t loss=0.026940 \t val_loss=0.022525 \t time=0.76s\n",
      "Best model: Epoch 4 \t loss=0.023264 \t val_loss=0.021226 \t time=0.85s\n",
      "Best model: Epoch 5 \t loss=0.021407 \t val_loss=0.019807 \t time=0.86s\n",
      "Best model: Epoch 6 \t loss=0.020540 \t val_loss=0.019256 \t time=0.77s\n",
      "Best model: Epoch 7 \t loss=0.019968 \t val_loss=0.018993 \t time=0.76s\n",
      "Best model: Epoch 8 \t loss=0.019501 \t val_loss=0.018829 \t time=0.78s\n",
      "Best model: Epoch 9 \t loss=0.019202 \t val_loss=0.018372 \t time=0.76s\n",
      "Best model: Epoch 10 \t loss=0.019079 \t val_loss=0.018125 \t time=0.76s\n",
      "Best model: Epoch 11 \t loss=0.018344 \t val_loss=0.017942 \t time=0.76s\n",
      "Best model: Epoch 12 \t loss=0.018138 \t val_loss=0.017556 \t time=0.75s\n",
      "Best model: Epoch 13 \t loss=0.017707 \t val_loss=0.017321 \t time=0.76s\n",
      "Best model: Epoch 14 \t loss=0.017455 \t val_loss=0.017192 \t time=0.77s\n",
      "Best model: Epoch 15 \t loss=0.017177 \t val_loss=0.017051 \t time=0.79s\n",
      "Best model: Epoch 16 \t loss=0.017017 \t val_loss=0.016919 \t time=0.77s\n",
      "Best model: Epoch 18 \t loss=0.016662 \t val_loss=0.016903 \t time=0.95s\n",
      "Best model: Epoch 19 \t loss=0.016553 \t val_loss=0.016804 \t time=0.78s\n",
      "Best model: Epoch 20 \t loss=0.016457 \t val_loss=0.016643 \t time=0.76s\n",
      "Best model: Epoch 21 \t loss=0.016250 \t val_loss=0.016637 \t time=0.77s\n",
      "Best model: Epoch 22 \t loss=0.016172 \t val_loss=0.016621 \t time=0.78s\n",
      "Best model: Epoch 23 \t loss=0.016010 \t val_loss=0.016544 \t time=0.79s\n",
      "Best model: Epoch 25 \t loss=0.015790 \t val_loss=0.016434 \t time=0.82s\n",
      "Best model: Epoch 26 \t loss=0.015725 \t val_loss=0.016409 \t time=0.99s\n",
      "Best model: Epoch 28 \t loss=0.015650 \t val_loss=0.016400 \t time=0.79s\n",
      "Best model: Epoch 29 \t loss=0.015497 \t val_loss=0.016379 \t time=0.77s\n",
      "Best model: Epoch 33 \t loss=0.015218 \t val_loss=0.016377 \t time=1.02s\n",
      "Best model: Epoch 35 \t loss=0.015280 \t val_loss=0.016351 \t time=0.77s\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 40 \t loss=0.014390 \t val_loss=0.016189 \t time=0.78s\n",
      "Fold 1 log loss: 0.016280616069086786\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.412084 \t val_loss=0.076539 \t time=1.02s\n",
      "Best model: Epoch 2 \t loss=0.049085 \t val_loss=0.028798 \t time=0.77s\n",
      "Best model: Epoch 3 \t loss=0.026930 \t val_loss=0.022483 \t time=0.82s\n",
      "Best model: Epoch 4 \t loss=0.023605 \t val_loss=0.021048 \t time=0.77s\n",
      "Best model: Epoch 5 \t loss=0.021499 \t val_loss=0.019946 \t time=0.76s\n",
      "Best model: Epoch 6 \t loss=0.020550 \t val_loss=0.019083 \t time=0.77s\n",
      "Best model: Epoch 7 \t loss=0.019963 \t val_loss=0.018862 \t time=0.76s\n",
      "Best model: Epoch 8 \t loss=0.019379 \t val_loss=0.018356 \t time=0.77s\n",
      "Best model: Epoch 9 \t loss=0.018966 \t val_loss=0.018103 \t time=0.76s\n",
      "Best model: Epoch 10 \t loss=0.018577 \t val_loss=0.017829 \t time=0.79s\n",
      "Best model: Epoch 12 \t loss=0.018119 \t val_loss=0.017426 \t time=0.75s\n",
      "Best model: Epoch 13 \t loss=0.017755 \t val_loss=0.017169 \t time=0.75s\n",
      "Best model: Epoch 15 \t loss=0.017197 \t val_loss=0.016995 \t time=0.76s\n",
      "Best model: Epoch 16 \t loss=0.016993 \t val_loss=0.016966 \t time=0.78s\n",
      "Best model: Epoch 17 \t loss=0.016980 \t val_loss=0.016896 \t time=0.77s\n",
      "Best model: Epoch 18 \t loss=0.016810 \t val_loss=0.016787 \t time=0.77s\n",
      "Best model: Epoch 19 \t loss=0.016601 \t val_loss=0.016749 \t time=0.75s\n",
      "Best model: Epoch 20 \t loss=0.016273 \t val_loss=0.016588 \t time=0.75s\n",
      "Best model: Epoch 21 \t loss=0.016209 \t val_loss=0.016517 \t time=0.77s\n",
      "Best model: Epoch 23 \t loss=0.015865 \t val_loss=0.016396 \t time=0.76s\n",
      "Best model: Epoch 25 \t loss=0.015726 \t val_loss=0.016323 \t time=0.75s\n",
      "Best model: Epoch 29 \t loss=0.015473 \t val_loss=0.016273 \t time=0.75s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014607 \t val_loss=0.016130 \t time=0.75s\n",
      "Best model: Epoch 35 \t loss=0.014302 \t val_loss=0.016075 \t time=0.76s\n",
      "Best model: Epoch 36 \t loss=0.014098 \t val_loss=0.016072 \t time=0.75s\n",
      "Best model: Epoch 37 \t loss=0.014008 \t val_loss=0.016029 \t time=0.75s\n",
      "Best model: Epoch 38 \t loss=0.013877 \t val_loss=0.016023 \t time=0.78s\n",
      "Best model: Epoch 40 \t loss=0.013495 \t val_loss=0.016019 \t time=0.80s\n",
      "Fold 2 log loss: 0.0160819916597495\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.411402 \t val_loss=0.077279 \t time=0.76s\n",
      "Best model: Epoch 2 \t loss=0.049547 \t val_loss=0.029069 \t time=0.76s\n",
      "Best model: Epoch 3 \t loss=0.027701 \t val_loss=0.022526 \t time=0.75s\n",
      "Best model: Epoch 4 \t loss=0.022974 \t val_loss=0.020914 \t time=0.76s\n",
      "Best model: Epoch 5 \t loss=0.021929 \t val_loss=0.020299 \t time=0.75s\n",
      "Best model: Epoch 6 \t loss=0.020641 \t val_loss=0.019507 \t time=0.75s\n",
      "Best model: Epoch 7 \t loss=0.020131 \t val_loss=0.019061 \t time=0.76s\n",
      "Best model: Epoch 9 \t loss=0.019336 \t val_loss=0.018609 \t time=0.79s\n",
      "Best model: Epoch 10 \t loss=0.018623 \t val_loss=0.017850 \t time=0.77s\n",
      "Best model: Epoch 12 \t loss=0.018119 \t val_loss=0.017799 \t time=0.77s\n",
      "Best model: Epoch 13 \t loss=0.017806 \t val_loss=0.017264 \t time=0.76s\n",
      "Best model: Epoch 15 \t loss=0.017459 \t val_loss=0.017123 \t time=0.77s\n",
      "Best model: Epoch 16 \t loss=0.017064 \t val_loss=0.016860 \t time=0.80s\n",
      "Best model: Epoch 18 \t loss=0.016741 \t val_loss=0.016715 \t time=0.76s\n",
      "Best model: Epoch 19 \t loss=0.016492 \t val_loss=0.016638 \t time=1.10s\n",
      "Best model: Epoch 20 \t loss=0.016265 \t val_loss=0.016467 \t time=1.28s\n",
      "Best model: Epoch 21 \t loss=0.016243 \t val_loss=0.016437 \t time=0.84s\n",
      "Best model: Epoch 23 \t loss=0.016021 \t val_loss=0.016410 \t time=1.12s\n",
      "Best model: Epoch 24 \t loss=0.015786 \t val_loss=0.016352 \t time=0.92s\n",
      "Best model: Epoch 26 \t loss=0.015692 \t val_loss=0.016319 \t time=0.81s\n",
      "Best model: Epoch 27 \t loss=0.015599 \t val_loss=0.016292 \t time=0.83s\n",
      "Best model: Epoch 29 \t loss=0.015427 \t val_loss=0.016235 \t time=0.83s\n",
      "Best model: Epoch 30 \t loss=0.015397 \t val_loss=0.016205 \t time=0.82s\n",
      "Best model: Epoch 31 \t loss=0.015269 \t val_loss=0.016160 \t time=0.84s\n",
      "Best model: Epoch 33 \t loss=0.015170 \t val_loss=0.016138 \t time=0.76s\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 38 \t loss=0.014412 \t val_loss=0.016074 \t time=0.75s\n",
      "Best model: Epoch 39 \t loss=0.014103 \t val_loss=0.016024 \t time=0.75s\n",
      "Best model: Epoch 40 \t loss=0.013940 \t val_loss=0.015989 \t time=0.76s\n",
      "Fold 3 log loss: 0.01598215567903187\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.414447 \t val_loss=0.083103 \t time=0.76s\n",
      "Best model: Epoch 2 \t loss=0.048190 \t val_loss=0.029241 \t time=0.75s\n",
      "Best model: Epoch 3 \t loss=0.028207 \t val_loss=0.022736 \t time=0.75s\n",
      "Best model: Epoch 4 \t loss=0.023313 \t val_loss=0.020537 \t time=0.75s\n",
      "Best model: Epoch 5 \t loss=0.021788 \t val_loss=0.019921 \t time=0.77s\n",
      "Best model: Epoch 6 \t loss=0.020538 \t val_loss=0.019117 \t time=0.89s\n",
      "Best model: Epoch 7 \t loss=0.020011 \t val_loss=0.018677 \t time=0.84s\n",
      "Best model: Epoch 8 \t loss=0.019755 \t val_loss=0.018538 \t time=0.76s\n",
      "Best model: Epoch 9 \t loss=0.019270 \t val_loss=0.017984 \t time=0.77s\n",
      "Best model: Epoch 10 \t loss=0.018757 \t val_loss=0.017874 \t time=0.76s\n",
      "Best model: Epoch 11 \t loss=0.018413 \t val_loss=0.017491 \t time=0.83s\n",
      "Best model: Epoch 12 \t loss=0.018074 \t val_loss=0.017276 \t time=0.94s\n",
      "Best model: Epoch 13 \t loss=0.017728 \t val_loss=0.017107 \t time=0.76s\n",
      "Best model: Epoch 14 \t loss=0.017529 \t val_loss=0.017003 \t time=1.11s\n",
      "Best model: Epoch 15 \t loss=0.017251 \t val_loss=0.016845 \t time=0.81s\n",
      "Best model: Epoch 16 \t loss=0.017310 \t val_loss=0.016813 \t time=0.78s\n",
      "Best model: Epoch 17 \t loss=0.017063 \t val_loss=0.016771 \t time=0.81s\n",
      "Best model: Epoch 18 \t loss=0.016799 \t val_loss=0.016573 \t time=0.83s\n",
      "Best model: Epoch 19 \t loss=0.016628 \t val_loss=0.016536 \t time=1.00s\n",
      "Best model: Epoch 20 \t loss=0.016486 \t val_loss=0.016442 \t time=0.79s\n",
      "Best model: Epoch 21 \t loss=0.016320 \t val_loss=0.016359 \t time=0.78s\n",
      "Best model: Epoch 22 \t loss=0.016065 \t val_loss=0.016320 \t time=0.76s\n",
      "Best model: Epoch 23 \t loss=0.016071 \t val_loss=0.016278 \t time=0.80s\n",
      "Best model: Epoch 24 \t loss=0.015930 \t val_loss=0.016261 \t time=0.75s\n",
      "Best model: Epoch 26 \t loss=0.015693 \t val_loss=0.016257 \t time=0.76s\n",
      "Best model: Epoch 28 \t loss=0.015580 \t val_loss=0.016195 \t time=0.75s\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 33 \t loss=0.014672 \t val_loss=0.016048 \t time=0.91s\n",
      "Best model: Epoch 34 \t loss=0.014376 \t val_loss=0.015966 \t time=0.75s\n",
      "Best model: Epoch 35 \t loss=0.014214 \t val_loss=0.015932 \t time=0.77s\n",
      "Best model: Epoch 37 \t loss=0.013900 \t val_loss=0.015920 \t time=0.76s\n",
      "Best model: Epoch 40 \t loss=0.013575 \t val_loss=0.015914 \t time=0.77s\n",
      "Fold 4 log loss: 0.015969970162948186\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.413087 \t val_loss=0.073699 \t time=0.76s\n",
      "Best model: Epoch 2 \t loss=0.049084 \t val_loss=0.029309 \t time=0.76s\n",
      "Best model: Epoch 3 \t loss=0.028082 \t val_loss=0.023504 \t time=0.96s\n",
      "Best model: Epoch 4 \t loss=0.023135 \t val_loss=0.020912 \t time=1.09s\n",
      "Best model: Epoch 5 \t loss=0.021648 \t val_loss=0.020058 \t time=0.75s\n",
      "Best model: Epoch 6 \t loss=0.020544 \t val_loss=0.019231 \t time=0.97s\n",
      "Best model: Epoch 7 \t loss=0.019869 \t val_loss=0.018961 \t time=0.76s\n",
      "Best model: Epoch 8 \t loss=0.019517 \t val_loss=0.018423 \t time=0.79s\n",
      "Best model: Epoch 9 \t loss=0.018956 \t val_loss=0.018169 \t time=0.78s\n",
      "Best model: Epoch 10 \t loss=0.018656 \t val_loss=0.017995 \t time=1.50s\n",
      "Best model: Epoch 11 \t loss=0.018387 \t val_loss=0.017671 \t time=0.78s\n",
      "Best model: Epoch 12 \t loss=0.017993 \t val_loss=0.017505 \t time=0.80s\n",
      "Best model: Epoch 13 \t loss=0.017741 \t val_loss=0.017458 \t time=0.76s\n",
      "Best model: Epoch 14 \t loss=0.017536 \t val_loss=0.017309 \t time=0.80s\n",
      "Best model: Epoch 15 \t loss=0.017324 \t val_loss=0.017063 \t time=1.03s\n",
      "Best model: Epoch 16 \t loss=0.017025 \t val_loss=0.017025 \t time=0.81s\n",
      "Best model: Epoch 17 \t loss=0.016810 \t val_loss=0.016959 \t time=0.75s\n",
      "Best model: Epoch 18 \t loss=0.016821 \t val_loss=0.016803 \t time=0.76s\n",
      "Best model: Epoch 19 \t loss=0.016505 \t val_loss=0.016753 \t time=0.76s\n",
      "Best model: Epoch 21 \t loss=0.016231 \t val_loss=0.016673 \t time=0.76s\n",
      "Best model: Epoch 22 \t loss=0.016069 \t val_loss=0.016543 \t time=0.75s\n",
      "Best model: Epoch 23 \t loss=0.015954 \t val_loss=0.016523 \t time=0.79s\n",
      "Best model: Epoch 25 \t loss=0.015810 \t val_loss=0.016492 \t time=0.76s\n",
      "Best model: Epoch 26 \t loss=0.015618 \t val_loss=0.016482 \t time=0.75s\n",
      "Best model: Epoch 27 \t loss=0.015527 \t val_loss=0.016386 \t time=0.75s\n",
      "Best model: Epoch 28 \t loss=0.015535 \t val_loss=0.016386 \t time=0.94s\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 32 \t loss=0.014660 \t val_loss=0.016190 \t time=0.75s\n",
      "Best model: Epoch 34 \t loss=0.014195 \t val_loss=0.016131 \t time=0.76s\n",
      "Best model: Epoch 35 \t loss=0.014032 \t val_loss=0.016129 \t time=0.77s\n",
      "Best model: Epoch 36 \t loss=0.013952 \t val_loss=0.016119 \t time=0.75s\n",
      "Best model: Epoch 38 \t loss=0.013619 \t val_loss=0.016104 \t time=0.75s\n",
      "Fold 5 log loss: 0.016105637577698048\n",
      "Seed 2\n",
      "Fold 1 log loss: 0.016280616069086786\n",
      "Fold 2 log loss: 0.0160819916597495\n",
      "Fold 3 log loss: 0.01598215567903187\n",
      "Fold 4 log loss: 0.015969970162948186\n",
      "Fold 5 log loss: 0.016105637577698048\n",
      "Std of log loss: 0.0001118079949946509\n",
      "Total log loss: 0.01608407334211503\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.413482 \t val_loss=0.085006 \t time=0.75s\n",
      "Best model: Epoch 2 \t loss=0.048716 \t val_loss=0.028552 \t time=0.76s\n",
      "Best model: Epoch 3 \t loss=0.027498 \t val_loss=0.023904 \t time=0.76s\n",
      "Best model: Epoch 4 \t loss=0.023643 \t val_loss=0.022032 \t time=1.05s\n",
      "Best model: Epoch 5 \t loss=0.021593 \t val_loss=0.019921 \t time=0.76s\n",
      "Best model: Epoch 6 \t loss=0.020638 \t val_loss=0.019664 \t time=0.75s\n",
      "Best model: Epoch 7 \t loss=0.019979 \t val_loss=0.019123 \t time=0.76s\n",
      "Best model: Epoch 8 \t loss=0.019318 \t val_loss=0.018529 \t time=0.80s\n",
      "Best model: Epoch 9 \t loss=0.019013 \t val_loss=0.018134 \t time=0.94s\n",
      "Best model: Epoch 11 \t loss=0.018321 \t val_loss=0.017871 \t time=0.75s\n",
      "Best model: Epoch 12 \t loss=0.018269 \t val_loss=0.017847 \t time=0.75s\n",
      "Best model: Epoch 13 \t loss=0.017851 \t val_loss=0.017346 \t time=0.76s\n",
      "Best model: Epoch 14 \t loss=0.017423 \t val_loss=0.017188 \t time=0.80s\n",
      "Best model: Epoch 16 \t loss=0.017074 \t val_loss=0.016996 \t time=0.76s\n",
      "Best model: Epoch 17 \t loss=0.016863 \t val_loss=0.016935 \t time=0.76s\n",
      "Best model: Epoch 18 \t loss=0.016690 \t val_loss=0.016766 \t time=0.76s\n",
      "Best model: Epoch 19 \t loss=0.016395 \t val_loss=0.016740 \t time=0.77s\n",
      "Best model: Epoch 20 \t loss=0.016392 \t val_loss=0.016729 \t time=0.78s\n",
      "Best model: Epoch 21 \t loss=0.016257 \t val_loss=0.016575 \t time=0.81s\n",
      "Best model: Epoch 22 \t loss=0.016079 \t val_loss=0.016537 \t time=0.98s\n",
      "Best model: Epoch 24 \t loss=0.015940 \t val_loss=0.016440 \t time=0.78s\n",
      "Best model: Epoch 26 \t loss=0.015711 \t val_loss=0.016439 \t time=0.75s\n",
      "Best model: Epoch 27 \t loss=0.015655 \t val_loss=0.016355 \t time=0.76s\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 32 \t loss=0.014718 \t val_loss=0.016233 \t time=0.75s\n",
      "Best model: Epoch 33 \t loss=0.014492 \t val_loss=0.016164 \t time=0.75s\n",
      "Best model: Epoch 34 \t loss=0.014242 \t val_loss=0.016162 \t time=0.98s\n",
      "Best model: Epoch 35 \t loss=0.014062 \t val_loss=0.016079 \t time=1.31s\n",
      "Best model: Epoch 38 \t loss=0.013740 \t val_loss=0.016068 \t time=0.78s\n",
      "Fold 1 log loss: 0.01615919115850787\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.410309 \t val_loss=0.088772 \t time=0.76s\n",
      "Best model: Epoch 2 \t loss=0.049774 \t val_loss=0.029268 \t time=0.75s\n",
      "Best model: Epoch 3 \t loss=0.027053 \t val_loss=0.022645 \t time=0.75s\n",
      "Best model: Epoch 4 \t loss=0.023236 \t val_loss=0.021036 \t time=0.77s\n",
      "Best model: Epoch 5 \t loss=0.021846 \t val_loss=0.019918 \t time=0.96s\n",
      "Best model: Epoch 6 \t loss=0.020478 \t val_loss=0.019441 \t time=0.75s\n",
      "Best model: Epoch 7 \t loss=0.020034 \t val_loss=0.019010 \t time=0.75s\n",
      "Best model: Epoch 8 \t loss=0.019641 \t val_loss=0.018800 \t time=0.78s\n",
      "Best model: Epoch 9 \t loss=0.019041 \t val_loss=0.018087 \t time=0.76s\n",
      "Best model: Epoch 10 \t loss=0.018651 \t val_loss=0.017874 \t time=0.75s\n",
      "Best model: Epoch 11 \t loss=0.018348 \t val_loss=0.017594 \t time=0.75s\n",
      "Best model: Epoch 12 \t loss=0.017880 \t val_loss=0.017560 \t time=0.76s\n",
      "Best model: Epoch 13 \t loss=0.017816 \t val_loss=0.017224 \t time=0.76s\n",
      "Best model: Epoch 14 \t loss=0.017406 \t val_loss=0.017038 \t time=0.81s\n",
      "Best model: Epoch 15 \t loss=0.017159 \t val_loss=0.016993 \t time=0.77s\n",
      "Best model: Epoch 16 \t loss=0.017050 \t val_loss=0.016916 \t time=0.80s\n",
      "Best model: Epoch 18 \t loss=0.016693 \t val_loss=0.016737 \t time=0.95s\n",
      "Best model: Epoch 19 \t loss=0.016392 \t val_loss=0.016562 \t time=0.76s\n",
      "Best model: Epoch 21 \t loss=0.016198 \t val_loss=0.016541 \t time=0.74s\n",
      "Best model: Epoch 22 \t loss=0.016053 \t val_loss=0.016421 \t time=0.75s\n",
      "Best model: Epoch 24 \t loss=0.015905 \t val_loss=0.016389 \t time=0.74s\n",
      "Best model: Epoch 25 \t loss=0.015724 \t val_loss=0.016362 \t time=0.75s\n",
      "Best model: Epoch 27 \t loss=0.015611 \t val_loss=0.016351 \t time=0.75s\n",
      "Best model: Epoch 29 \t loss=0.015418 \t val_loss=0.016304 \t time=0.75s\n",
      "Best model: Epoch 30 \t loss=0.015379 \t val_loss=0.016291 \t time=0.76s\n",
      "Best model: Epoch 31 \t loss=0.015286 \t val_loss=0.016288 \t time=0.88s\n",
      "Best model: Epoch 35 \t loss=0.015174 \t val_loss=0.016268 \t time=0.79s\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 40 \t loss=0.014326 \t val_loss=0.016078 \t time=1.05s\n",
      "Fold 2 log loss: 0.016131589753943455\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.416642 \t val_loss=0.077390 \t time=0.76s\n",
      "Best model: Epoch 2 \t loss=0.049910 \t val_loss=0.028385 \t time=0.95s\n",
      "Best model: Epoch 3 \t loss=0.027063 \t val_loss=0.022448 \t time=0.75s\n",
      "Best model: Epoch 4 \t loss=0.023517 \t val_loss=0.021003 \t time=0.75s\n",
      "Best model: Epoch 5 \t loss=0.021520 \t val_loss=0.020122 \t time=0.75s\n",
      "Best model: Epoch 6 \t loss=0.020809 \t val_loss=0.019284 \t time=0.75s\n",
      "Best model: Epoch 7 \t loss=0.019977 \t val_loss=0.018849 \t time=0.76s\n",
      "Best model: Epoch 8 \t loss=0.019361 \t val_loss=0.018720 \t time=0.75s\n",
      "Best model: Epoch 9 \t loss=0.019017 \t val_loss=0.018142 \t time=0.80s\n",
      "Best model: Epoch 10 \t loss=0.018557 \t val_loss=0.017915 \t time=0.76s\n",
      "Best model: Epoch 11 \t loss=0.018590 \t val_loss=0.017653 \t time=0.76s\n",
      "Best model: Epoch 12 \t loss=0.018100 \t val_loss=0.017520 \t time=0.76s\n",
      "Best model: Epoch 13 \t loss=0.017907 \t val_loss=0.017251 \t time=0.75s\n",
      "Best model: Epoch 14 \t loss=0.017499 \t val_loss=0.017095 \t time=0.75s\n",
      "Best model: Epoch 15 \t loss=0.017316 \t val_loss=0.017086 \t time=1.28s\n",
      "Best model: Epoch 16 \t loss=0.017026 \t val_loss=0.016796 \t time=0.81s\n",
      "Best model: Epoch 18 \t loss=0.016643 \t val_loss=0.016680 \t time=0.74s\n",
      "Best model: Epoch 20 \t loss=0.016413 \t val_loss=0.016520 \t time=0.98s\n",
      "Best model: Epoch 21 \t loss=0.016271 \t val_loss=0.016456 \t time=0.76s\n",
      "Best model: Epoch 22 \t loss=0.016094 \t val_loss=0.016416 \t time=1.04s\n",
      "Best model: Epoch 23 \t loss=0.015980 \t val_loss=0.016379 \t time=0.77s\n",
      "Best model: Epoch 25 \t loss=0.015681 \t val_loss=0.016315 \t time=0.82s\n",
      "Best model: Epoch 26 \t loss=0.015640 \t val_loss=0.016247 \t time=0.78s\n",
      "Best model: Epoch 30 \t loss=0.015300 \t val_loss=0.016237 \t time=0.80s\n",
      "Best model: Epoch 31 \t loss=0.015267 \t val_loss=0.016224 \t time=0.85s\n",
      "Best model: Epoch 32 \t loss=0.015209 \t val_loss=0.016183 \t time=0.78s\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 37 \t loss=0.014489 \t val_loss=0.016069 \t time=0.75s\n",
      "Best model: Epoch 38 \t loss=0.014129 \t val_loss=0.015990 \t time=0.75s\n",
      "Best model: Epoch 40 \t loss=0.013782 \t val_loss=0.015978 \t time=0.84s\n",
      "Fold 3 log loss: 0.015970096888850625\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.408462 \t val_loss=0.074284 \t time=0.79s\n",
      "Best model: Epoch 2 \t loss=0.049468 \t val_loss=0.028216 \t time=0.78s\n",
      "Best model: Epoch 3 \t loss=0.027922 \t val_loss=0.022309 \t time=0.75s\n",
      "Best model: Epoch 4 \t loss=0.023475 \t val_loss=0.020758 \t time=0.75s\n",
      "Best model: Epoch 5 \t loss=0.021691 \t val_loss=0.019736 \t time=0.75s\n",
      "Best model: Epoch 6 \t loss=0.020567 \t val_loss=0.019042 \t time=0.75s\n",
      "Best model: Epoch 7 \t loss=0.019984 \t val_loss=0.018598 \t time=0.75s\n",
      "Best model: Epoch 8 \t loss=0.019345 \t val_loss=0.018219 \t time=0.79s\n",
      "Best model: Epoch 9 \t loss=0.019050 \t val_loss=0.017943 \t time=0.75s\n",
      "Best model: Epoch 10 \t loss=0.018741 \t val_loss=0.017613 \t time=0.75s\n",
      "Best model: Epoch 12 \t loss=0.017970 \t val_loss=0.017243 \t time=0.75s\n",
      "Best model: Epoch 13 \t loss=0.017718 \t val_loss=0.017188 \t time=0.76s\n",
      "Best model: Epoch 14 \t loss=0.017529 \t val_loss=0.016956 \t time=0.76s\n",
      "Best model: Epoch 15 \t loss=0.017444 \t val_loss=0.016822 \t time=0.77s\n",
      "Best model: Epoch 16 \t loss=0.017227 \t val_loss=0.016729 \t time=1.02s\n",
      "Best model: Epoch 17 \t loss=0.016915 \t val_loss=0.016672 \t time=0.75s\n",
      "Best model: Epoch 18 \t loss=0.016773 \t val_loss=0.016519 \t time=0.75s\n",
      "Best model: Epoch 21 \t loss=0.016231 \t val_loss=0.016478 \t time=0.76s\n",
      "Best model: Epoch 22 \t loss=0.016146 \t val_loss=0.016421 \t time=0.76s\n",
      "Best model: Epoch 23 \t loss=0.016069 \t val_loss=0.016306 \t time=0.75s\n",
      "Best model: Epoch 24 \t loss=0.015932 \t val_loss=0.016244 \t time=0.97s\n",
      "Best model: Epoch 25 \t loss=0.015780 \t val_loss=0.016208 \t time=0.76s\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 30 \t loss=0.014801 \t val_loss=0.016035 \t time=0.75s\n",
      "Best model: Epoch 31 \t loss=0.014562 \t val_loss=0.015951 \t time=1.02s\n",
      "Best model: Epoch 32 \t loss=0.014330 \t val_loss=0.015949 \t time=0.77s\n",
      "Best model: Epoch 33 \t loss=0.014244 \t val_loss=0.015931 \t time=0.75s\n",
      "Best model: Epoch 34 \t loss=0.014103 \t val_loss=0.015871 \t time=0.76s\n",
      "Fold 4 log loss: 0.01593554139806171\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.412365 \t val_loss=0.078813 \t time=1.08s\n",
      "Best model: Epoch 2 \t loss=0.049562 \t val_loss=0.030582 \t time=0.94s\n",
      "Best model: Epoch 3 \t loss=0.027535 \t val_loss=0.022771 \t time=0.77s\n",
      "Best model: Epoch 4 \t loss=0.023436 \t val_loss=0.020868 \t time=0.76s\n",
      "Best model: Epoch 5 \t loss=0.021557 \t val_loss=0.019981 \t time=0.77s\n",
      "Best model: Epoch 6 \t loss=0.020607 \t val_loss=0.019295 \t time=0.76s\n",
      "Best model: Epoch 7 \t loss=0.020060 \t val_loss=0.019073 \t time=0.76s\n",
      "Best model: Epoch 8 \t loss=0.019507 \t val_loss=0.018563 \t time=1.04s\n",
      "Best model: Epoch 9 \t loss=0.019204 \t val_loss=0.018202 \t time=0.78s\n",
      "Best model: Epoch 10 \t loss=0.018767 \t val_loss=0.017955 \t time=1.05s\n",
      "Best model: Epoch 11 \t loss=0.018517 \t val_loss=0.017727 \t time=0.79s\n",
      "Best model: Epoch 12 \t loss=0.018120 \t val_loss=0.017525 \t time=0.79s\n",
      "Best model: Epoch 13 \t loss=0.017746 \t val_loss=0.017468 \t time=0.75s\n",
      "Best model: Epoch 14 \t loss=0.017485 \t val_loss=0.017203 \t time=0.80s\n",
      "Best model: Epoch 15 \t loss=0.017232 \t val_loss=0.017094 \t time=0.76s\n",
      "Best model: Epoch 16 \t loss=0.017003 \t val_loss=0.017025 \t time=0.78s\n",
      "Best model: Epoch 17 \t loss=0.016785 \t val_loss=0.016926 \t time=0.75s\n",
      "Best model: Epoch 18 \t loss=0.016714 \t val_loss=0.016916 \t time=0.76s\n",
      "Best model: Epoch 19 \t loss=0.016531 \t val_loss=0.016807 \t time=0.75s\n",
      "Best model: Epoch 20 \t loss=0.016316 \t val_loss=0.016705 \t time=0.78s\n",
      "Best model: Epoch 21 \t loss=0.016168 \t val_loss=0.016531 \t time=0.91s\n",
      "Best model: Epoch 22 \t loss=0.016100 \t val_loss=0.016499 \t time=0.80s\n",
      "Best model: Epoch 25 \t loss=0.015851 \t val_loss=0.016459 \t time=0.75s\n",
      "Best model: Epoch 29 \t loss=0.015470 \t val_loss=0.016444 \t time=0.76s\n",
      "Best model: Epoch 31 \t loss=0.015325 \t val_loss=0.016384 \t time=0.75s\n",
      "Best model: Epoch 35 \t loss=0.015085 \t val_loss=0.016368 \t time=0.77s\n",
      "Best model: Epoch 39 \t loss=0.014981 \t val_loss=0.016356 \t time=0.76s\n",
      "Fold 5 log loss: 0.01637002395976194\n",
      "Seed 3\n",
      "Fold 1 log loss: 0.01615919115850787\n",
      "Fold 2 log loss: 0.016131589753943455\n",
      "Fold 3 log loss: 0.015970096888850625\n",
      "Fold 4 log loss: 0.01593554139806171\n",
      "Fold 5 log loss: 0.01637002395976194\n",
      "Std of log loss: 0.0001552114403026376\n",
      "Total log loss: 0.01611327610054892\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.417376 \t val_loss=0.076125 \t time=0.78s\n",
      "Best model: Epoch 2 \t loss=0.050306 \t val_loss=0.028216 \t time=1.29s\n",
      "Best model: Epoch 3 \t loss=0.026912 \t val_loss=0.022988 \t time=0.76s\n",
      "Best model: Epoch 4 \t loss=0.023302 \t val_loss=0.021196 \t time=0.75s\n",
      "Best model: Epoch 5 \t loss=0.021557 \t val_loss=0.020048 \t time=0.77s\n",
      "Best model: Epoch 6 \t loss=0.020636 \t val_loss=0.019492 \t time=0.76s\n",
      "Best model: Epoch 7 \t loss=0.019718 \t val_loss=0.018851 \t time=0.76s\n",
      "Best model: Epoch 8 \t loss=0.019296 \t val_loss=0.018580 \t time=0.80s\n",
      "Best model: Epoch 9 \t loss=0.019243 \t val_loss=0.018538 \t time=0.76s\n",
      "Best model: Epoch 10 \t loss=0.018776 \t val_loss=0.018315 \t time=0.76s\n",
      "Best model: Epoch 11 \t loss=0.018431 \t val_loss=0.018107 \t time=0.76s\n",
      "Best model: Epoch 12 \t loss=0.018065 \t val_loss=0.017536 \t time=0.77s\n",
      "Best model: Epoch 13 \t loss=0.017735 \t val_loss=0.017326 \t time=0.77s\n",
      "Best model: Epoch 14 \t loss=0.017458 \t val_loss=0.017205 \t time=0.80s\n",
      "Best model: Epoch 15 \t loss=0.017398 \t val_loss=0.017164 \t time=0.94s\n",
      "Best model: Epoch 16 \t loss=0.017067 \t val_loss=0.017005 \t time=0.77s\n",
      "Best model: Epoch 17 \t loss=0.016868 \t val_loss=0.016867 \t time=0.81s\n",
      "Best model: Epoch 18 \t loss=0.016657 \t val_loss=0.016804 \t time=0.76s\n",
      "Best model: Epoch 19 \t loss=0.016450 \t val_loss=0.016644 \t time=0.79s\n",
      "Best model: Epoch 20 \t loss=0.016293 \t val_loss=0.016622 \t time=0.77s\n",
      "Best model: Epoch 21 \t loss=0.016240 \t val_loss=0.016574 \t time=1.10s\n",
      "Best model: Epoch 22 \t loss=0.016049 \t val_loss=0.016538 \t time=0.84s\n",
      "Best model: Epoch 23 \t loss=0.015954 \t val_loss=0.016425 \t time=0.79s\n",
      "Best model: Epoch 25 \t loss=0.015780 \t val_loss=0.016411 \t time=0.75s\n",
      "Best model: Epoch 26 \t loss=0.015589 \t val_loss=0.016411 \t time=1.11s\n",
      "Best model: Epoch 27 \t loss=0.015555 \t val_loss=0.016378 \t time=1.02s\n",
      "Best model: Epoch 30 \t loss=0.015372 \t val_loss=0.016330 \t time=0.75s\n",
      "Best model: Epoch 31 \t loss=0.015312 \t val_loss=0.016320 \t time=0.76s\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 36 \t loss=0.014407 \t val_loss=0.016174 \t time=0.78s\n",
      "Best model: Epoch 37 \t loss=0.014183 \t val_loss=0.016169 \t time=0.78s\n",
      "Best model: Epoch 38 \t loss=0.013891 \t val_loss=0.016102 \t time=0.79s\n",
      "Best model: Epoch 40 \t loss=0.013597 \t val_loss=0.016079 \t time=1.09s\n",
      "Fold 1 log loss: 0.016174484102574663\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.410688 \t val_loss=0.080734 \t time=0.80s\n",
      "Best model: Epoch 2 \t loss=0.048953 \t val_loss=0.028426 \t time=0.78s\n",
      "Best model: Epoch 3 \t loss=0.027673 \t val_loss=0.022677 \t time=0.78s\n",
      "Best model: Epoch 4 \t loss=0.023493 \t val_loss=0.021433 \t time=0.79s\n",
      "Best model: Epoch 5 \t loss=0.021537 \t val_loss=0.020268 \t time=0.78s\n",
      "Best model: Epoch 6 \t loss=0.020399 \t val_loss=0.019240 \t time=0.78s\n",
      "Best model: Epoch 7 \t loss=0.019899 \t val_loss=0.018880 \t time=0.78s\n",
      "Best model: Epoch 8 \t loss=0.019432 \t val_loss=0.018494 \t time=0.83s\n",
      "Best model: Epoch 9 \t loss=0.018890 \t val_loss=0.018014 \t time=0.76s\n",
      "Best model: Epoch 10 \t loss=0.018555 \t val_loss=0.017699 \t time=0.97s\n",
      "Best model: Epoch 11 \t loss=0.018488 \t val_loss=0.017471 \t time=0.76s\n",
      "Best model: Epoch 12 \t loss=0.017990 \t val_loss=0.017407 \t time=0.81s\n",
      "Best model: Epoch 13 \t loss=0.017646 \t val_loss=0.017239 \t time=0.76s\n",
      "Best model: Epoch 14 \t loss=0.017444 \t val_loss=0.017084 \t time=0.76s\n",
      "Best model: Epoch 15 \t loss=0.017198 \t val_loss=0.017004 \t time=0.81s\n",
      "Best model: Epoch 16 \t loss=0.017013 \t val_loss=0.016855 \t time=1.04s\n",
      "Best model: Epoch 18 \t loss=0.016731 \t val_loss=0.016750 \t time=0.75s\n",
      "Best model: Epoch 19 \t loss=0.016473 \t val_loss=0.016698 \t time=0.75s\n",
      "Best model: Epoch 20 \t loss=0.016372 \t val_loss=0.016563 \t time=0.76s\n",
      "Best model: Epoch 21 \t loss=0.016135 \t val_loss=0.016538 \t time=0.76s\n",
      "Best model: Epoch 22 \t loss=0.016041 \t val_loss=0.016417 \t time=0.85s\n",
      "Best model: Epoch 24 \t loss=0.015880 \t val_loss=0.016353 \t time=0.76s\n",
      "Best model: Epoch 25 \t loss=0.015686 \t val_loss=0.016303 \t time=0.76s\n",
      "Best model: Epoch 26 \t loss=0.015627 \t val_loss=0.016299 \t time=0.76s\n",
      "Best model: Epoch 27 \t loss=0.015532 \t val_loss=0.016210 \t time=0.77s\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 32 \t loss=0.014590 \t val_loss=0.016123 \t time=0.75s\n",
      "Best model: Epoch 33 \t loss=0.014261 \t val_loss=0.016048 \t time=1.02s\n",
      "Best model: Epoch 35 \t loss=0.013990 \t val_loss=0.016027 \t time=0.75s\n",
      "Best model: Epoch 37 \t loss=0.013710 \t val_loss=0.016001 \t time=0.76s\n",
      "Best model: Epoch 39 \t loss=0.013421 \t val_loss=0.015993 \t time=0.75s\n",
      "Fold 2 log loss: 0.016047175759869827\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.410937 \t val_loss=0.082875 \t time=0.77s\n",
      "Best model: Epoch 2 \t loss=0.049043 \t val_loss=0.027972 \t time=0.76s\n",
      "Best model: Epoch 3 \t loss=0.027629 \t val_loss=0.022424 \t time=0.78s\n",
      "Best model: Epoch 4 \t loss=0.023497 \t val_loss=0.020791 \t time=0.77s\n",
      "Best model: Epoch 5 \t loss=0.021301 \t val_loss=0.019844 \t time=0.81s\n",
      "Best model: Epoch 6 \t loss=0.020559 \t val_loss=0.019263 \t time=0.98s\n",
      "Best model: Epoch 7 \t loss=0.020173 \t val_loss=0.018889 \t time=0.78s\n",
      "Best model: Epoch 8 \t loss=0.019406 \t val_loss=0.018523 \t time=0.78s\n",
      "Best model: Epoch 9 \t loss=0.019178 \t val_loss=0.018472 \t time=0.77s\n",
      "Best model: Epoch 10 \t loss=0.018764 \t val_loss=0.017898 \t time=1.03s\n",
      "Best model: Epoch 11 \t loss=0.018385 \t val_loss=0.017611 \t time=0.91s\n",
      "Best model: Epoch 13 \t loss=0.018019 \t val_loss=0.017296 \t time=1.04s\n",
      "Best model: Epoch 14 \t loss=0.017689 \t val_loss=0.017133 \t time=0.78s\n",
      "Best model: Epoch 15 \t loss=0.017383 \t val_loss=0.016993 \t time=0.79s\n",
      "Best model: Epoch 16 \t loss=0.017100 \t val_loss=0.016782 \t time=0.76s\n",
      "Best model: Epoch 17 \t loss=0.016827 \t val_loss=0.016761 \t time=0.76s\n",
      "Best model: Epoch 18 \t loss=0.016695 \t val_loss=0.016640 \t time=1.03s\n",
      "Best model: Epoch 19 \t loss=0.016421 \t val_loss=0.016541 \t time=0.82s\n",
      "Best model: Epoch 20 \t loss=0.016383 \t val_loss=0.016434 \t time=0.80s\n",
      "Best model: Epoch 22 \t loss=0.016122 \t val_loss=0.016432 \t time=0.77s\n",
      "Best model: Epoch 23 \t loss=0.015940 \t val_loss=0.016393 \t time=0.79s\n",
      "Best model: Epoch 24 \t loss=0.015808 \t val_loss=0.016330 \t time=0.77s\n",
      "Best model: Epoch 25 \t loss=0.015735 \t val_loss=0.016300 \t time=0.79s\n",
      "Best model: Epoch 27 \t loss=0.015601 \t val_loss=0.016294 \t time=0.79s\n",
      "Best model: Epoch 29 \t loss=0.015337 \t val_loss=0.016287 \t time=0.76s\n",
      "Best model: Epoch 31 \t loss=0.015245 \t val_loss=0.016173 \t time=0.79s\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 36 \t loss=0.014393 \t val_loss=0.016052 \t time=0.76s\n",
      "Best model: Epoch 37 \t loss=0.014168 \t val_loss=0.015955 \t time=0.75s\n",
      "Best model: Epoch 38 \t loss=0.013916 \t val_loss=0.015939 \t time=0.75s\n",
      "Fold 3 log loss: 0.015928155998706423\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.414284 \t val_loss=0.077726 \t time=0.92s\n",
      "Best model: Epoch 2 \t loss=0.048858 \t val_loss=0.027310 \t time=0.77s\n",
      "Best model: Epoch 3 \t loss=0.027433 \t val_loss=0.022797 \t time=0.77s\n",
      "Best model: Epoch 4 \t loss=0.023323 \t val_loss=0.020871 \t time=0.79s\n",
      "Best model: Epoch 5 \t loss=0.021639 \t val_loss=0.019794 \t time=0.77s\n",
      "Best model: Epoch 6 \t loss=0.020495 \t val_loss=0.019132 \t time=0.76s\n",
      "Best model: Epoch 7 \t loss=0.019782 \t val_loss=0.018384 \t time=0.77s\n",
      "Best model: Epoch 8 \t loss=0.019526 \t val_loss=0.018299 \t time=1.12s\n",
      "Best model: Epoch 9 \t loss=0.019129 \t val_loss=0.017967 \t time=0.76s\n",
      "Best model: Epoch 11 \t loss=0.018860 \t val_loss=0.017541 \t time=0.76s\n",
      "Best model: Epoch 12 \t loss=0.018034 \t val_loss=0.017256 \t time=0.76s\n",
      "Best model: Epoch 13 \t loss=0.017711 \t val_loss=0.017202 \t time=0.88s\n",
      "Best model: Epoch 14 \t loss=0.017492 \t val_loss=0.016948 \t time=0.84s\n",
      "Best model: Epoch 15 \t loss=0.017307 \t val_loss=0.016830 \t time=0.80s\n",
      "Best model: Epoch 16 \t loss=0.017385 \t val_loss=0.016827 \t time=0.76s\n",
      "Best model: Epoch 17 \t loss=0.016998 \t val_loss=0.016789 \t time=0.77s\n",
      "Best model: Epoch 18 \t loss=0.016722 \t val_loss=0.016612 \t time=0.78s\n",
      "Best model: Epoch 19 \t loss=0.016558 \t val_loss=0.016508 \t time=0.77s\n",
      "Best model: Epoch 20 \t loss=0.016337 \t val_loss=0.016418 \t time=0.78s\n",
      "Best model: Epoch 21 \t loss=0.016235 \t val_loss=0.016326 \t time=1.10s\n",
      "Best model: Epoch 22 \t loss=0.016127 \t val_loss=0.016278 \t time=0.77s\n",
      "Best model: Epoch 25 \t loss=0.015683 \t val_loss=0.016225 \t time=0.78s\n",
      "Best model: Epoch 28 \t loss=0.015565 \t val_loss=0.016198 \t time=0.80s\n",
      "Best model: Epoch 29 \t loss=0.015424 \t val_loss=0.016165 \t time=0.77s\n",
      "Best model: Epoch 31 \t loss=0.015384 \t val_loss=0.016151 \t time=0.77s\n",
      "Best model: Epoch 35 \t loss=0.015113 \t val_loss=0.016146 \t time=0.77s\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 40 \t loss=0.014338 \t val_loss=0.016034 \t time=0.86s\n",
      "Fold 4 log loss: 0.016093105821003578\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.413252 \t val_loss=0.074956 \t time=0.82s\n",
      "Best model: Epoch 2 \t loss=0.048736 \t val_loss=0.028345 \t time=0.78s\n",
      "Best model: Epoch 3 \t loss=0.027761 \t val_loss=0.022420 \t time=0.88s\n",
      "Best model: Epoch 4 \t loss=0.023178 \t val_loss=0.021009 \t time=1.12s\n",
      "Best model: Epoch 5 \t loss=0.021552 \t val_loss=0.019836 \t time=0.83s\n",
      "Best model: Epoch 6 \t loss=0.020723 \t val_loss=0.019424 \t time=0.79s\n",
      "Best model: Epoch 7 \t loss=0.020016 \t val_loss=0.019338 \t time=0.80s\n",
      "Best model: Epoch 8 \t loss=0.019493 \t val_loss=0.018481 \t time=0.97s\n",
      "Best model: Epoch 9 \t loss=0.018870 \t val_loss=0.018145 \t time=0.85s\n",
      "Best model: Epoch 11 \t loss=0.018279 \t val_loss=0.017718 \t time=0.81s\n",
      "Best model: Epoch 12 \t loss=0.017971 \t val_loss=0.017576 \t time=0.79s\n",
      "Best model: Epoch 13 \t loss=0.017682 \t val_loss=0.017317 \t time=0.79s\n",
      "Best model: Epoch 14 \t loss=0.017413 \t val_loss=0.017172 \t time=1.11s\n",
      "Best model: Epoch 15 \t loss=0.017277 \t val_loss=0.017068 \t time=0.80s\n",
      "Best model: Epoch 16 \t loss=0.017148 \t val_loss=0.017005 \t time=0.80s\n",
      "Best model: Epoch 17 \t loss=0.016898 \t val_loss=0.016945 \t time=0.79s\n",
      "Best model: Epoch 18 \t loss=0.016702 \t val_loss=0.016738 \t time=0.79s\n",
      "Best model: Epoch 19 \t loss=0.016448 \t val_loss=0.016712 \t time=0.79s\n",
      "Best model: Epoch 21 \t loss=0.016312 \t val_loss=0.016602 \t time=1.00s\n",
      "Best model: Epoch 23 \t loss=0.015978 \t val_loss=0.016516 \t time=0.84s\n",
      "Best model: Epoch 24 \t loss=0.015868 \t val_loss=0.016480 \t time=0.84s\n",
      "Best model: Epoch 26 \t loss=0.015581 \t val_loss=0.016478 \t time=0.84s\n",
      "Best model: Epoch 27 \t loss=0.015653 \t val_loss=0.016433 \t time=0.83s\n",
      "Best model: Epoch 28 \t loss=0.015518 \t val_loss=0.016433 \t time=0.79s\n",
      "Best model: Epoch 29 \t loss=0.015479 \t val_loss=0.016413 \t time=0.78s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014591 \t val_loss=0.016284 \t time=1.00s\n",
      "Best model: Epoch 35 \t loss=0.014365 \t val_loss=0.016185 \t time=0.79s\n",
      "Best model: Epoch 36 \t loss=0.014121 \t val_loss=0.016150 \t time=0.78s\n",
      "Best model: Epoch 38 \t loss=0.013806 \t val_loss=0.016146 \t time=0.78s\n",
      "Best model: Epoch 40 \t loss=0.013604 \t val_loss=0.016141 \t time=0.80s\n",
      "Fold 5 log loss: 0.016142314651801725\n",
      "Seed 4\n",
      "Fold 1 log loss: 0.016174484102574663\n",
      "Fold 2 log loss: 0.016047175759869827\n",
      "Fold 3 log loss: 0.015928155998706423\n",
      "Fold 4 log loss: 0.016093105821003578\n",
      "Fold 5 log loss: 0.016142314651801725\n",
      "Std of log loss: 8.610756333173562e-05\n",
      "Total log loss: 0.016077045654075823\n"
     ]
    }
   ],
   "source": [
    "seeds = [0,1,2,3,4]\n",
    "pytorch1_oof = np.zeros([len(fn_train),fn_targets.shape[1]])\n",
    "pytorch1_test = np.zeros([len(fn_test),fn_targets.shape[1]])\n",
    "\n",
    "for seed_ in seeds:\n",
    "    oof, oof_targets, pytorch_pred = modelling_torch(fn_train, fn_targets, fn_test, seed_, fn_train.shape[1], fn_targets.shape[1],1)\n",
    "    pytorch1_oof += oof / len(seeds)\n",
    "    pytorch1_test += pytorch_pred / len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T11:12:28.632745Z",
     "iopub.status.busy": "2020-10-10T11:12:28.631415Z",
     "iopub.status.idle": "2020-10-10T11:12:34.495213Z",
     "shell.execute_reply": "2020-10-10T11:12:34.494563Z"
    },
    "papermill": {
     "duration": 6.516778,
     "end_time": "2020-10-10T11:12:34.495342",
     "exception": false,
     "start_time": "2020-10-10T11:12:27.978564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.014671957119226644\n"
     ]
    }
   ],
   "source": [
    "check_pytorch1 = targets.copy()\n",
    "check_pytorch1.loc[cons_train_index,target_feats] = pytorch1_oof\n",
    "check_pytorch1.loc[noncons_train_index,target_feats] = 0\n",
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(np.array(check_pytorch1.iloc[:,1:]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.680063,
     "end_time": "2020-10-10T11:12:35.830489",
     "exception": false,
     "start_time": "2020-10-10T11:12:35.150426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1st Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T11:12:37.164632Z",
     "iopub.status.busy": "2020-10-10T11:12:37.163792Z",
     "iopub.status.idle": "2020-10-10T11:12:37.166846Z",
     "shell.execute_reply": "2020-10-10T11:12:37.166293Z"
    },
    "papermill": {
     "duration": 0.671123,
     "end_time": "2020-10-10T11:12:37.166948",
     "exception": false,
     "start_time": "2020-10-10T11:12:36.495825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "\n",
    "def log_loss_metric(y_true, y_pred):\n",
    "    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    loss = - np.mean(np.mean(y_true * np.log(y_pred_clip) + (1 - y_true) * np.log(1 - y_pred_clip), axis = 1))\n",
    "    return loss\n",
    "\n",
    "def modelling_lr(tr, ta, te):    \n",
    "    oof = np.zeros([len(tr),ta.shape[1]])\n",
    "    pred_value = np.zeros([te.shape[0], ta.shape[1]])\n",
    "    \n",
    "    mskf_lr = MultilabelStratifiedKFold(n_splits = N_SPLITS, random_state = 0, shuffle = True)\n",
    "    \n",
    "    for n, (train_index, val_index) in enumerate(mskf_lr.split(tr, ta)):\n",
    "        x_tr, x_val = tr[train_index], tr[val_index]\n",
    "        y_tr, y_val = ta[train_index], ta[val_index]\n",
    "        \n",
    "        model = KernelRidge(alpha = 80, kernel = 'rbf')\n",
    "        model.fit(x_tr, y_tr)\n",
    "\n",
    "        fold_pred = model.predict(x_val)\n",
    "        pred_value += model.predict(te) / N_SPLITS\n",
    "        oof[val_index,:] = fold_pred\n",
    "        fold_score = log_loss_metric(y_val, fold_pred)\n",
    "        print('KRR: Fold {} Score {}:'.format(n+1, fold_score))\n",
    "    return oof, pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T11:12:38.493625Z",
     "iopub.status.busy": "2020-10-10T11:12:38.492032Z",
     "iopub.status.idle": "2020-10-10T11:18:12.916303Z",
     "shell.execute_reply": "2020-10-10T11:18:12.915642Z"
    },
    "papermill": {
     "duration": 335.090133,
     "end_time": "2020-10-10T11:18:12.916418",
     "exception": false,
     "start_time": "2020-10-10T11:12:37.826285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KRR: Fold 1 Score 0.020053231410728768:\n",
      "KRR: Fold 2 Score 0.020395236493920596:\n",
      "KRR: Fold 3 Score 0.02040202134683225:\n",
      "KRR: Fold 4 Score 0.02020233106232731:\n",
      "KRR: Fold 5 Score 0.02046674791227364:\n"
     ]
    }
   ],
   "source": [
    "lr0_oof = np.zeros([len(fn_train), fn_targets.shape[1]])\n",
    "lr0_test = np.zeros([len(fn_test), fn_targets.shape[1]])\n",
    "lr0_oof, lr0_test = modelling_lr(fn_train, fn_targets, fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T11:18:14.313364Z",
     "iopub.status.busy": "2020-10-10T11:18:14.305567Z",
     "iopub.status.idle": "2020-10-10T11:19:17.253280Z",
     "shell.execute_reply": "2020-10-10T11:19:17.249011Z"
    },
    "papermill": {
     "duration": 63.644837,
     "end_time": "2020-10-10T11:19:17.253426",
     "exception": false,
     "start_time": "2020-10-10T11:18:13.608589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673880b270594b6bb9a04df90928a80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=206.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lr1_test = pd.read_csv('../input/lish-moa/sample_submission.csv')\n",
    "lr1_test.loc[:, target_feats] = 0\n",
    "lr1_oof = np.zeros([fn_targets.shape[0],fn_targets.shape[1]]) \n",
    "\n",
    "for ind in tqdm(range(len(target_feats))):\n",
    "\n",
    "    ind_target_sum = targets.drop(\"sig_id\", axis=1).copy().values[:, ind].sum()\n",
    "\n",
    "    if ind_target_sum >= N_SPLITS:\n",
    "\n",
    "        skf = StratifiedKFold(n_splits = N_SPLITS, random_state = 0, shuffle = True)\n",
    "        for n, (train_index, val_index) in enumerate(skf.split(lr0_oof, fn_targets[:,ind])):\n",
    "            x_tr, x_val = lr0_oof[train_index, ind].reshape(-1, 1), lr0_oof[val_index, ind].reshape(-1, 1)\n",
    "            y_tr, y_val = fn_targets[train_index,ind], fn_targets[val_index,ind]\n",
    "            model = LogisticRegression(penalty = 'none', max_iter = 1000)\n",
    "            model.fit(x_tr, y_tr)\n",
    "            \n",
    "            lr1_test.loc[cons_test_index, target_feats[ind]] += model.predict_proba(lr0_test[:, ind].reshape(-1, 1))[:, 1] / N_SPLITS\n",
    "            lr1_oof[val_index, ind] += model.predict_proba(x_val)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T11:19:18.717843Z",
     "iopub.status.busy": "2020-10-10T11:19:18.716424Z",
     "iopub.status.idle": "2020-10-10T11:19:19.184557Z",
     "shell.execute_reply": "2020-10-10T11:19:19.185463Z"
    },
    "papermill": {
     "duration": 1.171267,
     "end_time": "2020-10-10T11:19:19.185632",
     "exception": false,
     "start_time": "2020-10-10T11:19:18.014365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e29e0d7b0f63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheck_lr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcheck_lr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcons_train_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_feats\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres_lr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcheck_lr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnoncons_train_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_feats\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'OOF log loss: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_lr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'res_lr' is not defined"
     ]
    }
   ],
   "source": [
    "check_lr1 = targets.copy()\n",
    "check_lr1.loc[cons_train_index,target_feats] = res_lr\n",
    "check_lr1.loc[noncons_train_index,target_feats] = 0\n",
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(np.array(check_lr1.iloc[:,1:]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.708075,
     "end_time": "2020-10-10T11:19:20.607778",
     "exception": false,
     "start_time": "2020-10-10T11:19:19.899703",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1st SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.697418,
     "end_time": "2020-10-10T11:19:22.000552",
     "exception": false,
     "start_time": "2020-10-10T11:19:21.303134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.712602,
     "end_time": "2020-10-10T11:19:23.413163",
     "exception": false,
     "start_time": "2020-10-10T11:19:22.700561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T11:19:24.819261Z",
     "iopub.status.busy": "2020-10-10T11:19:24.817544Z",
     "iopub.status.idle": "2020-10-10T11:19:24.820110Z",
     "shell.execute_reply": "2020-10-10T11:19:24.820659Z"
    },
    "papermill": {
     "duration": 0.700612,
     "end_time": "2020-10-10T11:19:24.820818",
     "exception": false,
     "start_time": "2020-10-10T11:19:24.120206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weight optimization\n",
    "#class OptimizedRounder(object):\n",
    "#    def __init__(self, length):\n",
    "#        self.coef_ = [0 for in range(length)]\n",
    "\n",
    "#    def _log_loss(self, coef, Xs, y):\n",
    "#        X_p = np.zeros_like(Xs[0])\n",
    "#        for i in range(len(coef)):\n",
    "#            X_p += coef[i] * Xs[i]\n",
    "#        return log_loss(np.ravel(y), np.ravel(np.array(X_p)))\n",
    "    \n",
    "#    def fit(self, X, y, random_flg = False):\n",
    "#        loss_partial = partial(self._log_loss, X=X, y=y)\n",
    "#        if random_flg:\n",
    "#            initial_coef = [np.random.uniform(0.4,0.5), np.random.uniform(0.5,0.6), np.random.uniform(0.6,0.7)]\n",
    "#        else:\n",
    "#            initial_coef = [1/length for i in range(length)]\n",
    "#        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead') #Powell\n",
    "        \n",
    "#    def predict(self, X, coef):\n",
    "#        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "#    def coefficients(self):\n",
    "#        return self.coef_\n",
    "    \n",
    "#best_score = 100\n",
    "#for i in range(10):\n",
    "#    optR = OptimizedRounder()\n",
    "#    optR.fit(, y, random_flg=False)\n",
    "#    coefficients = optR.coefficients()\n",
    "#    score = qwk(new_train.accuracy_group, final_valid_pred)\n",
    "#    print(i, np.sort(coefficients), score)\n",
    "#    if score > best_score:\n",
    "#        best_score = score\n",
    "#        best_coefficients = coefficients\n",
    "#final_test_pred = pd.cut(np.array(test_exp_accuracy).reshape(-1,), [-np.inf] + list(np.sort(best_coefficients)) + [np.inf], labels = [0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T11:19:26.249509Z",
     "iopub.status.busy": "2020-10-10T11:19:26.248160Z",
     "iopub.status.idle": "2020-10-10T11:19:27.672604Z",
     "shell.execute_reply": "2020-10-10T11:19:27.669757Z"
    },
    "papermill": {
     "duration": 2.149585,
     "end_time": "2020-10-10T11:19:27.672779",
     "exception": false,
     "start_time": "2020-10-10T11:19:25.523194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.007991414664058021\n"
     ]
    }
   ],
   "source": [
    "check = 0.1 * check_lr1.iloc[:,1:] + 0.2 * check_xgb1.iloc[:,1:] + 0.7 * check_pytorch1.iloc[:,1:]\n",
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(np.array(check))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T11:19:29.316450Z",
     "iopub.status.busy": "2020-10-10T11:19:29.315521Z",
     "iopub.status.idle": "2020-10-10T11:19:33.042108Z",
     "shell.execute_reply": "2020-10-10T11:19:33.040575Z"
    },
    "papermill": {
     "duration": 4.445078,
     "end_time": "2020-10-10T11:19:33.042228",
     "exception": false,
     "start_time": "2020-10-10T11:19:28.597150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_torch = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "sub_xgb = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "sub_lr = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "\n",
    "sub_torch.loc[cons_test_index,target_feats] = pytorch1_test\n",
    "sub_torch.loc[noncons_test_index,target_feats] = 0\n",
    "sub_xgb.loc[cons_test_index,target_feats] = xgb1_test\n",
    "sub_xgb.loc[noncons_test_index,target_feats] = 0\n",
    "sub_lr.loc[cons_test_index,target_feats] = lr1_test\n",
    "sub_lr.loc[noncons_test_index,target_feats] = 0\n",
    "\n",
    "sub[target_feats] = 0.1 * sub_lr.iloc[:,1:] + 0.2 * sub_xgb.iloc[:,1:] + 0.7 * sub_torch.iloc[:,1:]\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.725325,
     "end_time": "2020-10-10T11:19:34.455179",
     "exception": false,
     "start_time": "2020-10-10T11:19:33.729854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 5363.95428,
   "end_time": "2020-10-10T11:19:36.771292",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-10T09:50:12.817012",
   "version": "2.1.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "25fb3400700d422794883e8e53bdf758": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5f236ccbb98c46e8892e1181ff9262e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "673880b270594b6bb9a04df90928a80d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c14f545c095549139c1ed94843744b7f",
        "IPY_MODEL_f8ebf727aa384fbc9f8d18e0f30ebad5"
       ],
       "layout": "IPY_MODEL_760c4ae653bc4da6805d4c3db61e4fbd"
      }
     },
     "760c4ae653bc4da6805d4c3db61e4fbd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab48594a146e4948bf74f6a585c186fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c14f545c095549139c1ed94843744b7f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_25fb3400700d422794883e8e53bdf758",
       "max": 206.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5f236ccbb98c46e8892e1181ff9262e0",
       "value": 206.0
      }
     },
     "cc06ae56642a4fc99e447d6039fd6a6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f8ebf727aa384fbc9f8d18e0f30ebad5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ab48594a146e4948bf74f6a585c186fb",
       "placeholder": "",
       "style": "IPY_MODEL_cc06ae56642a4fc99e447d6039fd6a6f",
       "value": " 206/206 [01:01&lt;00:00,  3.34it/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
