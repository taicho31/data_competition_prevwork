{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02552,
     "end_time": "2020-10-11T02:59:06.863835",
     "exception": false,
     "start_time": "2020-10-11T02:59:06.838315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- add svm\n",
    "- check only one xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T02:59:06.929098Z",
     "iopub.status.busy": "2020-10-11T02:59:06.928159Z",
     "iopub.status.idle": "2020-10-11T03:00:20.498246Z",
     "shell.execute_reply": "2020-10-11T03:00:20.499239Z"
    },
    "papermill": {
     "duration": 73.610557,
     "end_time": "2020-10-11T03:00:20.499446",
     "exception": false,
     "start_time": "2020-10-11T02:59:06.888889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!cp ../input/rapids/rapids.0.15.0 /opt/conda/envs/rapids.tar.gz\n",
    "!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path\n",
    "!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-10-11T03:00:36.664959Z",
     "iopub.status.busy": "2020-10-11T03:00:36.663799Z",
     "iopub.status.idle": "2020-10-11T03:00:50.572358Z",
     "shell.execute_reply": "2020-10-11T03:00:50.571037Z"
    },
    "papermill": {
     "duration": 13.958572,
     "end_time": "2020-10-11T03:00:50.572525",
     "exception": false,
     "start_time": "2020-10-11T03:00:36.613953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from category_encoders import CountEncoder\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "sys.path.append('../input/multilabelstraifier/')\n",
    "from ml_stratifiers import MultilabelStratifiedKFold\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from cuml.svm import SVC, SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025667,
     "end_time": "2020-10-11T03:00:50.624362",
     "exception": false,
     "start_time": "2020-10-11T03:00:50.598695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocess & Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-11T03:00:50.810571Z",
     "iopub.status.busy": "2020-10-11T03:00:50.801347Z",
     "iopub.status.idle": "2020-10-11T03:00:56.573659Z",
     "shell.execute_reply": "2020-10-11T03:00:56.572350Z"
    },
    "papermill": {
     "duration": 5.923509,
     "end_time": "2020-10-11T03:00:56.573850",
     "exception": false,
     "start_time": "2020-10-11T03:00:50.650341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/kaggle/input/lish-moa/'\n",
    "train = pd.read_csv(DATA_DIR + 'train_features.csv')\n",
    "targets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "test = pd.read_csv(DATA_DIR + 'test_features.csv')\n",
    "#non_targets = pd.read_csv(DATA_DIR + 'train_targets_nonscored.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T03:00:56.670973Z",
     "iopub.status.busy": "2020-10-11T03:00:56.670003Z",
     "iopub.status.idle": "2020-10-11T03:00:56.675860Z",
     "shell.execute_reply": "2020-10-11T03:00:56.675289Z"
    },
    "papermill": {
     "duration": 0.058069,
     "end_time": "2020-10-11T03:00:56.675980",
     "exception": false,
     "start_time": "2020-10-11T03:00:56.617911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_feats = [ i for i in targets.columns if i != \"sig_id\"]\n",
    "g_feats = [i for i in train.columns if \"g-\" in i]\n",
    "c_feats = [i for i in train.columns if \"c-\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T03:00:56.757059Z",
     "iopub.status.busy": "2020-10-11T03:00:56.756186Z",
     "iopub.status.idle": "2020-10-11T03:00:56.897434Z",
     "shell.execute_reply": "2020-10-11T03:00:56.896700Z"
    },
    "papermill": {
     "duration": 0.188735,
     "end_time": "2020-10-11T03:00:56.897570",
     "exception": false,
     "start_time": "2020-10-11T03:00:56.708835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noncons_train_index = train[train.cp_type==\"ctl_vehicle\"].index\n",
    "cons_train_index = train[train.cp_type!=\"ctl_vehicle\"].index\n",
    "noncons_test_index = test[test.cp_type==\"ctl_vehicle\"].index\n",
    "cons_test_index = test[test.cp_type!=\"ctl_vehicle\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T03:00:56.951309Z",
     "iopub.status.busy": "2020-10-11T03:00:56.950654Z",
     "iopub.status.idle": "2020-10-11T03:00:56.972209Z",
     "shell.execute_reply": "2020-10-11T03:00:56.972814Z"
    },
    "papermill": {
     "duration": 0.051389,
     "end_time": "2020-10-11T03:00:56.972969",
     "exception": false,
     "start_time": "2020-10-11T03:00:56.921580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = test[test.index.isin(cons_test_index)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T03:00:57.039532Z",
     "iopub.status.busy": "2020-10-11T03:00:57.038630Z",
     "iopub.status.idle": "2020-10-11T03:00:57.058468Z",
     "shell.execute_reply": "2020-10-11T03:00:57.057927Z"
    },
    "papermill": {
     "duration": 0.060896,
     "end_time": "2020-10-11T03:00:57.058620",
     "exception": false,
     "start_time": "2020-10-11T03:00:56.997724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "categoricals = [\"cp_dose\"]\n",
    "\n",
    "def encoding(tr, te):\n",
    "    for f in categoricals:\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(tr[f]))\n",
    "        tr[f] = lbl.transform(list(tr[f]))\n",
    "        te[f] = lbl.transform(list(te[f])) \n",
    "        \n",
    "    return tr, te\n",
    "\n",
    "train, test = encoding(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T03:00:57.117193Z",
     "iopub.status.busy": "2020-10-11T03:00:57.115818Z",
     "iopub.status.idle": "2020-10-11T03:00:57.384580Z",
     "shell.execute_reply": "2020-10-11T03:00:57.384053Z"
    },
    "papermill": {
     "duration": 0.300431,
     "end_time": "2020-10-11T03:00:57.384695",
     "exception": false,
     "start_time": "2020-10-11T03:00:57.084264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23814, 874) (3624, 874)\n"
     ]
    }
   ],
   "source": [
    "def fe(df, remove_features):\n",
    "    tmp = df.copy()\n",
    "    tmp.drop(remove_features, axis=1, inplace=True)\n",
    "    return tmp\n",
    "\n",
    "remove_features = [\"cp_type\" , \"sig_id\"]\n",
    "        \n",
    "train = fe(train, remove_features)\n",
    "test = fe(test, remove_features)\n",
    "    \n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024675,
     "end_time": "2020-10-11T03:00:57.435418",
     "exception": false,
     "start_time": "2020-10-11T03:00:57.410743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T03:00:57.497542Z",
     "iopub.status.busy": "2020-10-11T03:00:57.496087Z",
     "iopub.status.idle": "2020-10-11T03:00:58.629967Z",
     "shell.execute_reply": "2020-10-11T03:00:58.629169Z"
    },
    "papermill": {
     "duration": 1.169412,
     "end_time": "2020-10-11T03:00:58.630092",
     "exception": false,
     "start_time": "2020-10-11T03:00:57.460680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# xgb --------------------------\n",
    "X = train.copy()\n",
    "y = targets.drop(\"sig_id\", axis=1).copy()\n",
    "X_test = test.copy()\n",
    "\n",
    "# pytorch and logistic regression and svm-----------------------\n",
    "fn_train = train.copy() \n",
    "fn_test = test.copy() \n",
    "fn_targets = targets.drop(\"sig_id\", axis=1).copy()\n",
    "\n",
    "fn_train = fn_train[fn_train.index.isin(cons_train_index)].copy().reset_index(drop=True).to_numpy()\n",
    "fn_targets = fn_targets[fn_targets.index.isin(cons_train_index)].copy().reset_index(drop=True).to_numpy()\n",
    "\n",
    "ss = preprocessing.StandardScaler()\n",
    "fn_train= ss.fit_transform(fn_train)\n",
    "fn_test = ss.transform(fn_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025009,
     "end_time": "2020-10-11T03:00:58.681463",
     "exception": false,
     "start_time": "2020-10-11T03:00:58.656454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1st XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T03:00:58.751070Z",
     "iopub.status.busy": "2020-10-11T03:00:58.740824Z",
     "iopub.status.idle": "2020-10-11T03:00:58.774987Z",
     "shell.execute_reply": "2020-10-11T03:00:58.775450Z"
    },
    "papermill": {
     "duration": 0.069192,
     "end_time": "2020-10-11T03:00:58.775602",
     "exception": false,
     "start_time": "2020-10-11T03:00:58.706410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('classify',\n",
       "                 MultiOutputClassifier(estimator=XGBClassifier(base_score=None,\n",
       "                                                               booster=None,\n",
       "                                                               colsample_bylevel=None,\n",
       "                                                               colsample_bynode=None,\n",
       "                                                               colsample_bytree=None,\n",
       "                                                               gamma=3.6975,\n",
       "                                                               gpu_id=None,\n",
       "                                                               importance_type='gain',\n",
       "                                                               interaction_constraints=None,\n",
       "                                                               learning_rate=0.0703,\n",
       "                                                               max_delta_step=2.0706,\n",
       "                                                               max_depth=10,\n",
       "                                                               min_child_weight=31.58,\n",
       "                                                               missing=nan,\n",
       "                                                               monotone_constraints=None,\n",
       "                                                               n_estimators=166,\n",
       "                                                               n_jobs=None,\n",
       "                                                               num_parallel_tree=None,\n",
       "                                                               random_state=None,\n",
       "                                                               reg_alpha=None,\n",
       "                                                               reg_lambda=None,\n",
       "                                                               scale_pos_weight=None,\n",
       "                                                               subsample=None,\n",
       "                                                               tree_method='gpu_hist',\n",
       "                                                               validate_parameters=None,\n",
       "                                                               verbosity=None)))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultiOutputClassifier(XGBClassifier(tree_method='gpu_hist'))\n",
    "\n",
    "clf = Pipeline([('classify', classifier)\n",
    "               ])\n",
    "\n",
    "params = {'classify__estimator__gamma': 3.6975,\n",
    "          'classify__estimator__learning_rate': 0.0703,\n",
    "          'classify__estimator__max_delta_step': 2.0706,\n",
    "          'classify__estimator__max_depth': 10,\n",
    "          'classify__estimator__min_child_weight': 31.5800,\n",
    "          'classify__estimator__n_estimators': 166,\n",
    "         }\n",
    "\n",
    "clf.set_params(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T03:00:58.845081Z",
     "iopub.status.busy": "2020-10-11T03:00:58.841151Z",
     "iopub.status.idle": "2020-10-11T03:00:58.848020Z",
     "shell.execute_reply": "2020-10-11T03:00:58.847465Z"
    },
    "papermill": {
     "duration": 0.048076,
     "end_time": "2020-10-11T03:00:58.848118",
     "exception": false,
     "start_time": "2020-10-11T03:00:58.800042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modelling_xgb(X, y, X_test, seed):\n",
    "    NFOLDS=5\n",
    "    oof_preds = np.zeros(y.shape)\n",
    "    test_preds = np.zeros((X_test.shape[0], y.shape[1]))\n",
    "    oof_losses = []\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=NFOLDS, random_state=seed, shuffle=True)\n",
    "    for fn, (trn_idx, val_idx) in enumerate(mskf.split(X, y)):\n",
    "        print('Starting fold: ', fn)\n",
    "        X_train, X_val = X.iloc[trn_idx,:], X.iloc[val_idx,:].to_numpy()\n",
    "        y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx].to_numpy()\n",
    "    \n",
    "        X_train = X_train[X_train.index.isin(cons_train_index)].to_numpy()\n",
    "        y_train = y_train[y_train.index.isin(cons_train_index)].to_numpy()\n",
    "    \n",
    "        clf.fit(X_train, y_train)\n",
    "        val_preds = clf.predict_proba(X_val) # list of preds per class\n",
    "        val_preds = np.array(val_preds)[:,:,1].T # take the positive class\n",
    "        oof_preds[val_idx] = val_preds\n",
    "    \n",
    "        loss = log_loss(np.ravel(y_val), np.ravel(val_preds))\n",
    "        print(loss)\n",
    "        oof_losses.append(loss)\n",
    "        preds = clf.predict_proba(X_test)\n",
    "        preds = np.array(preds)[:,:,1].T # take the positive class\n",
    "        test_preds += preds / NFOLDS\n",
    "    \n",
    "    print(oof_losses)\n",
    "    print('Mean OOF loss across folds', np.mean(oof_losses))\n",
    "    print('STD OOF loss across folds', np.std(oof_losses))\n",
    "    return oof_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T03:00:58.907652Z",
     "iopub.status.busy": "2020-10-11T03:00:58.906487Z",
     "iopub.status.idle": "2020-10-11T03:26:13.619523Z",
     "shell.execute_reply": "2020-10-11T03:26:13.620356Z"
    },
    "papermill": {
     "duration": 1514.746328,
     "end_time": "2020-10-11T03:26:13.620601",
     "exception": false,
     "start_time": "2020-10-11T03:00:58.874273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold:  0\n",
      "0.016738832513000318\n",
      "Starting fold:  1\n",
      "0.016908957230028557\n",
      "Starting fold:  2\n",
      "0.016773734659711288\n",
      "Starting fold:  3\n",
      "0.016497753560878757\n",
      "Starting fold:  4\n",
      "0.01667905765085504\n",
      "[0.016738832513000318, 0.016908957230028557, 0.016773734659711288, 0.016497753560878757, 0.01667905765085504]\n",
      "Mean OOF loss across folds 0.01671966712289479\n",
      "STD OOF loss across folds 0.00013417608848077545\n"
     ]
    }
   ],
   "source": [
    "seeds = [42]#,43,44]\n",
    "xgb1_oof = np.zeros(y.shape)\n",
    "xgb1_test = np.zeros((test.shape[0], y.shape[1]))\n",
    "for seed_ in seeds:\n",
    "    ind_preds, ind_test_preds = modelling_xgb(X, y, X_test, seed_)\n",
    "    xgb1_oof += ind_preds / len(seeds)\n",
    "    xgb1_test += ind_test_preds / len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T03:26:13.691280Z",
     "iopub.status.busy": "2020-10-11T03:26:13.689739Z",
     "iopub.status.idle": "2020-10-11T03:26:14.973294Z",
     "shell.execute_reply": "2020-10-11T03:26:14.972383Z"
    },
    "papermill": {
     "duration": 1.321178,
     "end_time": "2020-10-11T03:26:14.973419",
     "exception": false,
     "start_time": "2020-10-11T03:26:13.652241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.016449612618380825\n"
     ]
    }
   ],
   "source": [
    "check_xgb1 = np.zeros([targets.shape[0], targets.shape[1]-1])\n",
    "check_xgb1[:,:] = xgb1_oof\n",
    "check_xgb1[noncons_train_index,:] = 0\n",
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(check_xgb1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03196,
     "end_time": "2020-10-11T03:26:15.037223",
     "exception": false,
     "start_time": "2020-10-11T03:26:15.005263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1st NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T03:26:15.137927Z",
     "iopub.status.busy": "2020-10-11T03:26:15.122099Z",
     "iopub.status.idle": "2020-10-11T03:26:15.172890Z",
     "shell.execute_reply": "2020-10-11T03:26:15.172320Z"
    },
    "papermill": {
     "duration": 0.102715,
     "end_time": "2020-10-11T03:26:15.172999",
     "exception": false,
     "start_time": "2020-10-11T03:26:15.070284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "batch_size = 128\n",
    "train_epochs = 40\n",
    "n_folds=5\n",
    "\n",
    "def mean_log_loss(y_true, y_pred):\n",
    "    metrics = []\n",
    "    for i, target in enumerate(target_feats):\n",
    "        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n",
    "    return np.mean(metrics)\n",
    "\n",
    "def seed_everything(seed=1234): \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "class MoaModel(nn.Module):\n",
    "    def __init__(self, num_columns, last_columns_num):\n",
    "        super(MoaModel, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_columns)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_columns, 2048))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(2048)\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(2048, 1048))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(1048)\n",
    "        self.dropout3 = nn.Dropout(0.6)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(1048, last_columns_num))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def modelling_torch(tr, target, te, sample_seed, init_num, last_num, layer):\n",
    "    seed_everything(seed=sample_seed) \n",
    "    X_train = tr.copy()\n",
    "    y_train = target.copy()\n",
    "    X_test = te.copy()\n",
    "    test_len = X_test.shape[0]\n",
    "\n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=2)\n",
    "    models = []\n",
    "    \n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    X_test = torch.utils.data.TensorDataset(X_test) \n",
    "    test_loader = torch.utils.data.DataLoader(X_test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    pred_value = np.zeros([test_len, y_train.shape[1]])\n",
    "    scores = []\n",
    "    for fold, (train_index, valid_index) in enumerate(mskf.split(X_train, y_train)):\n",
    "        print(\"Fold \"+str(fold+1))\n",
    "        X_train2 = torch.tensor(X_train[train_index,:], dtype=torch.float32)\n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "\n",
    "        X_valid2 = torch.tensor(X_train[valid_index,:], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "            \n",
    "        clf = MoaModel(init_num, last_num)\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "        optimizer = optim.Adam(clf.parameters(), lr = 0.001, weight_decay=1e-5) \n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, eps=1e-4, verbose=True)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2, y_valid2)\n",
    "        \n",
    "        clf.to(device)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        stop_counts = 0\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            clf.train()\n",
    "            avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader) \n",
    "    \n",
    "            clf.eval()\n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        \n",
    "            elapsed_time = time.time() - start_time \n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                stop_counts = 0\n",
    "                best_val_loss = avg_val_loss\n",
    "                print('Best model: Epoch {} \\t loss={:.6f} \\t val_loss={:.6f} \\t time={:.2f}s'.format(\n",
    "                    epoch + 1, avg_loss, avg_val_loss, elapsed_time))\n",
    "                torch.save(clf.state_dict(), 'best-model-parameters.pt')\n",
    "            else:\n",
    "                stop_counts += 1\n",
    "         \n",
    "        pred_model = MoaModel(init_num, last_num)\n",
    "        pred_model.load_state_dict(torch.load('best-model-parameters.pt'))\n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                y_pred = pred_model(x_batch).sigmoid().detach()\n",
    "                oof_epoch[i * batch_size:(i+1) * batch_size,:] = y_pred.cpu().numpy()\n",
    "                target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        print(\"Fold {} log loss: {}\".format(fold+1, mean_log_loss(target_epoch, oof_epoch)))\n",
    "        scores.append(mean_log_loss(target_epoch, oof_epoch))\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "        # test predcition --------------\n",
    "        test_preds = np.zeros([test_len, y_train.shape[1]])\n",
    "        for i, (x_batch,) in enumerate(test_loader): \n",
    "            y_pred = pred_model(x_batch).sigmoid().detach()\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred.cpu().numpy()\n",
    "        pred_value += test_preds / n_folds\n",
    "        # ------------------------------\n",
    "        \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    for i, ele in enumerate(scores):\n",
    "        print(\"Fold {} log loss: {}\".format(i+1, scores[i]))\n",
    "    print(\"Std of log loss: {}\".format(np.std(scores)))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "    \n",
    "    return oof, oof_targets, pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T03:26:15.246433Z",
     "iopub.status.busy": "2020-10-11T03:26:15.244793Z",
     "iopub.status.idle": "2020-10-11T03:44:01.120216Z",
     "shell.execute_reply": "2020-10-11T03:44:01.121009Z"
    },
    "papermill": {
     "duration": 1065.917047,
     "end_time": "2020-10-11T03:44:01.121241",
     "exception": false,
     "start_time": "2020-10-11T03:26:15.204194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.412591 \t val_loss=0.077221 \t time=2.12s\n",
      "Best model: Epoch 2 \t loss=0.048580 \t val_loss=0.028151 \t time=0.89s\n",
      "Best model: Epoch 3 \t loss=0.027003 \t val_loss=0.022851 \t time=0.88s\n",
      "Best model: Epoch 4 \t loss=0.023460 \t val_loss=0.021035 \t time=0.88s\n",
      "Best model: Epoch 5 \t loss=0.021372 \t val_loss=0.019930 \t time=0.89s\n",
      "Best model: Epoch 6 \t loss=0.020710 \t val_loss=0.019312 \t time=0.89s\n",
      "Best model: Epoch 7 \t loss=0.020091 \t val_loss=0.019111 \t time=0.91s\n",
      "Best model: Epoch 8 \t loss=0.019413 \t val_loss=0.018614 \t time=0.91s\n",
      "Best model: Epoch 9 \t loss=0.018994 \t val_loss=0.018114 \t time=0.88s\n",
      "Best model: Epoch 10 \t loss=0.018733 \t val_loss=0.017949 \t time=0.91s\n",
      "Best model: Epoch 11 \t loss=0.018251 \t val_loss=0.017597 \t time=0.89s\n",
      "Best model: Epoch 13 \t loss=0.017651 \t val_loss=0.017239 \t time=0.98s\n",
      "Best model: Epoch 14 \t loss=0.017350 \t val_loss=0.017176 \t time=0.89s\n",
      "Best model: Epoch 15 \t loss=0.017291 \t val_loss=0.017116 \t time=0.88s\n",
      "Best model: Epoch 16 \t loss=0.017107 \t val_loss=0.016993 \t time=0.90s\n",
      "Best model: Epoch 17 \t loss=0.016768 \t val_loss=0.016788 \t time=0.89s\n",
      "Best model: Epoch 18 \t loss=0.016622 \t val_loss=0.016715 \t time=0.93s\n",
      "Best model: Epoch 21 \t loss=0.016251 \t val_loss=0.016576 \t time=1.30s\n",
      "Best model: Epoch 22 \t loss=0.016066 \t val_loss=0.016500 \t time=0.91s\n",
      "Best model: Epoch 24 \t loss=0.015943 \t val_loss=0.016470 \t time=1.58s\n",
      "Best model: Epoch 25 \t loss=0.015734 \t val_loss=0.016424 \t time=0.95s\n",
      "Best model: Epoch 28 \t loss=0.015489 \t val_loss=0.016377 \t time=0.88s\n",
      "Best model: Epoch 31 \t loss=0.015327 \t val_loss=0.016313 \t time=0.93s\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 36 \t loss=0.014506 \t val_loss=0.016155 \t time=0.90s\n",
      "Best model: Epoch 37 \t loss=0.014171 \t val_loss=0.016128 \t time=0.88s\n",
      "Best model: Epoch 38 \t loss=0.014019 \t val_loss=0.016098 \t time=0.89s\n",
      "Best model: Epoch 39 \t loss=0.013838 \t val_loss=0.016042 \t time=0.88s\n",
      "Fold 1 log loss: 0.016144690589180097\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.414173 \t val_loss=0.076114 \t time=0.89s\n",
      "Best model: Epoch 2 \t loss=0.048296 \t val_loss=0.029272 \t time=0.90s\n",
      "Best model: Epoch 3 \t loss=0.027405 \t val_loss=0.022898 \t time=1.14s\n",
      "Best model: Epoch 4 \t loss=0.023185 \t val_loss=0.021045 \t time=0.90s\n",
      "Best model: Epoch 5 \t loss=0.021348 \t val_loss=0.019655 \t time=0.92s\n",
      "Best model: Epoch 6 \t loss=0.020497 \t val_loss=0.019208 \t time=0.91s\n",
      "Best model: Epoch 7 \t loss=0.019868 \t val_loss=0.018997 \t time=0.90s\n",
      "Best model: Epoch 8 \t loss=0.019279 \t val_loss=0.018520 \t time=0.90s\n",
      "Best model: Epoch 9 \t loss=0.018965 \t val_loss=0.017982 \t time=0.89s\n",
      "Best model: Epoch 10 \t loss=0.018675 \t val_loss=0.017759 \t time=0.90s\n",
      "Best model: Epoch 11 \t loss=0.018200 \t val_loss=0.017652 \t time=0.91s\n",
      "Best model: Epoch 12 \t loss=0.017911 \t val_loss=0.017377 \t time=0.90s\n",
      "Best model: Epoch 13 \t loss=0.017592 \t val_loss=0.017221 \t time=0.92s\n",
      "Best model: Epoch 15 \t loss=0.017180 \t val_loss=0.017090 \t time=1.03s\n",
      "Best model: Epoch 16 \t loss=0.017055 \t val_loss=0.016951 \t time=0.88s\n",
      "Best model: Epoch 17 \t loss=0.016813 \t val_loss=0.016760 \t time=0.91s\n",
      "Best model: Epoch 18 \t loss=0.016564 \t val_loss=0.016719 \t time=0.97s\n",
      "Best model: Epoch 19 \t loss=0.016340 \t val_loss=0.016593 \t time=0.94s\n",
      "Best model: Epoch 20 \t loss=0.016182 \t val_loss=0.016538 \t time=0.94s\n",
      "Best model: Epoch 21 \t loss=0.016051 \t val_loss=0.016499 \t time=0.95s\n",
      "Best model: Epoch 22 \t loss=0.016053 \t val_loss=0.016487 \t time=0.93s\n",
      "Best model: Epoch 24 \t loss=0.015813 \t val_loss=0.016389 \t time=0.88s\n",
      "Best model: Epoch 25 \t loss=0.015707 \t val_loss=0.016370 \t time=1.09s\n",
      "Best model: Epoch 26 \t loss=0.015672 \t val_loss=0.016358 \t time=0.91s\n",
      "Best model: Epoch 27 \t loss=0.015543 \t val_loss=0.016294 \t time=0.89s\n",
      "Best model: Epoch 28 \t loss=0.015430 \t val_loss=0.016279 \t time=0.92s\n",
      "Best model: Epoch 29 \t loss=0.015341 \t val_loss=0.016245 \t time=0.88s\n",
      "Best model: Epoch 30 \t loss=0.015306 \t val_loss=0.016228 \t time=0.90s\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 35 \t loss=0.014418 \t val_loss=0.016175 \t time=0.88s\n",
      "Best model: Epoch 36 \t loss=0.014187 \t val_loss=0.016067 \t time=0.89s\n",
      "Best model: Epoch 37 \t loss=0.013917 \t val_loss=0.016034 \t time=1.13s\n",
      "Best model: Epoch 38 \t loss=0.013762 \t val_loss=0.016030 \t time=1.15s\n",
      "Best model: Epoch 39 \t loss=0.013603 \t val_loss=0.015983 \t time=1.02s\n",
      "Fold 2 log loss: 0.016044396281569673\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.413986 \t val_loss=0.076896 \t time=1.05s\n",
      "Best model: Epoch 2 \t loss=0.049192 \t val_loss=0.028994 \t time=0.92s\n",
      "Best model: Epoch 3 \t loss=0.027106 \t val_loss=0.022515 \t time=0.89s\n",
      "Best model: Epoch 4 \t loss=0.023059 \t val_loss=0.020760 \t time=0.88s\n",
      "Best model: Epoch 5 \t loss=0.021690 \t val_loss=0.020538 \t time=1.71s\n",
      "Best model: Epoch 6 \t loss=0.020625 \t val_loss=0.019409 \t time=0.97s\n",
      "Best model: Epoch 7 \t loss=0.019750 \t val_loss=0.018819 \t time=0.93s\n",
      "Best model: Epoch 8 \t loss=0.019329 \t val_loss=0.018315 \t time=0.91s\n",
      "Best model: Epoch 9 \t loss=0.019057 \t val_loss=0.018125 \t time=0.89s\n",
      "Best model: Epoch 10 \t loss=0.018609 \t val_loss=0.017987 \t time=0.91s\n",
      "Best model: Epoch 11 \t loss=0.018311 \t val_loss=0.017812 \t time=0.89s\n",
      "Best model: Epoch 12 \t loss=0.018039 \t val_loss=0.017600 \t time=0.90s\n",
      "Best model: Epoch 13 \t loss=0.017822 \t val_loss=0.017359 \t time=0.92s\n",
      "Best model: Epoch 14 \t loss=0.017500 \t val_loss=0.017197 \t time=1.00s\n",
      "Best model: Epoch 15 \t loss=0.017385 \t val_loss=0.017085 \t time=1.03s\n",
      "Best model: Epoch 16 \t loss=0.017066 \t val_loss=0.016969 \t time=0.89s\n",
      "Best model: Epoch 17 \t loss=0.016860 \t val_loss=0.016758 \t time=0.89s\n",
      "Best model: Epoch 18 \t loss=0.016605 \t val_loss=0.016644 \t time=0.92s\n",
      "Best model: Epoch 19 \t loss=0.016521 \t val_loss=0.016627 \t time=0.89s\n",
      "Best model: Epoch 21 \t loss=0.016267 \t val_loss=0.016498 \t time=0.89s\n",
      "Best model: Epoch 23 \t loss=0.015995 \t val_loss=0.016465 \t time=0.89s\n",
      "Best model: Epoch 24 \t loss=0.015823 \t val_loss=0.016417 \t time=0.90s\n",
      "Best model: Epoch 25 \t loss=0.015703 \t val_loss=0.016356 \t time=0.90s\n",
      "Best model: Epoch 26 \t loss=0.015659 \t val_loss=0.016288 \t time=1.08s\n",
      "Best model: Epoch 30 \t loss=0.015393 \t val_loss=0.016223 \t time=0.89s\n",
      "Best model: Epoch 33 \t loss=0.015215 \t val_loss=0.016222 \t time=0.90s\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 35 \t loss=0.014525 \t val_loss=0.016054 \t time=0.88s\n",
      "Best model: Epoch 36 \t loss=0.014278 \t val_loss=0.015991 \t time=0.91s\n",
      "Best model: Epoch 37 \t loss=0.014126 \t val_loss=0.015968 \t time=0.98s\n",
      "Best model: Epoch 38 \t loss=0.013885 \t val_loss=0.015964 \t time=1.21s\n",
      "Best model: Epoch 39 \t loss=0.013728 \t val_loss=0.015933 \t time=1.05s\n",
      "Fold 3 log loss: 0.015932693904937074\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.414550 \t val_loss=0.077203 \t time=0.89s\n",
      "Best model: Epoch 2 \t loss=0.050157 \t val_loss=0.030820 \t time=0.88s\n",
      "Best model: Epoch 3 \t loss=0.028654 \t val_loss=0.022643 \t time=0.89s\n",
      "Best model: Epoch 4 \t loss=0.022900 \t val_loss=0.020759 \t time=0.92s\n",
      "Best model: Epoch 5 \t loss=0.021488 \t val_loss=0.019617 \t time=0.89s\n",
      "Best model: Epoch 6 \t loss=0.020599 \t val_loss=0.019256 \t time=1.08s\n",
      "Best model: Epoch 7 \t loss=0.020185 \t val_loss=0.018790 \t time=0.91s\n",
      "Best model: Epoch 8 \t loss=0.019538 \t val_loss=0.018186 \t time=0.91s\n",
      "Best model: Epoch 9 \t loss=0.019437 \t val_loss=0.017978 \t time=0.89s\n",
      "Best model: Epoch 10 \t loss=0.018881 \t val_loss=0.017777 \t time=0.90s\n",
      "Best model: Epoch 11 \t loss=0.018440 \t val_loss=0.017634 \t time=0.90s\n",
      "Best model: Epoch 12 \t loss=0.018149 \t val_loss=0.017331 \t time=0.91s\n",
      "Best model: Epoch 13 \t loss=0.017822 \t val_loss=0.017247 \t time=1.02s\n",
      "Best model: Epoch 14 \t loss=0.017593 \t val_loss=0.017178 \t time=1.18s\n",
      "Best model: Epoch 15 \t loss=0.017360 \t val_loss=0.017018 \t time=0.89s\n",
      "Best model: Epoch 16 \t loss=0.017128 \t val_loss=0.016771 \t time=0.90s\n",
      "Best model: Epoch 18 \t loss=0.016888 \t val_loss=0.016649 \t time=0.95s\n",
      "Best model: Epoch 19 \t loss=0.016572 \t val_loss=0.016569 \t time=1.11s\n",
      "Best model: Epoch 20 \t loss=0.016374 \t val_loss=0.016445 \t time=0.91s\n",
      "Best model: Epoch 22 \t loss=0.016243 \t val_loss=0.016368 \t time=0.87s\n",
      "Best model: Epoch 23 \t loss=0.016123 \t val_loss=0.016358 \t time=0.93s\n",
      "Best model: Epoch 25 \t loss=0.015849 \t val_loss=0.016256 \t time=0.96s\n",
      "Best model: Epoch 27 \t loss=0.015719 \t val_loss=0.016254 \t time=0.90s\n",
      "Best model: Epoch 28 \t loss=0.015601 \t val_loss=0.016216 \t time=1.13s\n",
      "Best model: Epoch 29 \t loss=0.015491 \t val_loss=0.016183 \t time=1.21s\n",
      "Best model: Epoch 30 \t loss=0.015505 \t val_loss=0.016177 \t time=0.92s\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 35 \t loss=0.014550 \t val_loss=0.016025 \t time=0.89s\n",
      "Best model: Epoch 36 \t loss=0.014275 \t val_loss=0.015951 \t time=0.89s\n",
      "Best model: Epoch 37 \t loss=0.014051 \t val_loss=0.015898 \t time=0.89s\n",
      "Best model: Epoch 39 \t loss=0.013728 \t val_loss=0.015897 \t time=1.08s\n",
      "Best model: Epoch 40 \t loss=0.013633 \t val_loss=0.015858 \t time=0.89s\n",
      "Fold 4 log loss: 0.01591978898403236\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.412187 \t val_loss=0.088095 \t time=0.91s\n",
      "Best model: Epoch 2 \t loss=0.049344 \t val_loss=0.028300 \t time=0.89s\n",
      "Best model: Epoch 3 \t loss=0.027639 \t val_loss=0.022942 \t time=0.88s\n",
      "Best model: Epoch 4 \t loss=0.023355 \t val_loss=0.020933 \t time=0.89s\n",
      "Best model: Epoch 5 \t loss=0.021415 \t val_loss=0.019995 \t time=0.90s\n",
      "Best model: Epoch 6 \t loss=0.020525 \t val_loss=0.019669 \t time=0.90s\n",
      "Best model: Epoch 7 \t loss=0.019796 \t val_loss=0.018820 \t time=0.89s\n",
      "Best model: Epoch 8 \t loss=0.019289 \t val_loss=0.018472 \t time=1.12s\n",
      "Best model: Epoch 9 \t loss=0.019185 \t val_loss=0.018166 \t time=0.90s\n",
      "Best model: Epoch 11 \t loss=0.018588 \t val_loss=0.017788 \t time=0.89s\n",
      "Best model: Epoch 12 \t loss=0.018107 \t val_loss=0.017631 \t time=0.90s\n",
      "Best model: Epoch 13 \t loss=0.017794 \t val_loss=0.017333 \t time=0.92s\n",
      "Best model: Epoch 14 \t loss=0.017391 \t val_loss=0.017204 \t time=0.89s\n",
      "Best model: Epoch 15 \t loss=0.017268 \t val_loss=0.017087 \t time=0.93s\n",
      "Best model: Epoch 16 \t loss=0.017022 \t val_loss=0.016977 \t time=0.89s\n",
      "Best model: Epoch 17 \t loss=0.016798 \t val_loss=0.016941 \t time=0.92s\n",
      "Best model: Epoch 18 \t loss=0.016580 \t val_loss=0.016778 \t time=0.91s\n",
      "Best model: Epoch 19 \t loss=0.016403 \t val_loss=0.016737 \t time=1.07s\n",
      "Best model: Epoch 21 \t loss=0.016202 \t val_loss=0.016591 \t time=1.03s\n",
      "Best model: Epoch 22 \t loss=0.016031 \t val_loss=0.016581 \t time=0.92s\n",
      "Best model: Epoch 23 \t loss=0.015834 \t val_loss=0.016542 \t time=0.90s\n",
      "Best model: Epoch 24 \t loss=0.015865 \t val_loss=0.016475 \t time=0.89s\n",
      "Best model: Epoch 27 \t loss=0.015609 \t val_loss=0.016414 \t time=0.87s\n",
      "Best model: Epoch 30 \t loss=0.015342 \t val_loss=0.016413 \t time=1.01s\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 32 \t loss=0.014680 \t val_loss=0.016273 \t time=1.30s\n",
      "Best model: Epoch 33 \t loss=0.014344 \t val_loss=0.016219 \t time=0.92s\n",
      "Best model: Epoch 34 \t loss=0.014130 \t val_loss=0.016189 \t time=0.90s\n",
      "Best model: Epoch 35 \t loss=0.014023 \t val_loss=0.016185 \t time=0.92s\n",
      "Best model: Epoch 36 \t loss=0.013851 \t val_loss=0.016143 \t time=0.91s\n",
      "Best model: Epoch 37 \t loss=0.013768 \t val_loss=0.016134 \t time=1.22s\n",
      "Best model: Epoch 40 \t loss=0.013329 \t val_loss=0.016125 \t time=0.90s\n",
      "Fold 5 log loss: 0.016124345038051664\n",
      "Seed 0\n",
      "Fold 1 log loss: 0.016144690589180097\n",
      "Fold 2 log loss: 0.016044396281569673\n",
      "Fold 3 log loss: 0.015932693904937074\n",
      "Fold 4 log loss: 0.01591978898403236\n",
      "Fold 5 log loss: 0.016124345038051664\n",
      "Std of log loss: 9.362313645021518e-05\n",
      "Total log loss: 0.016033178295101806\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.412054 \t val_loss=0.076325 \t time=1.02s\n",
      "Best model: Epoch 2 \t loss=0.048526 \t val_loss=0.028851 \t time=0.90s\n",
      "Best model: Epoch 3 \t loss=0.026873 \t val_loss=0.023145 \t time=0.88s\n",
      "Best model: Epoch 4 \t loss=0.023518 \t val_loss=0.020954 \t time=0.90s\n",
      "Best model: Epoch 5 \t loss=0.021069 \t val_loss=0.019802 \t time=0.88s\n",
      "Best model: Epoch 6 \t loss=0.020366 \t val_loss=0.019167 \t time=0.97s\n",
      "Best model: Epoch 7 \t loss=0.019715 \t val_loss=0.018802 \t time=1.34s\n",
      "Best model: Epoch 8 \t loss=0.019131 \t val_loss=0.018499 \t time=1.07s\n",
      "Best model: Epoch 9 \t loss=0.018863 \t val_loss=0.018232 \t time=0.94s\n",
      "Best model: Epoch 10 \t loss=0.018576 \t val_loss=0.017987 \t time=0.89s\n",
      "Best model: Epoch 11 \t loss=0.018388 \t val_loss=0.017902 \t time=0.90s\n",
      "Best model: Epoch 12 \t loss=0.018011 \t val_loss=0.017349 \t time=0.92s\n",
      "Best model: Epoch 13 \t loss=0.017622 \t val_loss=0.017302 \t time=0.90s\n",
      "Best model: Epoch 14 \t loss=0.017534 \t val_loss=0.017155 \t time=0.90s\n",
      "Best model: Epoch 16 \t loss=0.017059 \t val_loss=0.016986 \t time=0.90s\n",
      "Best model: Epoch 17 \t loss=0.016869 \t val_loss=0.016890 \t time=0.94s\n",
      "Best model: Epoch 18 \t loss=0.016664 \t val_loss=0.016821 \t time=1.07s\n",
      "Best model: Epoch 19 \t loss=0.016417 \t val_loss=0.016683 \t time=0.89s\n",
      "Best model: Epoch 20 \t loss=0.016225 \t val_loss=0.016591 \t time=0.89s\n",
      "Best model: Epoch 22 \t loss=0.015986 \t val_loss=0.016465 \t time=0.89s\n",
      "Best model: Epoch 24 \t loss=0.015784 \t val_loss=0.016374 \t time=0.92s\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 29 \t loss=0.014782 \t val_loss=0.016199 \t time=1.05s\n",
      "Best model: Epoch 30 \t loss=0.014521 \t val_loss=0.016131 \t time=1.02s\n",
      "Best model: Epoch 31 \t loss=0.014358 \t val_loss=0.016063 \t time=0.94s\n",
      "Best model: Epoch 32 \t loss=0.014177 \t val_loss=0.016035 \t time=0.98s\n",
      "Best model: Epoch 34 \t loss=0.013939 \t val_loss=0.016024 \t time=0.89s\n",
      "Best model: Epoch 35 \t loss=0.013725 \t val_loss=0.016020 \t time=0.89s\n",
      "Best model: Epoch 36 \t loss=0.013646 \t val_loss=0.016016 \t time=0.90s\n",
      "Best model: Epoch 40 \t loss=0.013190 \t val_loss=0.015981 \t time=0.89s\n",
      "Fold 1 log loss: 0.016079840868431054\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.414855 \t val_loss=0.075799 \t time=0.89s\n",
      "Best model: Epoch 2 \t loss=0.048749 \t val_loss=0.029421 \t time=0.89s\n",
      "Best model: Epoch 3 \t loss=0.027499 \t val_loss=0.023112 \t time=0.88s\n",
      "Best model: Epoch 4 \t loss=0.023323 \t val_loss=0.020927 \t time=1.28s\n",
      "Best model: Epoch 5 \t loss=0.021469 \t val_loss=0.019786 \t time=1.12s\n",
      "Best model: Epoch 6 \t loss=0.020582 \t val_loss=0.019286 \t time=0.89s\n",
      "Best model: Epoch 7 \t loss=0.020079 \t val_loss=0.018804 \t time=0.95s\n",
      "Best model: Epoch 8 \t loss=0.019293 \t val_loss=0.018348 \t time=1.11s\n",
      "Best model: Epoch 9 \t loss=0.019088 \t val_loss=0.018044 \t time=0.99s\n",
      "Best model: Epoch 10 \t loss=0.018684 \t val_loss=0.018009 \t time=0.91s\n",
      "Best model: Epoch 11 \t loss=0.018275 \t val_loss=0.017549 \t time=0.92s\n",
      "Best model: Epoch 12 \t loss=0.017908 \t val_loss=0.017357 \t time=0.92s\n",
      "Best model: Epoch 13 \t loss=0.017698 \t val_loss=0.017252 \t time=0.95s\n",
      "Best model: Epoch 14 \t loss=0.017397 \t val_loss=0.017156 \t time=0.92s\n",
      "Best model: Epoch 15 \t loss=0.017207 \t val_loss=0.016991 \t time=0.93s\n",
      "Best model: Epoch 16 \t loss=0.016897 \t val_loss=0.016768 \t time=0.92s\n",
      "Best model: Epoch 18 \t loss=0.016568 \t val_loss=0.016672 \t time=0.95s\n",
      "Best model: Epoch 19 \t loss=0.016429 \t val_loss=0.016570 \t time=1.08s\n",
      "Best model: Epoch 20 \t loss=0.016223 \t val_loss=0.016533 \t time=1.00s\n",
      "Best model: Epoch 21 \t loss=0.016095 \t val_loss=0.016416 \t time=0.90s\n",
      "Best model: Epoch 23 \t loss=0.015917 \t val_loss=0.016410 \t time=0.88s\n",
      "Best model: Epoch 24 \t loss=0.015726 \t val_loss=0.016333 \t time=0.97s\n",
      "Best model: Epoch 27 \t loss=0.015412 \t val_loss=0.016291 \t time=0.88s\n",
      "Best model: Epoch 29 \t loss=0.015315 \t val_loss=0.016222 \t time=0.90s\n",
      "Best model: Epoch 32 \t loss=0.015167 \t val_loss=0.016173 \t time=1.26s\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 37 \t loss=0.014376 \t val_loss=0.016120 \t time=0.89s\n",
      "Best model: Epoch 38 \t loss=0.014005 \t val_loss=0.016016 \t time=0.89s\n",
      "Best model: Epoch 40 \t loss=0.013573 \t val_loss=0.016004 \t time=0.90s\n",
      "Fold 2 log loss: 0.016064742658142592\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.415098 \t val_loss=0.080733 \t time=0.94s\n",
      "Best model: Epoch 2 \t loss=0.048767 \t val_loss=0.028509 \t time=0.94s\n",
      "Best model: Epoch 3 \t loss=0.026784 \t val_loss=0.023150 \t time=0.92s\n",
      "Best model: Epoch 4 \t loss=0.023161 \t val_loss=0.021295 \t time=0.89s\n",
      "Best model: Epoch 5 \t loss=0.021450 \t val_loss=0.019731 \t time=0.90s\n",
      "Best model: Epoch 6 \t loss=0.020675 \t val_loss=0.019469 \t time=0.89s\n",
      "Best model: Epoch 7 \t loss=0.020227 \t val_loss=0.019188 \t time=0.94s\n",
      "Best model: Epoch 8 \t loss=0.019444 \t val_loss=0.018675 \t time=0.89s\n",
      "Best model: Epoch 9 \t loss=0.019434 \t val_loss=0.018288 \t time=0.95s\n",
      "Best model: Epoch 10 \t loss=0.018643 \t val_loss=0.018101 \t time=1.00s\n",
      "Best model: Epoch 11 \t loss=0.018215 \t val_loss=0.017674 \t time=0.98s\n",
      "Best model: Epoch 12 \t loss=0.017846 \t val_loss=0.017324 \t time=0.92s\n",
      "Best model: Epoch 15 \t loss=0.017431 \t val_loss=0.017053 \t time=0.89s\n",
      "Best model: Epoch 16 \t loss=0.017095 \t val_loss=0.016751 \t time=0.90s\n",
      "Best model: Epoch 19 \t loss=0.016675 \t val_loss=0.016733 \t time=0.88s\n",
      "Best model: Epoch 21 \t loss=0.016288 \t val_loss=0.016438 \t time=0.88s\n",
      "Best model: Epoch 22 \t loss=0.016149 \t val_loss=0.016371 \t time=1.76s\n",
      "Best model: Epoch 24 \t loss=0.015809 \t val_loss=0.016315 \t time=0.90s\n",
      "Best model: Epoch 25 \t loss=0.015760 \t val_loss=0.016313 \t time=0.91s\n",
      "Best model: Epoch 26 \t loss=0.015703 \t val_loss=0.016287 \t time=1.16s\n",
      "Best model: Epoch 27 \t loss=0.015558 \t val_loss=0.016223 \t time=0.93s\n",
      "Best model: Epoch 29 \t loss=0.015499 \t val_loss=0.016220 \t time=0.92s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014648 \t val_loss=0.016064 \t time=0.93s\n",
      "Best model: Epoch 35 \t loss=0.014381 \t val_loss=0.016009 \t time=0.91s\n",
      "Best model: Epoch 36 \t loss=0.014148 \t val_loss=0.015964 \t time=0.93s\n",
      "Best model: Epoch 37 \t loss=0.014036 \t val_loss=0.015958 \t time=0.92s\n",
      "Best model: Epoch 38 \t loss=0.013845 \t val_loss=0.015945 \t time=0.94s\n",
      "Best model: Epoch 39 \t loss=0.013695 \t val_loss=0.015936 \t time=0.96s\n",
      "Best model: Epoch 40 \t loss=0.013596 \t val_loss=0.015928 \t time=0.90s\n",
      "Fold 3 log loss: 0.01592049020117655\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.413201 \t val_loss=0.077351 \t time=1.08s\n",
      "Best model: Epoch 2 \t loss=0.047845 \t val_loss=0.027291 \t time=0.88s\n",
      "Best model: Epoch 3 \t loss=0.027879 \t val_loss=0.022880 \t time=0.90s\n",
      "Best model: Epoch 4 \t loss=0.023083 \t val_loss=0.021099 \t time=0.90s\n",
      "Best model: Epoch 5 \t loss=0.021983 \t val_loss=0.020159 \t time=0.89s\n",
      "Best model: Epoch 6 \t loss=0.020838 \t val_loss=0.019249 \t time=0.89s\n",
      "Best model: Epoch 7 \t loss=0.019902 \t val_loss=0.018613 \t time=0.90s\n",
      "Best model: Epoch 8 \t loss=0.019397 \t val_loss=0.018178 \t time=0.90s\n",
      "Best model: Epoch 9 \t loss=0.018991 \t val_loss=0.017956 \t time=0.90s\n",
      "Best model: Epoch 10 \t loss=0.018744 \t val_loss=0.017936 \t time=0.93s\n",
      "Best model: Epoch 11 \t loss=0.018373 \t val_loss=0.017435 \t time=0.90s\n",
      "Best model: Epoch 12 \t loss=0.017988 \t val_loss=0.017253 \t time=1.11s\n",
      "Best model: Epoch 13 \t loss=0.017706 \t val_loss=0.017092 \t time=1.17s\n",
      "Best model: Epoch 14 \t loss=0.017475 \t val_loss=0.016959 \t time=0.90s\n",
      "Best model: Epoch 15 \t loss=0.017304 \t val_loss=0.016845 \t time=0.92s\n",
      "Best model: Epoch 16 \t loss=0.016985 \t val_loss=0.016721 \t time=0.89s\n",
      "Best model: Epoch 17 \t loss=0.016821 \t val_loss=0.016602 \t time=0.90s\n",
      "Best model: Epoch 18 \t loss=0.016629 \t val_loss=0.016580 \t time=0.90s\n",
      "Best model: Epoch 19 \t loss=0.016474 \t val_loss=0.016482 \t time=0.92s\n",
      "Best model: Epoch 20 \t loss=0.016306 \t val_loss=0.016427 \t time=0.90s\n",
      "Best model: Epoch 21 \t loss=0.016230 \t val_loss=0.016412 \t time=0.92s\n",
      "Best model: Epoch 22 \t loss=0.016012 \t val_loss=0.016284 \t time=0.89s\n",
      "Best model: Epoch 23 \t loss=0.015916 \t val_loss=0.016250 \t time=1.02s\n",
      "Best model: Epoch 26 \t loss=0.015605 \t val_loss=0.016187 \t time=0.88s\n",
      "Best model: Epoch 29 \t loss=0.015414 \t val_loss=0.016160 \t time=0.89s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014523 \t val_loss=0.016007 \t time=0.89s\n",
      "Best model: Epoch 35 \t loss=0.014326 \t val_loss=0.015953 \t time=1.12s\n",
      "Best model: Epoch 36 \t loss=0.014128 \t val_loss=0.015927 \t time=0.89s\n",
      "Best model: Epoch 37 \t loss=0.013961 \t val_loss=0.015913 \t time=0.89s\n",
      "Best model: Epoch 38 \t loss=0.013800 \t val_loss=0.015902 \t time=0.89s\n",
      "Best model: Epoch 39 \t loss=0.013707 \t val_loss=0.015887 \t time=0.89s\n",
      "Best model: Epoch 40 \t loss=0.013494 \t val_loss=0.015874 \t time=1.12s\n",
      "Fold 4 log loss: 0.01592111042134359\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.414129 \t val_loss=0.077634 \t time=0.92s\n",
      "Best model: Epoch 2 \t loss=0.048715 \t val_loss=0.028425 \t time=0.88s\n",
      "Best model: Epoch 3 \t loss=0.027408 \t val_loss=0.022374 \t time=1.41s\n",
      "Best model: Epoch 4 \t loss=0.023334 \t val_loss=0.020878 \t time=1.18s\n",
      "Best model: Epoch 5 \t loss=0.021927 \t val_loss=0.020042 \t time=0.96s\n",
      "Best model: Epoch 6 \t loss=0.020496 \t val_loss=0.019306 \t time=0.92s\n",
      "Best model: Epoch 7 \t loss=0.019769 \t val_loss=0.018887 \t time=0.99s\n",
      "Best model: Epoch 8 \t loss=0.019723 \t val_loss=0.018739 \t time=0.90s\n",
      "Best model: Epoch 9 \t loss=0.018921 \t val_loss=0.018396 \t time=0.93s\n",
      "Best model: Epoch 10 \t loss=0.018596 \t val_loss=0.017923 \t time=0.90s\n",
      "Best model: Epoch 11 \t loss=0.018246 \t val_loss=0.017664 \t time=0.92s\n",
      "Best model: Epoch 12 \t loss=0.018002 \t val_loss=0.017398 \t time=0.89s\n",
      "Best model: Epoch 13 \t loss=0.017603 \t val_loss=0.017307 \t time=0.89s\n",
      "Best model: Epoch 15 \t loss=0.017409 \t val_loss=0.017181 \t time=0.92s\n",
      "Best model: Epoch 16 \t loss=0.017084 \t val_loss=0.017008 \t time=0.88s\n",
      "Best model: Epoch 18 \t loss=0.016757 \t val_loss=0.016826 \t time=0.91s\n",
      "Best model: Epoch 19 \t loss=0.016483 \t val_loss=0.016648 \t time=0.96s\n",
      "Best model: Epoch 20 \t loss=0.016282 \t val_loss=0.016632 \t time=0.88s\n",
      "Best model: Epoch 22 \t loss=0.016100 \t val_loss=0.016561 \t time=0.88s\n",
      "Best model: Epoch 23 \t loss=0.015977 \t val_loss=0.016516 \t time=0.88s\n",
      "Best model: Epoch 24 \t loss=0.015867 \t val_loss=0.016485 \t time=0.89s\n",
      "Best model: Epoch 25 \t loss=0.015713 \t val_loss=0.016440 \t time=0.89s\n",
      "Best model: Epoch 28 \t loss=0.015600 \t val_loss=0.016433 \t time=0.96s\n",
      "Best model: Epoch 29 \t loss=0.015441 \t val_loss=0.016363 \t time=0.92s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014609 \t val_loss=0.016205 \t time=0.89s\n",
      "Best model: Epoch 35 \t loss=0.014249 \t val_loss=0.016198 \t time=0.90s\n",
      "Best model: Epoch 36 \t loss=0.014074 \t val_loss=0.016103 \t time=0.91s\n",
      "Best model: Epoch 37 \t loss=0.013971 \t val_loss=0.016069 \t time=1.11s\n",
      "Best model: Epoch 38 \t loss=0.013754 \t val_loss=0.016059 \t time=1.13s\n",
      "Fold 5 log loss: 0.016063688369179374\n",
      "Seed 1\n",
      "Fold 1 log loss: 0.016079840868431054\n",
      "Fold 2 log loss: 0.016064742658142592\n",
      "Fold 3 log loss: 0.01592049020117655\n",
      "Fold 4 log loss: 0.01592111042134359\n",
      "Fold 5 log loss: 0.016063688369179374\n",
      "Std of log loss: 7.303465552417108e-05\n",
      "Total log loss: 0.01600996956097101\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.414243 \t val_loss=0.075401 \t time=0.92s\n",
      "Best model: Epoch 2 \t loss=0.049196 \t val_loss=0.028064 \t time=0.90s\n",
      "Best model: Epoch 3 \t loss=0.026940 \t val_loss=0.022525 \t time=1.18s\n",
      "Best model: Epoch 4 \t loss=0.023264 \t val_loss=0.021226 \t time=0.90s\n",
      "Best model: Epoch 5 \t loss=0.021407 \t val_loss=0.019807 \t time=0.96s\n",
      "Best model: Epoch 6 \t loss=0.020540 \t val_loss=0.019256 \t time=0.89s\n",
      "Best model: Epoch 7 \t loss=0.019968 \t val_loss=0.018993 \t time=0.91s\n",
      "Best model: Epoch 8 \t loss=0.019501 \t val_loss=0.018829 \t time=0.89s\n",
      "Best model: Epoch 9 \t loss=0.019202 \t val_loss=0.018372 \t time=0.89s\n",
      "Best model: Epoch 10 \t loss=0.019079 \t val_loss=0.018125 \t time=0.90s\n",
      "Best model: Epoch 11 \t loss=0.018344 \t val_loss=0.017942 \t time=0.91s\n",
      "Best model: Epoch 12 \t loss=0.018138 \t val_loss=0.017556 \t time=0.90s\n",
      "Best model: Epoch 13 \t loss=0.017707 \t val_loss=0.017321 \t time=1.31s\n",
      "Best model: Epoch 14 \t loss=0.017455 \t val_loss=0.017192 \t time=1.41s\n",
      "Best model: Epoch 15 \t loss=0.017177 \t val_loss=0.017051 \t time=0.88s\n",
      "Best model: Epoch 16 \t loss=0.017017 \t val_loss=0.016919 \t time=0.90s\n",
      "Best model: Epoch 18 \t loss=0.016662 \t val_loss=0.016903 \t time=0.94s\n",
      "Best model: Epoch 19 \t loss=0.016553 \t val_loss=0.016804 \t time=0.94s\n",
      "Best model: Epoch 20 \t loss=0.016457 \t val_loss=0.016643 \t time=0.91s\n",
      "Best model: Epoch 21 \t loss=0.016250 \t val_loss=0.016637 \t time=0.90s\n",
      "Best model: Epoch 22 \t loss=0.016172 \t val_loss=0.016621 \t time=0.92s\n",
      "Best model: Epoch 23 \t loss=0.016010 \t val_loss=0.016544 \t time=0.90s\n",
      "Best model: Epoch 25 \t loss=0.015790 \t val_loss=0.016434 \t time=1.50s\n",
      "Best model: Epoch 26 \t loss=0.015725 \t val_loss=0.016409 \t time=1.06s\n",
      "Best model: Epoch 28 \t loss=0.015650 \t val_loss=0.016400 \t time=0.91s\n",
      "Best model: Epoch 29 \t loss=0.015497 \t val_loss=0.016379 \t time=0.91s\n",
      "Best model: Epoch 33 \t loss=0.015218 \t val_loss=0.016377 \t time=0.99s\n",
      "Best model: Epoch 35 \t loss=0.015280 \t val_loss=0.016351 \t time=0.98s\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 40 \t loss=0.014390 \t val_loss=0.016189 \t time=0.88s\n",
      "Fold 1 log loss: 0.016280616069086786\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.412084 \t val_loss=0.076539 \t time=0.88s\n",
      "Best model: Epoch 2 \t loss=0.049085 \t val_loss=0.028798 \t time=0.88s\n",
      "Best model: Epoch 3 \t loss=0.026930 \t val_loss=0.022483 \t time=0.88s\n",
      "Best model: Epoch 4 \t loss=0.023605 \t val_loss=0.021048 \t time=1.09s\n",
      "Best model: Epoch 5 \t loss=0.021499 \t val_loss=0.019946 \t time=0.90s\n",
      "Best model: Epoch 6 \t loss=0.020550 \t val_loss=0.019083 \t time=0.89s\n",
      "Best model: Epoch 7 \t loss=0.019963 \t val_loss=0.018862 \t time=0.88s\n",
      "Best model: Epoch 8 \t loss=0.019379 \t val_loss=0.018356 \t time=0.89s\n",
      "Best model: Epoch 9 \t loss=0.018966 \t val_loss=0.018103 \t time=0.91s\n",
      "Best model: Epoch 10 \t loss=0.018577 \t val_loss=0.017829 \t time=0.88s\n",
      "Best model: Epoch 12 \t loss=0.018119 \t val_loss=0.017426 \t time=0.88s\n",
      "Best model: Epoch 13 \t loss=0.017755 \t val_loss=0.017169 \t time=0.91s\n",
      "Best model: Epoch 15 \t loss=0.017197 \t val_loss=0.016995 \t time=0.89s\n",
      "Best model: Epoch 16 \t loss=0.016993 \t val_loss=0.016966 \t time=1.38s\n",
      "Best model: Epoch 17 \t loss=0.016980 \t val_loss=0.016896 \t time=0.99s\n",
      "Best model: Epoch 18 \t loss=0.016810 \t val_loss=0.016787 \t time=0.89s\n",
      "Best model: Epoch 19 \t loss=0.016601 \t val_loss=0.016749 \t time=0.89s\n",
      "Best model: Epoch 20 \t loss=0.016273 \t val_loss=0.016588 \t time=0.89s\n",
      "Best model: Epoch 21 \t loss=0.016209 \t val_loss=0.016517 \t time=0.94s\n",
      "Best model: Epoch 23 \t loss=0.015865 \t val_loss=0.016396 \t time=0.88s\n",
      "Best model: Epoch 25 \t loss=0.015726 \t val_loss=0.016323 \t time=0.89s\n",
      "Best model: Epoch 29 \t loss=0.015473 \t val_loss=0.016273 \t time=0.88s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014607 \t val_loss=0.016130 \t time=0.90s\n",
      "Best model: Epoch 35 \t loss=0.014302 \t val_loss=0.016075 \t time=1.15s\n",
      "Best model: Epoch 36 \t loss=0.014098 \t val_loss=0.016072 \t time=0.89s\n",
      "Best model: Epoch 37 \t loss=0.014008 \t val_loss=0.016029 \t time=0.92s\n",
      "Best model: Epoch 38 \t loss=0.013877 \t val_loss=0.016023 \t time=1.12s\n",
      "Best model: Epoch 40 \t loss=0.013495 \t val_loss=0.016019 \t time=0.94s\n",
      "Fold 2 log loss: 0.0160819916597495\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.411402 \t val_loss=0.077279 \t time=0.89s\n",
      "Best model: Epoch 2 \t loss=0.049547 \t val_loss=0.029069 \t time=0.91s\n",
      "Best model: Epoch 3 \t loss=0.027701 \t val_loss=0.022526 \t time=0.89s\n",
      "Best model: Epoch 4 \t loss=0.022974 \t val_loss=0.020914 \t time=0.94s\n",
      "Best model: Epoch 5 \t loss=0.021929 \t val_loss=0.020299 \t time=0.91s\n",
      "Best model: Epoch 6 \t loss=0.020641 \t val_loss=0.019507 \t time=0.99s\n",
      "Best model: Epoch 7 \t loss=0.020131 \t val_loss=0.019061 \t time=1.39s\n",
      "Best model: Epoch 9 \t loss=0.019336 \t val_loss=0.018609 \t time=0.90s\n",
      "Best model: Epoch 10 \t loss=0.018623 \t val_loss=0.017850 \t time=0.88s\n",
      "Best model: Epoch 12 \t loss=0.018119 \t val_loss=0.017799 \t time=0.87s\n",
      "Best model: Epoch 13 \t loss=0.017806 \t val_loss=0.017264 \t time=0.89s\n",
      "Best model: Epoch 15 \t loss=0.017459 \t val_loss=0.017123 \t time=0.89s\n",
      "Best model: Epoch 16 \t loss=0.017064 \t val_loss=0.016860 \t time=0.90s\n",
      "Best model: Epoch 18 \t loss=0.016741 \t val_loss=0.016715 \t time=1.01s\n",
      "Best model: Epoch 19 \t loss=0.016492 \t val_loss=0.016638 \t time=0.89s\n",
      "Best model: Epoch 20 \t loss=0.016265 \t val_loss=0.016467 \t time=0.88s\n",
      "Best model: Epoch 21 \t loss=0.016243 \t val_loss=0.016437 \t time=0.90s\n",
      "Best model: Epoch 23 \t loss=0.016021 \t val_loss=0.016410 \t time=0.90s\n",
      "Best model: Epoch 24 \t loss=0.015786 \t val_loss=0.016352 \t time=0.90s\n",
      "Best model: Epoch 26 \t loss=0.015692 \t val_loss=0.016319 \t time=0.89s\n",
      "Best model: Epoch 27 \t loss=0.015599 \t val_loss=0.016292 \t time=0.88s\n",
      "Best model: Epoch 29 \t loss=0.015427 \t val_loss=0.016235 \t time=1.12s\n",
      "Best model: Epoch 30 \t loss=0.015397 \t val_loss=0.016205 \t time=0.89s\n",
      "Best model: Epoch 31 \t loss=0.015269 \t val_loss=0.016160 \t time=0.88s\n",
      "Best model: Epoch 33 \t loss=0.015170 \t val_loss=0.016138 \t time=0.88s\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 38 \t loss=0.014412 \t val_loss=0.016074 \t time=0.88s\n",
      "Best model: Epoch 39 \t loss=0.014103 \t val_loss=0.016024 \t time=0.87s\n",
      "Best model: Epoch 40 \t loss=0.013940 \t val_loss=0.015989 \t time=0.95s\n",
      "Fold 3 log loss: 0.01598215567903187\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.414447 \t val_loss=0.083103 \t time=0.89s\n",
      "Best model: Epoch 2 \t loss=0.048190 \t val_loss=0.029241 \t time=0.87s\n",
      "Best model: Epoch 3 \t loss=0.028207 \t val_loss=0.022736 \t time=0.96s\n",
      "Best model: Epoch 4 \t loss=0.023313 \t val_loss=0.020537 \t time=0.87s\n",
      "Best model: Epoch 5 \t loss=0.021788 \t val_loss=0.019921 \t time=0.86s\n",
      "Best model: Epoch 6 \t loss=0.020538 \t val_loss=0.019117 \t time=0.97s\n",
      "Best model: Epoch 7 \t loss=0.020011 \t val_loss=0.018677 \t time=1.19s\n",
      "Best model: Epoch 8 \t loss=0.019755 \t val_loss=0.018538 \t time=1.09s\n",
      "Best model: Epoch 9 \t loss=0.019270 \t val_loss=0.017984 \t time=1.19s\n",
      "Best model: Epoch 10 \t loss=0.018757 \t val_loss=0.017874 \t time=0.87s\n",
      "Best model: Epoch 11 \t loss=0.018413 \t val_loss=0.017491 \t time=0.90s\n",
      "Best model: Epoch 12 \t loss=0.018074 \t val_loss=0.017276 \t time=0.88s\n",
      "Best model: Epoch 13 \t loss=0.017728 \t val_loss=0.017107 \t time=0.91s\n",
      "Best model: Epoch 14 \t loss=0.017529 \t val_loss=0.017003 \t time=0.91s\n",
      "Best model: Epoch 15 \t loss=0.017251 \t val_loss=0.016845 \t time=0.88s\n",
      "Best model: Epoch 16 \t loss=0.017310 \t val_loss=0.016813 \t time=0.86s\n",
      "Best model: Epoch 17 \t loss=0.017063 \t val_loss=0.016771 \t time=0.87s\n",
      "Best model: Epoch 18 \t loss=0.016799 \t val_loss=0.016573 \t time=0.87s\n",
      "Best model: Epoch 19 \t loss=0.016628 \t val_loss=0.016536 \t time=0.89s\n",
      "Best model: Epoch 20 \t loss=0.016486 \t val_loss=0.016442 \t time=1.09s\n",
      "Best model: Epoch 21 \t loss=0.016320 \t val_loss=0.016359 \t time=0.88s\n",
      "Best model: Epoch 22 \t loss=0.016065 \t val_loss=0.016320 \t time=0.88s\n",
      "Best model: Epoch 23 \t loss=0.016071 \t val_loss=0.016278 \t time=0.87s\n",
      "Best model: Epoch 24 \t loss=0.015930 \t val_loss=0.016261 \t time=0.90s\n",
      "Best model: Epoch 26 \t loss=0.015693 \t val_loss=0.016257 \t time=0.87s\n",
      "Best model: Epoch 28 \t loss=0.015580 \t val_loss=0.016195 \t time=0.86s\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 33 \t loss=0.014672 \t val_loss=0.016048 \t time=1.04s\n",
      "Best model: Epoch 34 \t loss=0.014376 \t val_loss=0.015966 \t time=0.91s\n",
      "Best model: Epoch 35 \t loss=0.014214 \t val_loss=0.015932 \t time=0.97s\n",
      "Best model: Epoch 37 \t loss=0.013900 \t val_loss=0.015920 \t time=0.87s\n",
      "Best model: Epoch 40 \t loss=0.013575 \t val_loss=0.015914 \t time=0.90s\n",
      "Fold 4 log loss: 0.015969970162948186\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.413087 \t val_loss=0.073699 \t time=0.91s\n",
      "Best model: Epoch 2 \t loss=0.049084 \t val_loss=0.029309 \t time=0.89s\n",
      "Best model: Epoch 3 \t loss=0.028082 \t val_loss=0.023504 \t time=0.90s\n",
      "Best model: Epoch 4 \t loss=0.023135 \t val_loss=0.020912 \t time=0.88s\n",
      "Best model: Epoch 5 \t loss=0.021648 \t val_loss=0.020058 \t time=0.90s\n",
      "Best model: Epoch 6 \t loss=0.020544 \t val_loss=0.019231 \t time=0.88s\n",
      "Best model: Epoch 7 \t loss=0.019869 \t val_loss=0.018961 \t time=0.88s\n",
      "Best model: Epoch 8 \t loss=0.019517 \t val_loss=0.018423 \t time=0.88s\n",
      "Best model: Epoch 9 \t loss=0.018956 \t val_loss=0.018169 \t time=0.91s\n",
      "Best model: Epoch 10 \t loss=0.018656 \t val_loss=0.017995 \t time=0.93s\n",
      "Best model: Epoch 11 \t loss=0.018387 \t val_loss=0.017671 \t time=0.91s\n",
      "Best model: Epoch 12 \t loss=0.017993 \t val_loss=0.017505 \t time=1.24s\n",
      "Best model: Epoch 13 \t loss=0.017741 \t val_loss=0.017458 \t time=0.89s\n",
      "Best model: Epoch 14 \t loss=0.017536 \t val_loss=0.017309 \t time=0.94s\n",
      "Best model: Epoch 15 \t loss=0.017324 \t val_loss=0.017063 \t time=0.93s\n",
      "Best model: Epoch 16 \t loss=0.017025 \t val_loss=0.017025 \t time=0.94s\n",
      "Best model: Epoch 17 \t loss=0.016810 \t val_loss=0.016959 \t time=0.91s\n",
      "Best model: Epoch 18 \t loss=0.016821 \t val_loss=0.016803 \t time=0.90s\n",
      "Best model: Epoch 19 \t loss=0.016505 \t val_loss=0.016753 \t time=0.92s\n",
      "Best model: Epoch 21 \t loss=0.016231 \t val_loss=0.016673 \t time=0.94s\n",
      "Best model: Epoch 22 \t loss=0.016069 \t val_loss=0.016543 \t time=0.92s\n",
      "Best model: Epoch 23 \t loss=0.015954 \t val_loss=0.016523 \t time=1.61s\n",
      "Best model: Epoch 25 \t loss=0.015810 \t val_loss=0.016492 \t time=1.34s\n",
      "Best model: Epoch 26 \t loss=0.015618 \t val_loss=0.016482 \t time=0.96s\n",
      "Best model: Epoch 27 \t loss=0.015527 \t val_loss=0.016386 \t time=0.93s\n",
      "Best model: Epoch 28 \t loss=0.015535 \t val_loss=0.016386 \t time=0.97s\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 32 \t loss=0.014660 \t val_loss=0.016190 \t time=0.92s\n",
      "Best model: Epoch 34 \t loss=0.014195 \t val_loss=0.016131 \t time=0.94s\n",
      "Best model: Epoch 35 \t loss=0.014032 \t val_loss=0.016129 \t time=0.98s\n",
      "Best model: Epoch 36 \t loss=0.013952 \t val_loss=0.016119 \t time=0.92s\n",
      "Best model: Epoch 38 \t loss=0.013619 \t val_loss=0.016104 \t time=0.91s\n",
      "Fold 5 log loss: 0.016105637577698048\n",
      "Seed 2\n",
      "Fold 1 log loss: 0.016280616069086786\n",
      "Fold 2 log loss: 0.0160819916597495\n",
      "Fold 3 log loss: 0.01598215567903187\n",
      "Fold 4 log loss: 0.015969970162948186\n",
      "Fold 5 log loss: 0.016105637577698048\n",
      "Std of log loss: 0.0001118079949946509\n",
      "Total log loss: 0.01608407334211503\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.413482 \t val_loss=0.085006 \t time=0.95s\n",
      "Best model: Epoch 2 \t loss=0.048716 \t val_loss=0.028552 \t time=0.93s\n",
      "Best model: Epoch 3 \t loss=0.027498 \t val_loss=0.023904 \t time=0.92s\n",
      "Best model: Epoch 4 \t loss=0.023643 \t val_loss=0.022032 \t time=0.95s\n",
      "Best model: Epoch 5 \t loss=0.021593 \t val_loss=0.019921 \t time=0.94s\n",
      "Best model: Epoch 6 \t loss=0.020638 \t val_loss=0.019664 \t time=0.96s\n",
      "Best model: Epoch 7 \t loss=0.019979 \t val_loss=0.019123 \t time=0.95s\n",
      "Best model: Epoch 8 \t loss=0.019318 \t val_loss=0.018529 \t time=1.16s\n",
      "Best model: Epoch 9 \t loss=0.019013 \t val_loss=0.018134 \t time=1.45s\n",
      "Best model: Epoch 11 \t loss=0.018321 \t val_loss=0.017871 \t time=0.98s\n",
      "Best model: Epoch 12 \t loss=0.018269 \t val_loss=0.017847 \t time=0.93s\n",
      "Best model: Epoch 13 \t loss=0.017851 \t val_loss=0.017346 \t time=0.94s\n",
      "Best model: Epoch 14 \t loss=0.017423 \t val_loss=0.017188 \t time=0.93s\n",
      "Best model: Epoch 16 \t loss=0.017074 \t val_loss=0.016996 \t time=0.91s\n",
      "Best model: Epoch 17 \t loss=0.016863 \t val_loss=0.016935 \t time=0.92s\n",
      "Best model: Epoch 18 \t loss=0.016690 \t val_loss=0.016766 \t time=0.95s\n",
      "Best model: Epoch 19 \t loss=0.016395 \t val_loss=0.016740 \t time=1.12s\n",
      "Best model: Epoch 20 \t loss=0.016392 \t val_loss=0.016729 \t time=0.91s\n",
      "Best model: Epoch 21 \t loss=0.016257 \t val_loss=0.016575 \t time=0.91s\n",
      "Best model: Epoch 22 \t loss=0.016079 \t val_loss=0.016537 \t time=0.92s\n",
      "Best model: Epoch 24 \t loss=0.015940 \t val_loss=0.016440 \t time=0.89s\n",
      "Best model: Epoch 26 \t loss=0.015711 \t val_loss=0.016439 \t time=0.88s\n",
      "Best model: Epoch 27 \t loss=0.015655 \t val_loss=0.016355 \t time=0.95s\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 32 \t loss=0.014718 \t val_loss=0.016233 \t time=0.91s\n",
      "Best model: Epoch 33 \t loss=0.014492 \t val_loss=0.016164 \t time=0.89s\n",
      "Best model: Epoch 34 \t loss=0.014242 \t val_loss=0.016162 \t time=0.91s\n",
      "Best model: Epoch 35 \t loss=0.014062 \t val_loss=0.016079 \t time=0.92s\n",
      "Best model: Epoch 38 \t loss=0.013740 \t val_loss=0.016068 \t time=0.93s\n",
      "Fold 1 log loss: 0.01615919115850787\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.410309 \t val_loss=0.088772 \t time=1.13s\n",
      "Best model: Epoch 2 \t loss=0.049774 \t val_loss=0.029268 \t time=0.93s\n",
      "Best model: Epoch 3 \t loss=0.027053 \t val_loss=0.022645 \t time=0.89s\n",
      "Best model: Epoch 4 \t loss=0.023236 \t val_loss=0.021036 \t time=0.92s\n",
      "Best model: Epoch 5 \t loss=0.021846 \t val_loss=0.019918 \t time=0.93s\n",
      "Best model: Epoch 6 \t loss=0.020478 \t val_loss=0.019441 \t time=0.90s\n",
      "Best model: Epoch 7 \t loss=0.020034 \t val_loss=0.019010 \t time=0.90s\n",
      "Best model: Epoch 8 \t loss=0.019641 \t val_loss=0.018800 \t time=1.09s\n",
      "Best model: Epoch 9 \t loss=0.019041 \t val_loss=0.018087 \t time=1.01s\n",
      "Best model: Epoch 10 \t loss=0.018651 \t val_loss=0.017874 \t time=0.94s\n",
      "Best model: Epoch 11 \t loss=0.018348 \t val_loss=0.017594 \t time=0.95s\n",
      "Best model: Epoch 12 \t loss=0.017880 \t val_loss=0.017560 \t time=0.92s\n",
      "Best model: Epoch 13 \t loss=0.017816 \t val_loss=0.017224 \t time=0.92s\n",
      "Best model: Epoch 14 \t loss=0.017406 \t val_loss=0.017038 \t time=0.90s\n",
      "Best model: Epoch 15 \t loss=0.017159 \t val_loss=0.016993 \t time=0.93s\n",
      "Best model: Epoch 16 \t loss=0.017050 \t val_loss=0.016916 \t time=0.90s\n",
      "Best model: Epoch 18 \t loss=0.016693 \t val_loss=0.016737 \t time=0.90s\n",
      "Best model: Epoch 19 \t loss=0.016392 \t val_loss=0.016562 \t time=0.95s\n",
      "Best model: Epoch 21 \t loss=0.016198 \t val_loss=0.016541 \t time=0.89s\n",
      "Best model: Epoch 22 \t loss=0.016053 \t val_loss=0.016421 \t time=0.89s\n",
      "Best model: Epoch 24 \t loss=0.015905 \t val_loss=0.016389 \t time=0.91s\n",
      "Best model: Epoch 25 \t loss=0.015724 \t val_loss=0.016362 \t time=0.89s\n",
      "Best model: Epoch 27 \t loss=0.015611 \t val_loss=0.016351 \t time=0.87s\n",
      "Best model: Epoch 29 \t loss=0.015418 \t val_loss=0.016304 \t time=0.87s\n",
      "Best model: Epoch 30 \t loss=0.015379 \t val_loss=0.016291 \t time=0.89s\n",
      "Best model: Epoch 31 \t loss=0.015286 \t val_loss=0.016288 \t time=1.22s\n",
      "Best model: Epoch 35 \t loss=0.015174 \t val_loss=0.016268 \t time=0.93s\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 40 \t loss=0.014326 \t val_loss=0.016078 \t time=0.87s\n",
      "Fold 2 log loss: 0.016131589753943455\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.416642 \t val_loss=0.077390 \t time=0.88s\n",
      "Best model: Epoch 2 \t loss=0.049910 \t val_loss=0.028385 \t time=0.89s\n",
      "Best model: Epoch 3 \t loss=0.027063 \t val_loss=0.022448 \t time=0.92s\n",
      "Best model: Epoch 4 \t loss=0.023517 \t val_loss=0.021003 \t time=0.89s\n",
      "Best model: Epoch 5 \t loss=0.021520 \t val_loss=0.020122 \t time=0.88s\n",
      "Best model: Epoch 6 \t loss=0.020809 \t val_loss=0.019284 \t time=0.89s\n",
      "Best model: Epoch 7 \t loss=0.019977 \t val_loss=0.018849 \t time=0.88s\n",
      "Best model: Epoch 8 \t loss=0.019361 \t val_loss=0.018720 \t time=0.90s\n",
      "Best model: Epoch 9 \t loss=0.019017 \t val_loss=0.018142 \t time=0.89s\n",
      "Best model: Epoch 10 \t loss=0.018557 \t val_loss=0.017915 \t time=0.90s\n",
      "Best model: Epoch 11 \t loss=0.018590 \t val_loss=0.017653 \t time=1.13s\n",
      "Best model: Epoch 12 \t loss=0.018100 \t val_loss=0.017520 \t time=0.89s\n",
      "Best model: Epoch 13 \t loss=0.017907 \t val_loss=0.017251 \t time=1.23s\n",
      "Best model: Epoch 14 \t loss=0.017499 \t val_loss=0.017095 \t time=1.11s\n",
      "Best model: Epoch 15 \t loss=0.017316 \t val_loss=0.017086 \t time=1.04s\n",
      "Best model: Epoch 16 \t loss=0.017026 \t val_loss=0.016796 \t time=0.92s\n",
      "Best model: Epoch 18 \t loss=0.016643 \t val_loss=0.016680 \t time=0.90s\n",
      "Best model: Epoch 20 \t loss=0.016413 \t val_loss=0.016520 \t time=0.90s\n",
      "Best model: Epoch 21 \t loss=0.016271 \t val_loss=0.016456 \t time=0.99s\n",
      "Best model: Epoch 22 \t loss=0.016094 \t val_loss=0.016416 \t time=1.01s\n",
      "Best model: Epoch 23 \t loss=0.015980 \t val_loss=0.016379 \t time=1.12s\n",
      "Best model: Epoch 25 \t loss=0.015681 \t val_loss=0.016315 \t time=0.90s\n",
      "Best model: Epoch 26 \t loss=0.015640 \t val_loss=0.016247 \t time=0.88s\n",
      "Best model: Epoch 30 \t loss=0.015300 \t val_loss=0.016237 \t time=0.89s\n",
      "Best model: Epoch 31 \t loss=0.015267 \t val_loss=0.016224 \t time=0.87s\n",
      "Best model: Epoch 32 \t loss=0.015209 \t val_loss=0.016183 \t time=0.89s\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 37 \t loss=0.014489 \t val_loss=0.016069 \t time=0.90s\n",
      "Best model: Epoch 38 \t loss=0.014129 \t val_loss=0.015990 \t time=0.89s\n",
      "Best model: Epoch 40 \t loss=0.013782 \t val_loss=0.015978 \t time=0.88s\n",
      "Fold 3 log loss: 0.015970096888850625\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.408462 \t val_loss=0.074284 \t time=0.89s\n",
      "Best model: Epoch 2 \t loss=0.049468 \t val_loss=0.028216 \t time=1.11s\n",
      "Best model: Epoch 3 \t loss=0.027922 \t val_loss=0.022309 \t time=0.90s\n",
      "Best model: Epoch 4 \t loss=0.023475 \t val_loss=0.020758 \t time=0.89s\n",
      "Best model: Epoch 5 \t loss=0.021691 \t val_loss=0.019736 \t time=0.90s\n",
      "Best model: Epoch 6 \t loss=0.020567 \t val_loss=0.019042 \t time=0.89s\n",
      "Best model: Epoch 7 \t loss=0.019984 \t val_loss=0.018598 \t time=0.92s\n",
      "Best model: Epoch 8 \t loss=0.019345 \t val_loss=0.018219 \t time=0.89s\n",
      "Best model: Epoch 9 \t loss=0.019050 \t val_loss=0.017943 \t time=1.02s\n",
      "Best model: Epoch 10 \t loss=0.018741 \t val_loss=0.017613 \t time=0.89s\n",
      "Best model: Epoch 12 \t loss=0.017970 \t val_loss=0.017243 \t time=0.91s\n",
      "Best model: Epoch 13 \t loss=0.017718 \t val_loss=0.017188 \t time=1.03s\n",
      "Best model: Epoch 14 \t loss=0.017529 \t val_loss=0.016956 \t time=1.21s\n",
      "Best model: Epoch 15 \t loss=0.017444 \t val_loss=0.016822 \t time=0.95s\n",
      "Best model: Epoch 16 \t loss=0.017227 \t val_loss=0.016729 \t time=0.88s\n",
      "Best model: Epoch 17 \t loss=0.016915 \t val_loss=0.016672 \t time=0.88s\n",
      "Best model: Epoch 18 \t loss=0.016773 \t val_loss=0.016519 \t time=0.88s\n",
      "Best model: Epoch 21 \t loss=0.016231 \t val_loss=0.016478 \t time=0.87s\n",
      "Best model: Epoch 22 \t loss=0.016146 \t val_loss=0.016421 \t time=0.90s\n",
      "Best model: Epoch 23 \t loss=0.016069 \t val_loss=0.016306 \t time=0.88s\n",
      "Best model: Epoch 24 \t loss=0.015932 \t val_loss=0.016244 \t time=0.88s\n",
      "Best model: Epoch 25 \t loss=0.015780 \t val_loss=0.016208 \t time=1.05s\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 30 \t loss=0.014801 \t val_loss=0.016035 \t time=0.89s\n",
      "Best model: Epoch 31 \t loss=0.014562 \t val_loss=0.015951 \t time=0.90s\n",
      "Best model: Epoch 32 \t loss=0.014330 \t val_loss=0.015949 \t time=0.88s\n",
      "Best model: Epoch 33 \t loss=0.014244 \t val_loss=0.015931 \t time=1.32s\n",
      "Best model: Epoch 34 \t loss=0.014103 \t val_loss=0.015871 \t time=0.91s\n",
      "Fold 4 log loss: 0.01593554139806171\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.412365 \t val_loss=0.078813 \t time=0.89s\n",
      "Best model: Epoch 2 \t loss=0.049562 \t val_loss=0.030582 \t time=0.91s\n",
      "Best model: Epoch 3 \t loss=0.027535 \t val_loss=0.022771 \t time=0.87s\n",
      "Best model: Epoch 4 \t loss=0.023436 \t val_loss=0.020868 \t time=1.05s\n",
      "Best model: Epoch 5 \t loss=0.021557 \t val_loss=0.019981 \t time=1.20s\n",
      "Best model: Epoch 6 \t loss=0.020607 \t val_loss=0.019295 \t time=0.94s\n",
      "Best model: Epoch 7 \t loss=0.020060 \t val_loss=0.019073 \t time=0.88s\n",
      "Best model: Epoch 8 \t loss=0.019507 \t val_loss=0.018563 \t time=0.90s\n",
      "Best model: Epoch 9 \t loss=0.019204 \t val_loss=0.018202 \t time=0.92s\n",
      "Best model: Epoch 10 \t loss=0.018767 \t val_loss=0.017955 \t time=0.90s\n",
      "Best model: Epoch 11 \t loss=0.018517 \t val_loss=0.017727 \t time=0.88s\n",
      "Best model: Epoch 12 \t loss=0.018120 \t val_loss=0.017525 \t time=0.89s\n",
      "Best model: Epoch 13 \t loss=0.017746 \t val_loss=0.017468 \t time=0.88s\n",
      "Best model: Epoch 14 \t loss=0.017485 \t val_loss=0.017203 \t time=0.91s\n",
      "Best model: Epoch 15 \t loss=0.017232 \t val_loss=0.017094 \t time=1.08s\n",
      "Best model: Epoch 16 \t loss=0.017003 \t val_loss=0.017025 \t time=0.91s\n",
      "Best model: Epoch 17 \t loss=0.016785 \t val_loss=0.016926 \t time=0.88s\n",
      "Best model: Epoch 18 \t loss=0.016714 \t val_loss=0.016916 \t time=0.89s\n",
      "Best model: Epoch 19 \t loss=0.016531 \t val_loss=0.016807 \t time=0.88s\n",
      "Best model: Epoch 20 \t loss=0.016316 \t val_loss=0.016705 \t time=0.92s\n",
      "Best model: Epoch 21 \t loss=0.016168 \t val_loss=0.016531 \t time=0.89s\n",
      "Best model: Epoch 22 \t loss=0.016100 \t val_loss=0.016499 \t time=0.88s\n",
      "Best model: Epoch 25 \t loss=0.015851 \t val_loss=0.016459 \t time=0.88s\n",
      "Best model: Epoch 29 \t loss=0.015470 \t val_loss=0.016444 \t time=0.88s\n",
      "Best model: Epoch 31 \t loss=0.015325 \t val_loss=0.016384 \t time=0.90s\n",
      "Best model: Epoch 35 \t loss=0.015085 \t val_loss=0.016368 \t time=0.89s\n",
      "Best model: Epoch 39 \t loss=0.014981 \t val_loss=0.016356 \t time=1.17s\n",
      "Fold 5 log loss: 0.01637002395976194\n",
      "Seed 3\n",
      "Fold 1 log loss: 0.01615919115850787\n",
      "Fold 2 log loss: 0.016131589753943455\n",
      "Fold 3 log loss: 0.015970096888850625\n",
      "Fold 4 log loss: 0.01593554139806171\n",
      "Fold 5 log loss: 0.01637002395976194\n",
      "Std of log loss: 0.0001552114403026376\n",
      "Total log loss: 0.01611327610054892\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.417376 \t val_loss=0.076125 \t time=0.88s\n",
      "Best model: Epoch 2 \t loss=0.050306 \t val_loss=0.028216 \t time=0.89s\n",
      "Best model: Epoch 3 \t loss=0.026912 \t val_loss=0.022988 \t time=0.88s\n",
      "Best model: Epoch 4 \t loss=0.023302 \t val_loss=0.021196 \t time=0.98s\n",
      "Best model: Epoch 5 \t loss=0.021557 \t val_loss=0.020048 \t time=0.98s\n",
      "Best model: Epoch 6 \t loss=0.020636 \t val_loss=0.019492 \t time=1.31s\n",
      "Best model: Epoch 7 \t loss=0.019718 \t val_loss=0.018851 \t time=1.01s\n",
      "Best model: Epoch 8 \t loss=0.019296 \t val_loss=0.018580 \t time=1.17s\n",
      "Best model: Epoch 9 \t loss=0.019243 \t val_loss=0.018538 \t time=0.90s\n",
      "Best model: Epoch 10 \t loss=0.018776 \t val_loss=0.018315 \t time=0.88s\n",
      "Best model: Epoch 11 \t loss=0.018431 \t val_loss=0.018107 \t time=0.90s\n",
      "Best model: Epoch 12 \t loss=0.018065 \t val_loss=0.017536 \t time=0.94s\n",
      "Best model: Epoch 13 \t loss=0.017735 \t val_loss=0.017326 \t time=0.91s\n",
      "Best model: Epoch 14 \t loss=0.017458 \t val_loss=0.017205 \t time=0.88s\n",
      "Best model: Epoch 15 \t loss=0.017398 \t val_loss=0.017164 \t time=1.15s\n",
      "Best model: Epoch 16 \t loss=0.017067 \t val_loss=0.017005 \t time=0.88s\n",
      "Best model: Epoch 17 \t loss=0.016868 \t val_loss=0.016867 \t time=1.07s\n",
      "Best model: Epoch 18 \t loss=0.016657 \t val_loss=0.016804 \t time=0.88s\n",
      "Best model: Epoch 19 \t loss=0.016450 \t val_loss=0.016644 \t time=0.89s\n",
      "Best model: Epoch 20 \t loss=0.016293 \t val_loss=0.016622 \t time=0.88s\n",
      "Best model: Epoch 21 \t loss=0.016240 \t val_loss=0.016574 \t time=0.88s\n",
      "Best model: Epoch 22 \t loss=0.016049 \t val_loss=0.016538 \t time=0.88s\n",
      "Best model: Epoch 23 \t loss=0.015954 \t val_loss=0.016425 \t time=0.90s\n",
      "Best model: Epoch 25 \t loss=0.015780 \t val_loss=0.016411 \t time=0.87s\n",
      "Best model: Epoch 26 \t loss=0.015589 \t val_loss=0.016411 \t time=1.06s\n",
      "Best model: Epoch 27 \t loss=0.015555 \t val_loss=0.016378 \t time=1.20s\n",
      "Best model: Epoch 30 \t loss=0.015372 \t val_loss=0.016330 \t time=0.87s\n",
      "Best model: Epoch 31 \t loss=0.015312 \t val_loss=0.016320 \t time=0.87s\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 36 \t loss=0.014407 \t val_loss=0.016174 \t time=0.91s\n",
      "Best model: Epoch 37 \t loss=0.014183 \t val_loss=0.016169 \t time=0.90s\n",
      "Best model: Epoch 38 \t loss=0.013891 \t val_loss=0.016102 \t time=1.12s\n",
      "Best model: Epoch 40 \t loss=0.013597 \t val_loss=0.016079 \t time=0.90s\n",
      "Fold 1 log loss: 0.016174484102574663\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.410688 \t val_loss=0.080734 \t time=0.89s\n",
      "Best model: Epoch 2 \t loss=0.048953 \t val_loss=0.028426 \t time=0.89s\n",
      "Best model: Epoch 3 \t loss=0.027673 \t val_loss=0.022677 \t time=0.89s\n",
      "Best model: Epoch 4 \t loss=0.023493 \t val_loss=0.021433 \t time=0.92s\n",
      "Best model: Epoch 5 \t loss=0.021537 \t val_loss=0.020268 \t time=0.90s\n",
      "Best model: Epoch 6 \t loss=0.020399 \t val_loss=0.019240 \t time=0.95s\n",
      "Best model: Epoch 7 \t loss=0.019899 \t val_loss=0.018880 \t time=1.27s\n",
      "Best model: Epoch 8 \t loss=0.019432 \t val_loss=0.018494 \t time=0.88s\n",
      "Best model: Epoch 9 \t loss=0.018890 \t val_loss=0.018014 \t time=0.88s\n",
      "Best model: Epoch 10 \t loss=0.018555 \t val_loss=0.017699 \t time=0.88s\n",
      "Best model: Epoch 11 \t loss=0.018488 \t val_loss=0.017471 \t time=0.87s\n",
      "Best model: Epoch 12 \t loss=0.017990 \t val_loss=0.017407 \t time=0.89s\n",
      "Best model: Epoch 13 \t loss=0.017646 \t val_loss=0.017239 \t time=0.89s\n",
      "Best model: Epoch 14 \t loss=0.017444 \t val_loss=0.017084 \t time=0.90s\n",
      "Best model: Epoch 15 \t loss=0.017198 \t val_loss=0.017004 \t time=0.92s\n",
      "Best model: Epoch 16 \t loss=0.017013 \t val_loss=0.016855 \t time=0.89s\n",
      "Best model: Epoch 18 \t loss=0.016731 \t val_loss=0.016750 \t time=1.17s\n",
      "Best model: Epoch 19 \t loss=0.016473 \t val_loss=0.016698 \t time=1.20s\n",
      "Best model: Epoch 20 \t loss=0.016372 \t val_loss=0.016563 \t time=0.93s\n",
      "Best model: Epoch 21 \t loss=0.016135 \t val_loss=0.016538 \t time=0.93s\n",
      "Best model: Epoch 22 \t loss=0.016041 \t val_loss=0.016417 \t time=0.87s\n",
      "Best model: Epoch 24 \t loss=0.015880 \t val_loss=0.016353 \t time=1.29s\n",
      "Best model: Epoch 25 \t loss=0.015686 \t val_loss=0.016303 \t time=0.91s\n",
      "Best model: Epoch 26 \t loss=0.015627 \t val_loss=0.016299 \t time=0.90s\n",
      "Best model: Epoch 27 \t loss=0.015532 \t val_loss=0.016210 \t time=0.92s\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 32 \t loss=0.014590 \t val_loss=0.016123 \t time=0.88s\n",
      "Best model: Epoch 33 \t loss=0.014261 \t val_loss=0.016048 \t time=0.86s\n",
      "Best model: Epoch 35 \t loss=0.013990 \t val_loss=0.016027 \t time=0.95s\n",
      "Best model: Epoch 37 \t loss=0.013710 \t val_loss=0.016001 \t time=0.91s\n",
      "Best model: Epoch 39 \t loss=0.013421 \t val_loss=0.015993 \t time=0.87s\n",
      "Fold 2 log loss: 0.016047175759869827\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.410937 \t val_loss=0.082875 \t time=0.87s\n",
      "Best model: Epoch 2 \t loss=0.049043 \t val_loss=0.027972 \t time=0.90s\n",
      "Best model: Epoch 3 \t loss=0.027629 \t val_loss=0.022424 \t time=0.87s\n",
      "Best model: Epoch 4 \t loss=0.023497 \t val_loss=0.020791 \t time=0.86s\n",
      "Best model: Epoch 5 \t loss=0.021301 \t val_loss=0.019844 \t time=0.86s\n",
      "Best model: Epoch 6 \t loss=0.020559 \t val_loss=0.019263 \t time=0.88s\n",
      "Best model: Epoch 7 \t loss=0.020173 \t val_loss=0.018889 \t time=0.86s\n",
      "Best model: Epoch 8 \t loss=0.019406 \t val_loss=0.018523 \t time=0.88s\n",
      "Best model: Epoch 9 \t loss=0.019178 \t val_loss=0.018472 \t time=1.28s\n",
      "Best model: Epoch 10 \t loss=0.018764 \t val_loss=0.017898 \t time=0.96s\n",
      "Best model: Epoch 11 \t loss=0.018385 \t val_loss=0.017611 \t time=0.88s\n",
      "Best model: Epoch 13 \t loss=0.018019 \t val_loss=0.017296 \t time=0.87s\n",
      "Best model: Epoch 14 \t loss=0.017689 \t val_loss=0.017133 \t time=0.88s\n",
      "Best model: Epoch 15 \t loss=0.017383 \t val_loss=0.016993 \t time=0.87s\n",
      "Best model: Epoch 16 \t loss=0.017100 \t val_loss=0.016782 \t time=0.87s\n",
      "Best model: Epoch 17 \t loss=0.016827 \t val_loss=0.016761 \t time=0.89s\n",
      "Best model: Epoch 18 \t loss=0.016695 \t val_loss=0.016640 \t time=0.89s\n",
      "Best model: Epoch 19 \t loss=0.016421 \t val_loss=0.016541 \t time=0.89s\n",
      "Best model: Epoch 20 \t loss=0.016383 \t val_loss=0.016434 \t time=1.08s\n",
      "Best model: Epoch 22 \t loss=0.016122 \t val_loss=0.016432 \t time=0.87s\n",
      "Best model: Epoch 23 \t loss=0.015940 \t val_loss=0.016393 \t time=0.87s\n",
      "Best model: Epoch 24 \t loss=0.015808 \t val_loss=0.016330 \t time=0.85s\n",
      "Best model: Epoch 25 \t loss=0.015735 \t val_loss=0.016300 \t time=0.86s\n",
      "Best model: Epoch 27 \t loss=0.015601 \t val_loss=0.016294 \t time=0.83s\n",
      "Best model: Epoch 29 \t loss=0.015337 \t val_loss=0.016287 \t time=0.84s\n",
      "Best model: Epoch 31 \t loss=0.015245 \t val_loss=0.016173 \t time=0.86s\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 36 \t loss=0.014393 \t val_loss=0.016052 \t time=0.85s\n",
      "Best model: Epoch 37 \t loss=0.014168 \t val_loss=0.015955 \t time=0.82s\n",
      "Best model: Epoch 38 \t loss=0.013916 \t val_loss=0.015939 \t time=0.88s\n",
      "Fold 3 log loss: 0.015928155998706423\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.414284 \t val_loss=0.077726 \t time=0.93s\n",
      "Best model: Epoch 2 \t loss=0.048858 \t val_loss=0.027310 \t time=1.56s\n",
      "Best model: Epoch 3 \t loss=0.027433 \t val_loss=0.022797 \t time=1.08s\n",
      "Best model: Epoch 4 \t loss=0.023323 \t val_loss=0.020871 \t time=0.91s\n",
      "Best model: Epoch 5 \t loss=0.021639 \t val_loss=0.019794 \t time=0.92s\n",
      "Best model: Epoch 6 \t loss=0.020495 \t val_loss=0.019132 \t time=1.28s\n",
      "Best model: Epoch 7 \t loss=0.019782 \t val_loss=0.018384 \t time=0.92s\n",
      "Best model: Epoch 8 \t loss=0.019526 \t val_loss=0.018299 \t time=0.87s\n",
      "Best model: Epoch 9 \t loss=0.019129 \t val_loss=0.017967 \t time=0.85s\n",
      "Best model: Epoch 11 \t loss=0.018860 \t val_loss=0.017541 \t time=0.87s\n",
      "Best model: Epoch 12 \t loss=0.018034 \t val_loss=0.017256 \t time=1.11s\n",
      "Best model: Epoch 13 \t loss=0.017711 \t val_loss=0.017202 \t time=0.86s\n",
      "Best model: Epoch 14 \t loss=0.017492 \t val_loss=0.016948 \t time=0.87s\n",
      "Best model: Epoch 15 \t loss=0.017307 \t val_loss=0.016830 \t time=0.86s\n",
      "Best model: Epoch 16 \t loss=0.017385 \t val_loss=0.016827 \t time=0.96s\n",
      "Best model: Epoch 17 \t loss=0.016998 \t val_loss=0.016789 \t time=1.06s\n",
      "Best model: Epoch 18 \t loss=0.016722 \t val_loss=0.016612 \t time=0.89s\n",
      "Best model: Epoch 19 \t loss=0.016558 \t val_loss=0.016508 \t time=0.91s\n",
      "Best model: Epoch 20 \t loss=0.016337 \t val_loss=0.016418 \t time=0.89s\n",
      "Best model: Epoch 21 \t loss=0.016235 \t val_loss=0.016326 \t time=0.86s\n",
      "Best model: Epoch 22 \t loss=0.016127 \t val_loss=0.016278 \t time=0.85s\n",
      "Best model: Epoch 25 \t loss=0.015683 \t val_loss=0.016225 \t time=0.89s\n",
      "Best model: Epoch 28 \t loss=0.015565 \t val_loss=0.016198 \t time=0.88s\n",
      "Best model: Epoch 29 \t loss=0.015424 \t val_loss=0.016165 \t time=0.87s\n",
      "Best model: Epoch 31 \t loss=0.015384 \t val_loss=0.016151 \t time=0.90s\n",
      "Best model: Epoch 35 \t loss=0.015113 \t val_loss=0.016146 \t time=1.13s\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 40 \t loss=0.014338 \t val_loss=0.016034 \t time=0.88s\n",
      "Fold 4 log loss: 0.016093105821003578\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.413252 \t val_loss=0.074956 \t time=0.88s\n",
      "Best model: Epoch 2 \t loss=0.048736 \t val_loss=0.028345 \t time=0.88s\n",
      "Best model: Epoch 3 \t loss=0.027761 \t val_loss=0.022420 \t time=0.91s\n",
      "Best model: Epoch 4 \t loss=0.023178 \t val_loss=0.021009 \t time=1.15s\n",
      "Best model: Epoch 5 \t loss=0.021552 \t val_loss=0.019836 \t time=0.91s\n",
      "Best model: Epoch 6 \t loss=0.020723 \t val_loss=0.019424 \t time=0.91s\n",
      "Best model: Epoch 7 \t loss=0.020016 \t val_loss=0.019338 \t time=1.06s\n",
      "Best model: Epoch 8 \t loss=0.019493 \t val_loss=0.018481 \t time=0.91s\n",
      "Best model: Epoch 9 \t loss=0.018870 \t val_loss=0.018145 \t time=0.90s\n",
      "Best model: Epoch 11 \t loss=0.018279 \t val_loss=0.017718 \t time=0.88s\n",
      "Best model: Epoch 12 \t loss=0.017971 \t val_loss=0.017576 \t time=0.88s\n",
      "Best model: Epoch 13 \t loss=0.017682 \t val_loss=0.017317 \t time=0.89s\n",
      "Best model: Epoch 14 \t loss=0.017413 \t val_loss=0.017172 \t time=0.89s\n",
      "Best model: Epoch 15 \t loss=0.017277 \t val_loss=0.017068 \t time=1.09s\n",
      "Best model: Epoch 16 \t loss=0.017148 \t val_loss=0.017005 \t time=0.89s\n",
      "Best model: Epoch 17 \t loss=0.016898 \t val_loss=0.016945 \t time=0.88s\n",
      "Best model: Epoch 18 \t loss=0.016702 \t val_loss=0.016738 \t time=0.89s\n",
      "Best model: Epoch 19 \t loss=0.016448 \t val_loss=0.016712 \t time=0.89s\n",
      "Best model: Epoch 21 \t loss=0.016312 \t val_loss=0.016602 \t time=1.23s\n",
      "Best model: Epoch 23 \t loss=0.015978 \t val_loss=0.016516 \t time=0.91s\n",
      "Best model: Epoch 24 \t loss=0.015868 \t val_loss=0.016480 \t time=1.08s\n",
      "Best model: Epoch 26 \t loss=0.015581 \t val_loss=0.016478 \t time=1.26s\n",
      "Best model: Epoch 27 \t loss=0.015653 \t val_loss=0.016433 \t time=1.20s\n",
      "Best model: Epoch 28 \t loss=0.015518 \t val_loss=0.016433 \t time=0.93s\n",
      "Best model: Epoch 29 \t loss=0.015479 \t val_loss=0.016413 \t time=0.92s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014591 \t val_loss=0.016284 \t time=0.87s\n",
      "Best model: Epoch 35 \t loss=0.014365 \t val_loss=0.016185 \t time=0.86s\n",
      "Best model: Epoch 36 \t loss=0.014121 \t val_loss=0.016150 \t time=0.87s\n",
      "Best model: Epoch 38 \t loss=0.013806 \t val_loss=0.016146 \t time=0.98s\n",
      "Best model: Epoch 40 \t loss=0.013604 \t val_loss=0.016141 \t time=0.85s\n",
      "Fold 5 log loss: 0.016142314651801725\n",
      "Seed 4\n",
      "Fold 1 log loss: 0.016174484102574663\n",
      "Fold 2 log loss: 0.016047175759869827\n",
      "Fold 3 log loss: 0.015928155998706423\n",
      "Fold 4 log loss: 0.016093105821003578\n",
      "Fold 5 log loss: 0.016142314651801725\n",
      "Std of log loss: 8.610756333173562e-05\n",
      "Total log loss: 0.016077045654075823\n"
     ]
    }
   ],
   "source": [
    "seeds = [0,1,2,3,4]\n",
    "pytorch1_oof = np.zeros([len(fn_train),fn_targets.shape[1]])\n",
    "pytorch1_test = np.zeros([len(fn_test),fn_targets.shape[1]])\n",
    "\n",
    "for seed_ in seeds:\n",
    "    oof, oof_targets, pytorch_pred = modelling_torch(fn_train, fn_targets, fn_test, seed_, fn_train.shape[1], fn_targets.shape[1],1)\n",
    "    pytorch1_oof += oof / len(seeds)\n",
    "    pytorch1_test += pytorch_pred / len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T03:44:01.809986Z",
     "iopub.status.busy": "2020-10-11T03:44:01.808234Z",
     "iopub.status.idle": "2020-10-11T03:44:03.092782Z",
     "shell.execute_reply": "2020-10-11T03:44:03.092135Z"
    },
    "papermill": {
     "duration": 1.632049,
     "end_time": "2020-10-11T03:44:03.092930",
     "exception": false,
     "start_time": "2020-10-11T03:44:01.460881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.014671957119226644\n"
     ]
    }
   ],
   "source": [
    "check_pytorch1 = np.zeros([targets.shape[0], targets.shape[1]-1])\n",
    "check_pytorch1[cons_train_index,:] = pytorch1_oof\n",
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(check_pytorch1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.34342,
     "end_time": "2020-10-11T03:44:03.783714",
     "exception": false,
     "start_time": "2020-10-11T03:44:03.440294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1st Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T03:44:04.483658Z",
     "iopub.status.busy": "2020-10-11T03:44:04.481899Z",
     "iopub.status.idle": "2020-10-11T03:44:04.484575Z",
     "shell.execute_reply": "2020-10-11T03:44:04.485122Z"
    },
    "papermill": {
     "duration": 0.36094,
     "end_time": "2020-10-11T03:44:04.485279",
     "exception": false,
     "start_time": "2020-10-11T03:44:04.124339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "\n",
    "def log_loss_metric(y_true, y_pred):\n",
    "    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    loss = - np.mean(np.mean(y_true * np.log(y_pred_clip) + (1 - y_true) * np.log(1 - y_pred_clip), axis = 1))\n",
    "    return loss\n",
    "\n",
    "def modelling_lr(tr, ta, te):    \n",
    "    oof = np.zeros([len(tr),ta.shape[1]])\n",
    "    pred_value = np.zeros([te.shape[0], ta.shape[1]])\n",
    "    \n",
    "    mskf_lr = MultilabelStratifiedKFold(n_splits = N_SPLITS, random_state = 0, shuffle = True)\n",
    "    \n",
    "    for n, (train_index, val_index) in enumerate(mskf_lr.split(tr, ta)):\n",
    "        x_tr, x_val = tr[train_index], tr[val_index]\n",
    "        y_tr, y_val = ta[train_index], ta[val_index]\n",
    "        \n",
    "        model = KernelRidge(alpha = 80, kernel = 'rbf')\n",
    "        model.fit(x_tr, y_tr)\n",
    "\n",
    "        fold_pred = model.predict(x_val)\n",
    "        pred_value += model.predict(te) / N_SPLITS\n",
    "        oof[val_index,:] = fold_pred\n",
    "        fold_score = log_loss_metric(y_val, fold_pred)\n",
    "        print('KRR: Fold {} Score {}:'.format(n+1, fold_score))\n",
    "    return oof, pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T03:44:05.176639Z",
     "iopub.status.busy": "2020-10-11T03:44:05.174943Z",
     "iopub.status.idle": "2020-10-11T03:49:39.740806Z",
     "shell.execute_reply": "2020-10-11T03:49:39.741441Z"
    },
    "papermill": {
     "duration": 334.916522,
     "end_time": "2020-10-11T03:49:39.741658",
     "exception": false,
     "start_time": "2020-10-11T03:44:04.825136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KRR: Fold 1 Score 0.020053231410728768:\n",
      "KRR: Fold 2 Score 0.020395236493920596:\n",
      "KRR: Fold 3 Score 0.02040202134683225:\n",
      "KRR: Fold 4 Score 0.02020233106232731:\n",
      "KRR: Fold 5 Score 0.02046674791227364:\n"
     ]
    }
   ],
   "source": [
    "lr0_oof = np.zeros([len(fn_train), fn_targets.shape[1]])\n",
    "lr0_test = np.zeros([len(fn_test), fn_targets.shape[1]])\n",
    "lr0_oof, lr0_test = modelling_lr(fn_train, fn_targets, fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T03:49:40.415056Z",
     "iopub.status.busy": "2020-10-11T03:49:40.413347Z",
     "iopub.status.idle": "2020-10-11T03:50:41.784447Z",
     "shell.execute_reply": "2020-10-11T03:50:41.780162Z"
    },
    "papermill": {
     "duration": 61.710534,
     "end_time": "2020-10-11T03:50:41.784636",
     "exception": false,
     "start_time": "2020-10-11T03:49:40.074102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684c1082887e49edb76516c21f88900e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=206.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lr1_test = np.zeros([len(fn_test),fn_targets.shape[1]])\n",
    "lr1_oof = np.zeros([fn_targets.shape[0],fn_targets.shape[1]]) \n",
    "\n",
    "for ind in tqdm(range(len(target_feats))):\n",
    "\n",
    "    ind_target_sum = targets.drop(\"sig_id\", axis=1).copy().values[:, ind].sum()\n",
    "\n",
    "    if ind_target_sum >= N_SPLITS:\n",
    "\n",
    "        skf = StratifiedKFold(n_splits = N_SPLITS, random_state = 0, shuffle = True)\n",
    "        for n, (train_index, val_index) in enumerate(skf.split(lr0_oof, fn_targets[:,ind])):\n",
    "            x_tr, x_val = lr0_oof[train_index, ind].reshape(-1, 1), lr0_oof[val_index, ind].reshape(-1, 1)\n",
    "            y_tr, y_val = fn_targets[train_index,ind], fn_targets[val_index,ind]\n",
    "            model = LogisticRegression(penalty = 'none', max_iter = 1000)\n",
    "            model.fit(x_tr, y_tr)\n",
    "            \n",
    "            lr1_test[:,ind] += model.predict_proba(lr0_test[:, ind].reshape(-1, 1))[:, 1] / N_SPLITS\n",
    "            lr1_oof[val_index, ind] += model.predict_proba(x_val)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T03:50:42.544514Z",
     "iopub.status.busy": "2020-10-11T03:50:42.543421Z",
     "iopub.status.idle": "2020-10-11T03:50:44.127184Z",
     "shell.execute_reply": "2020-10-11T03:50:44.127754Z"
    },
    "papermill": {
     "duration": 1.960683,
     "end_time": "2020-10-11T03:50:44.127905",
     "exception": false,
     "start_time": "2020-10-11T03:50:42.167222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.016353195134826838\n"
     ]
    }
   ],
   "source": [
    "check_lr1 = np.zeros([targets.shape[0], targets.shape[1]-1])\n",
    "check_lr1[cons_train_index,:] = lr1_oof\n",
    "print('OOF log loss: ', log_loss(np.ravel(targets.iloc[:,1:]), np.ravel(check_lr1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.347048,
     "end_time": "2020-10-11T03:50:44.818932",
     "exception": false,
     "start_time": "2020-10-11T03:50:44.471884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1st SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T03:50:45.524258Z",
     "iopub.status.busy": "2020-10-11T03:50:45.523239Z",
     "iopub.status.idle": "2020-10-11T04:14:58.471993Z",
     "shell.execute_reply": "2020-10-11T04:14:58.469155Z"
    },
    "papermill": {
     "duration": 1453.295639,
     "end_time": "2020-10-11T04:14:58.472147",
     "exception": false,
     "start_time": "2020-10-11T03:50:45.176508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9054a60986648e4be2b347ce91d9244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=206.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Target ind 0 score 0.0267522871657328:\n",
      "SVM Target ind 1 score 0.028325951116658196:\n",
      "SVM Target ind 2 score 0.0377679348222106:\n",
      "SVM Target ind 3 score 0.298996150675827:\n",
      "SVM Target ind 4 score 0.47367284922854636:\n",
      "SVM Target ind 5 score 0.11487746841755518:\n",
      "SVM Target ind 6 score 0.0849778533499726:\n",
      "SVM Target ind 7 score 0.15107173928883938:\n",
      "SVM Target ind 8 score 0.0188839674111058:\n",
      "SVM Target ind 9 score 0.41774788362703535:\n",
      "SVM Target ind 10 score 0.566519022333145:\n",
      "SVM Target ind 11 score 0.07872803928766862:\n",
      "SVM Target ind 12 score 0.0110156476564788:\n",
      "SVM Target ind 13 score 0.06609388593886781:\n",
      "SVM Target ind 14 score 0.0188839674111058:\n",
      "SVM Target ind 15 score 0.018883967411105797:\n",
      "SVM Target ind 16 score 0.0755358696444202:\n",
      "SVM Target ind 17 score 0.14005609163236157:\n",
      "SVM Target ind 18 score 0.125893116074033:\n",
      "SVM Target ind 19 score 0.0566519022333154:\n",
      "SVM Target ind 20 score 0.058225566184240796:\n",
      "SVM Target ind 21 score 0.1148774684175552:\n",
      "SVM Target ind 22 score 0.009441983705553398:\n",
      "SVM Target ind 23 score 0.06766754988979319:\n",
      "SVM Target ind 24 score 0.0188839674111058:\n",
      "SVM Target ind 25 score 0.020457631362031197:\n",
      "SVM Target ind 26 score 0.018883967411105797:\n",
      "SVM Target ind 27 score 0.028325951116658196:\n",
      "SVM Target ind 28 score 0.1148774684175552:\n",
      "SVM Target ind 29 score 0.0566519022333154:\n",
      "SVM Target ind 30 score 0.036194270871285204:\n",
      "SVM Target ind 31 score 0.0771095335953456:\n",
      "SVM Target ind 32 score 0.0739622056934948:\n",
      "SVM Target ind 33 score 0.009441983705553401:\n",
      "SVM Target ind 34 score 0.0015736639509263987:\n",
      "SVM Target ind 35 score 0.015995075305086637:\n",
      "SVM Target ind 36 score 0.15124648042858196:\n",
      "SVM Target ind 37 score 0.027124797939191:\n",
      "SVM Target ind 38 score 0.1044778435377165:\n",
      "SVM Target ind 39 score 0.0094419837055534:\n",
      "SVM Target ind 40 score 0.094419837055525:\n",
      "SVM Target ind 41 score 0.12590554647677943:\n",
      "SVM Target ind 42 score 0.056651902233315385:\n",
      "SVM Target ind 43 score 0.3021434785776778:\n",
      "SVM Target ind 44 score 0.1400560916323616:\n",
      "SVM Target ind 45 score 0.180971354356422:\n",
      "SVM Target ind 46 score 0.0110156476564788:\n",
      "SVM Target ind 47 score 0.047423884432121616:\n",
      "SVM Target ind 48 score 0.05979923013516619:\n",
      "SVM Target ind 49 score 0.1054354847120028:\n",
      "SVM Target ind 50 score 0.037767934822210594:\n",
      "SVM Target ind 51 score 0.07890719076961206:\n",
      "SVM Target ind 52 score 0.04563625457683759:\n",
      "SVM Target ind 53 score 0.0094419837055534:\n",
      "SVM Target ind 54 score 0.4421995702100385:\n",
      "SVM Target ind 55 score 0.0660938859388678:\n",
      "SVM Target ind 56 score 0.086551517300898:\n",
      "SVM Target ind 57 score 0.0566519022333154:\n",
      "SVM Target ind 58 score 0.0566519022333154:\n",
      "SVM Target ind 59 score 0.028325951116658196:\n",
      "SVM Target ind 60 score 0.018883967411105793:\n",
      "SVM Target ind 61 score 0.16051372299439182:\n",
      "SVM Target ind 62 score 0.028325951116658196:\n",
      "SVM Target ind 63 score 0.1793920120975412:\n",
      "SVM Target ind 64 score 0.0849778533499726:\n",
      "SVM Target ind 65 score 0.03338897509357419:\n",
      "SVM Target ind 66 score 0.06609388593886781:\n",
      "SVM Target ind 67 score 0.0755358696444202:\n",
      "SVM Target ind 68 score 0.0849778533499726:\n",
      "SVM Target ind 69 score 0.0094419837055534:\n",
      "SVM Target ind 70 score 0.0566519022333154:\n",
      "SVM Target ind 71 score 0.684548680257873:\n",
      "SVM Target ind 72 score 0.16366105089624258:\n",
      "SVM Target ind 73 score 0.04774183629825685:\n",
      "SVM Target ind 74 score 0.039341598773136:\n",
      "SVM Target ind 75 score 0.0094419837055534:\n",
      "SVM Target ind 76 score 0.07553586964442019:\n",
      "SVM Target ind 77 score 0.6088592730169317:\n",
      "SVM Target ind 78 score 0.19041333806197436:\n",
      "SVM Target ind 79 score 0.6642648874720656:\n",
      "SVM Target ind 80 score 0.16225055894789492:\n",
      "SVM Target ind 81 score 0.009441983705553398:\n",
      "SVM Target ind 82 score 0.0015736639509263994:\n",
      "SVM Target ind 83 score 0.24863890424621418:\n",
      "SVM Target ind 84 score 0.0755358696444202:\n",
      "SVM Target ind 85 score 0.0566519022333154:\n",
      "SVM Target ind 86 score 0.022584783288437618:\n",
      "SVM Target ind 87 score 0.039341598773136:\n",
      "SVM Target ind 88 score 0.07721290404483384:\n",
      "SVM Target ind 89 score 0.242375772561839:\n",
      "SVM Target ind 90 score 0.02535790750240864:\n",
      "SVM Target ind 91 score 0.028325951116658196:\n",
      "SVM Target ind 92 score 0.036194270871285204:\n",
      "SVM Target ind 93 score 0.1668083787980934:\n",
      "SVM Target ind 94 score 0.259654551902692:\n",
      "SVM Target ind 95 score 0.07344128945531933:\n",
      "SVM Target ind 96 score 0.08764997356425683:\n",
      "SVM Target ind 97 score 0.020457631362031197:\n",
      "SVM Target ind 98 score 0.11645113236848059:\n",
      "SVM Target ind 99 score 0.5775346699896229:\n",
      "SVM Target ind 100 score 0.028325951116658202:\n",
      "SVM Target ind 101 score 0.06571739794288631:\n",
      "SVM Target ind 102 score 0.1133038044666298:\n",
      "SVM Target ind 103 score 0.10311027686494656:\n",
      "SVM Target ind 104 score 0.0928461731045996:\n",
      "SVM Target ind 105 score 0.3792530121730223:\n",
      "SVM Target ind 106 score 0.0318727335426549:\n",
      "SVM Target ind 107 score 0.045971158228606486:\n",
      "SVM Target ind 108 score 0.1133038044666298:\n",
      "SVM Target ind 109 score 0.14201088202136827:\n",
      "SVM Target ind 110 score 0.05370601981780244:\n",
      "SVM Target ind 111 score 0.058225566184240796:\n",
      "SVM Target ind 112 score 0.039916546630116394:\n",
      "SVM Target ind 113 score 0.04878358247868839:\n",
      "SVM Target ind 114 score 0.1148774684175552:\n",
      "SVM Target ind 115 score 0.047209918527762997:\n",
      "SVM Target ind 116 score 0.08025686149719641:\n",
      "SVM Target ind 117 score 0.06609388593886781:\n",
      "SVM Target ind 118 score 0.11637851892742375:\n",
      "SVM Target ind 119 score 0.21787635797653412:\n",
      "SVM Target ind 120 score 0.009441983705553398:\n",
      "SVM Target ind 121 score 0.009441983705553398:\n",
      "SVM Target ind 122 score 0.09756716495737582:\n",
      "SVM Target ind 123 score 0.018883967411105797:\n",
      "SVM Target ind 124 score 0.09599350100645039:\n",
      "SVM Target ind 125 score 0.0094419837055534:\n",
      "SVM Target ind 126 score 0.04131009905515071:\n",
      "SVM Target ind 127 score 0.06110886724006275:\n",
      "SVM Target ind 128 score 0.11645113236848059:\n",
      "SVM Target ind 129 score 0.039341598773136:\n",
      "SVM Target ind 130 score 0.018883967411105797:\n",
      "SVM Target ind 131 score 0.13376143582866:\n",
      "SVM Target ind 132 score 0.028325951116658196:\n",
      "SVM Target ind 133 score 0.1522731777831692:\n",
      "SVM Target ind 134 score 0.0755358696444202:\n",
      "SVM Target ind 135 score 0.058225566184240796:\n",
      "SVM Target ind 136 score 0.30223037603120917:\n",
      "SVM Target ind 137 score 0.009441983705553398:\n",
      "SVM Target ind 138 score 0.040915262724061395:\n",
      "SVM Target ind 139 score 0.018883967411105797:\n",
      "SVM Target ind 140 score 0.040915262724061395:\n",
      "SVM Target ind 141 score 0.0110156476564788:\n",
      "SVM Target ind 142 score 0.028325951116658196:\n",
      "SVM Target ind 143 score 0.09599350100645039:\n",
      "SVM Target ind 144 score 0.1510717392888394:\n",
      "SVM Target ind 145 score 0.058225566184240796:\n",
      "SVM Target ind 146 score 0.08454839599216883:\n",
      "SVM Target ind 147 score 0.0377679348222106:\n",
      "SVM Target ind 148 score 0.09599350100645039:\n",
      "SVM Target ind 149 score 0.24742502258266805:\n",
      "SVM Target ind 150 score 0.028325951116658196:\n",
      "SVM Target ind 151 score 0.4140538306189051:\n",
      "SVM Target ind 152 score 0.039341598773136:\n",
      "SVM Target ind 153 score 0.19716547993231362:\n",
      "SVM Target ind 154 score 0.0487835824786884:\n",
      "SVM Target ind 155 score 0.086551517300898:\n",
      "SVM Target ind 156 score 0.1542190671906902:\n",
      "SVM Target ind 157 score 0.16768898436559215:\n",
      "SVM Target ind 158 score 0.047209918527763:\n",
      "SVM Target ind 159 score 0.18726601016012356:\n",
      "SVM Target ind 160 score 0.028325951116658196:\n",
      "SVM Target ind 161 score 0.056651902233315406:\n",
      "SVM Target ind 162 score 0.13218777187773462:\n",
      "SVM Target ind 163 score 0.10778050385643265:\n",
      "SVM Target ind 164 score 0.0755358696444202:\n",
      "SVM Target ind 165 score 0.009441983705553398:\n",
      "SVM Target ind 166 score 0.16208738694531719:\n",
      "SVM Target ind 167 score 0.029899615067583596:\n",
      "SVM Target ind 168 score 0.08812518125182339:\n",
      "SVM Target ind 169 score 0.07518409849826634:\n",
      "SVM Target ind 170 score 0.018883967411105797:\n",
      "SVM Target ind 171 score 0.09358236502516057:\n",
      "SVM Target ind 172 score 0.009441983705553398:\n",
      "SVM Target ind 173 score 0.052083647737664183:\n",
      "SVM Target ind 174 score 0.05822556618424081:\n",
      "SVM Target ind 175 score 0.039341598773136:\n",
      "SVM Target ind 176 score 0.3713846924183954:\n",
      "SVM Target ind 177 score 0.6357643130338573:\n",
      "SVM Target ind 178 score 0.06924121384071859:\n",
      "SVM Target ind 179 score 0.0566519022333154:\n",
      "SVM Target ind 180 score 0.0566519022333154:\n",
      "SVM Target ind 181 score 0.039341598773136:\n",
      "SVM Target ind 182 score 0.4201682748970828:\n",
      "SVM Target ind 183 score 0.039341598773136:\n",
      "SVM Target ind 184 score 0.10727714714417527:\n",
      "SVM Target ind 185 score 0.0094419837055534:\n",
      "SVM Target ind 186 score 0.0298996150675836:\n",
      "SVM Target ind 187 score 0.094419837055525:\n",
      "SVM Target ind 188 score 0.032345693082092376:\n",
      "SVM Target ind 189 score 0.029899615067583596:\n",
      "SVM Target ind 190 score 0.05822556618424079:\n",
      "SVM Target ind 191 score 0.047209918527763:\n",
      "SVM Target ind 192 score 0.011015647656478798:\n",
      "SVM Target ind 193 score 0.0566519022333154:\n",
      "SVM Target ind 194 score 0.1519872334051322:\n",
      "SVM Target ind 195 score 0.028325951116658196:\n",
      "SVM Target ind 196 score 0.009441983705553398:\n",
      "SVM Target ind 197 score 0.039341598773136:\n",
      "SVM Target ind 198 score 0.0755358696444202:\n",
      "SVM Target ind 199 score 0.13812470816611183:\n",
      "SVM Target ind 200 score 0.1148774684175552:\n",
      "SVM Target ind 201 score 0.0094419837055534:\n",
      "SVM Target ind 202 score 0.2317331252523695:\n",
      "SVM Target ind 203 score 0.040915262724061395:\n",
      "SVM Target ind 204 score 0.04019826154166387:\n",
      "SVM Target ind 205 score 0.047209918527763:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_STARTS = 3\n",
    "N_SPLITS = 5\n",
    "\n",
    "svm0_oof = np.zeros([len(fn_train), fn_targets.shape[1]])\n",
    "svm0_test = np.zeros([len(fn_test), fn_targets.shape[1]])\n",
    "\n",
    "for ind in tqdm(range(len(target_feats))):\n",
    "\n",
    "    ind_target_sum = targets.drop(\"sig_id\", axis=1).copy().values[:, ind].sum()\n",
    "\n",
    "    if ind_target_sum >= N_SPLITS:\n",
    "\n",
    "        for seed in range(N_STARTS):\n",
    "\n",
    "            skf = StratifiedKFold(n_splits = N_SPLITS, random_state = seed, shuffle = True)\n",
    "\n",
    "            for n, (train_index, val_index) in enumerate(skf.split(fn_train, fn_targets[:,ind])):\n",
    "                \n",
    "                x_tr, x_val = fn_train[train_index], fn_train[val_index]\n",
    "                y_tr, y_val = fn_targets[train_index,ind], fn_targets[val_index,ind]\n",
    "\n",
    "                model = SVC(C = 40, cache_size = 2000)\n",
    "                model.fit(x_tr, y_tr)\n",
    "                svm0_test[:, ind] += model.decision_function(fn_test) / (N_SPLITS * N_STARTS)\n",
    "                svm0_oof[val_index, ind] += model.decision_function(x_val) / N_STARTS\n",
    "\n",
    "    score = log_loss(fn_targets[:, ind], svm0_oof[:, ind])\n",
    "    print('SVM Target ind {} score {}:'.format(ind,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T04:14:59.226988Z",
     "iopub.status.busy": "2020-10-11T04:14:59.225390Z",
     "iopub.status.idle": "2020-10-11T04:16:46.794187Z",
     "shell.execute_reply": "2020-10-11T04:16:46.790673Z"
    },
    "papermill": {
     "duration": 107.951909,
     "end_time": "2020-10-11T04:16:46.794364",
     "exception": false,
     "start_time": "2020-10-11T04:14:58.842455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721237623e674c8481841bd5fe1fd639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=206.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Target ind 0 score 0.0267522871657328:\n",
      "SVM Target ind 1 score 0.028325951116658196:\n",
      "SVM Target ind 2 score 0.0377679348222106:\n",
      "SVM Target ind 3 score 0.298996150675827:\n",
      "SVM Target ind 4 score 0.47367284922854636:\n",
      "SVM Target ind 5 score 0.11487746841755518:\n",
      "SVM Target ind 6 score 0.0849778533499726:\n",
      "SVM Target ind 7 score 0.15107173928883938:\n",
      "SVM Target ind 8 score 0.0188839674111058:\n",
      "SVM Target ind 9 score 0.41774788362703535:\n",
      "SVM Target ind 10 score 0.566519022333145:\n",
      "SVM Target ind 11 score 0.07872803928766862:\n",
      "SVM Target ind 12 score 0.0110156476564788:\n",
      "SVM Target ind 13 score 0.06609388593886781:\n",
      "SVM Target ind 14 score 0.0188839674111058:\n",
      "SVM Target ind 15 score 0.018883967411105797:\n",
      "SVM Target ind 16 score 0.0755358696444202:\n",
      "SVM Target ind 17 score 0.14005609163236157:\n",
      "SVM Target ind 18 score 0.125893116074033:\n",
      "SVM Target ind 19 score 0.0566519022333154:\n",
      "SVM Target ind 20 score 0.058225566184240796:\n",
      "SVM Target ind 21 score 0.1148774684175552:\n",
      "SVM Target ind 22 score 0.009441983705553398:\n",
      "SVM Target ind 23 score 0.06766754988979319:\n",
      "SVM Target ind 24 score 0.0188839674111058:\n",
      "SVM Target ind 25 score 0.020457631362031197:\n",
      "SVM Target ind 26 score 0.018883967411105797:\n",
      "SVM Target ind 27 score 0.028325951116658196:\n",
      "SVM Target ind 28 score 0.1148774684175552:\n",
      "SVM Target ind 29 score 0.0566519022333154:\n",
      "SVM Target ind 30 score 0.036194270871285204:\n",
      "SVM Target ind 31 score 0.0771095335953456:\n",
      "SVM Target ind 32 score 0.0739622056934948:\n",
      "SVM Target ind 33 score 0.009441983705553401:\n",
      "SVM Target ind 34 score 0.0015736639509263987:\n",
      "SVM Target ind 35 score 0.015995075305086637:\n",
      "SVM Target ind 36 score 0.15124648042858196:\n",
      "SVM Target ind 37 score 0.027124797939191:\n",
      "SVM Target ind 38 score 0.1044778435377165:\n",
      "SVM Target ind 39 score 0.0094419837055534:\n",
      "SVM Target ind 40 score 0.094419837055525:\n",
      "SVM Target ind 41 score 0.12590554647677943:\n",
      "SVM Target ind 42 score 0.056651902233315385:\n",
      "SVM Target ind 43 score 0.3021434785776778:\n",
      "SVM Target ind 44 score 0.1400560916323616:\n",
      "SVM Target ind 45 score 0.180971354356422:\n",
      "SVM Target ind 46 score 0.0110156476564788:\n",
      "SVM Target ind 47 score 0.047423884432121616:\n",
      "SVM Target ind 48 score 0.05979923013516619:\n",
      "SVM Target ind 49 score 0.1054354847120028:\n",
      "SVM Target ind 50 score 0.037767934822210594:\n",
      "SVM Target ind 51 score 0.07890719076961206:\n",
      "SVM Target ind 52 score 0.04563625457683759:\n",
      "SVM Target ind 53 score 0.0094419837055534:\n",
      "SVM Target ind 54 score 0.4421995702100385:\n",
      "SVM Target ind 55 score 0.0660938859388678:\n",
      "SVM Target ind 56 score 0.086551517300898:\n",
      "SVM Target ind 57 score 0.0566519022333154:\n",
      "SVM Target ind 58 score 0.0566519022333154:\n",
      "SVM Target ind 59 score 0.028325951116658196:\n",
      "SVM Target ind 60 score 0.018883967411105793:\n",
      "SVM Target ind 61 score 0.16051372299439182:\n",
      "SVM Target ind 62 score 0.028325951116658196:\n",
      "SVM Target ind 63 score 0.1793920120975412:\n",
      "SVM Target ind 64 score 0.0849778533499726:\n",
      "SVM Target ind 65 score 0.03338897509357419:\n",
      "SVM Target ind 66 score 0.06609388593886781:\n",
      "SVM Target ind 67 score 0.0755358696444202:\n",
      "SVM Target ind 68 score 0.0849778533499726:\n",
      "SVM Target ind 69 score 0.0094419837055534:\n",
      "SVM Target ind 70 score 0.0566519022333154:\n",
      "SVM Target ind 71 score 0.684548680257873:\n",
      "SVM Target ind 72 score 0.16366105089624258:\n",
      "SVM Target ind 73 score 0.04774183629825685:\n",
      "SVM Target ind 74 score 0.039341598773136:\n",
      "SVM Target ind 75 score 0.0094419837055534:\n",
      "SVM Target ind 76 score 0.07553586964442019:\n",
      "SVM Target ind 77 score 0.6088592730169317:\n",
      "SVM Target ind 78 score 0.19041333806197436:\n",
      "SVM Target ind 79 score 0.6642648874720656:\n",
      "SVM Target ind 80 score 0.16225055894789492:\n",
      "SVM Target ind 81 score 0.009441983705553398:\n",
      "SVM Target ind 82 score 0.0015736639509263994:\n",
      "SVM Target ind 83 score 0.24863890424621418:\n",
      "SVM Target ind 84 score 0.0755358696444202:\n",
      "SVM Target ind 85 score 0.0566519022333154:\n",
      "SVM Target ind 86 score 0.022584783288437618:\n",
      "SVM Target ind 87 score 0.039341598773136:\n",
      "SVM Target ind 88 score 0.07721290404483384:\n",
      "SVM Target ind 89 score 0.242375772561839:\n",
      "SVM Target ind 90 score 0.02535790750240864:\n",
      "SVM Target ind 91 score 0.028325951116658196:\n",
      "SVM Target ind 92 score 0.036194270871285204:\n",
      "SVM Target ind 93 score 0.1668083787980934:\n",
      "SVM Target ind 94 score 0.259654551902692:\n",
      "SVM Target ind 95 score 0.07344128945531933:\n",
      "SVM Target ind 96 score 0.08764997356425683:\n",
      "SVM Target ind 97 score 0.020457631362031197:\n",
      "SVM Target ind 98 score 0.11645113236848059:\n",
      "SVM Target ind 99 score 0.5775346699896229:\n",
      "SVM Target ind 100 score 0.028325951116658202:\n",
      "SVM Target ind 101 score 0.06571739794288631:\n",
      "SVM Target ind 102 score 0.1133038044666298:\n",
      "SVM Target ind 103 score 0.10311027686494656:\n",
      "SVM Target ind 104 score 0.0928461731045996:\n",
      "SVM Target ind 105 score 0.3792530121730223:\n",
      "SVM Target ind 106 score 0.0318727335426549:\n",
      "SVM Target ind 107 score 0.045971158228606486:\n",
      "SVM Target ind 108 score 0.1133038044666298:\n",
      "SVM Target ind 109 score 0.14201088202136827:\n",
      "SVM Target ind 110 score 0.05370601981780244:\n",
      "SVM Target ind 111 score 0.058225566184240796:\n",
      "SVM Target ind 112 score 0.039916546630116394:\n",
      "SVM Target ind 113 score 0.04878358247868839:\n",
      "SVM Target ind 114 score 0.1148774684175552:\n",
      "SVM Target ind 115 score 0.047209918527762997:\n",
      "SVM Target ind 116 score 0.08025686149719641:\n",
      "SVM Target ind 117 score 0.06609388593886781:\n",
      "SVM Target ind 118 score 0.11637851892742375:\n",
      "SVM Target ind 119 score 0.21787635797653412:\n",
      "SVM Target ind 120 score 0.009441983705553398:\n",
      "SVM Target ind 121 score 0.009441983705553398:\n",
      "SVM Target ind 122 score 0.09756716495737582:\n",
      "SVM Target ind 123 score 0.018883967411105797:\n",
      "SVM Target ind 124 score 0.09599350100645039:\n",
      "SVM Target ind 125 score 0.0094419837055534:\n",
      "SVM Target ind 126 score 0.04131009905515071:\n",
      "SVM Target ind 127 score 0.06110886724006275:\n",
      "SVM Target ind 128 score 0.11645113236848059:\n",
      "SVM Target ind 129 score 0.039341598773136:\n",
      "SVM Target ind 130 score 0.018883967411105797:\n",
      "SVM Target ind 131 score 0.13376143582866:\n",
      "SVM Target ind 132 score 0.028325951116658196:\n",
      "SVM Target ind 133 score 0.1522731777831692:\n",
      "SVM Target ind 134 score 0.0755358696444202:\n",
      "SVM Target ind 135 score 0.058225566184240796:\n",
      "SVM Target ind 136 score 0.30223037603120917:\n",
      "SVM Target ind 137 score 0.009441983705553398:\n",
      "SVM Target ind 138 score 0.040915262724061395:\n",
      "SVM Target ind 139 score 0.018883967411105797:\n",
      "SVM Target ind 140 score 0.040915262724061395:\n",
      "SVM Target ind 141 score 0.0110156476564788:\n",
      "SVM Target ind 142 score 0.028325951116658196:\n",
      "SVM Target ind 143 score 0.09599350100645039:\n",
      "SVM Target ind 144 score 0.1510717392888394:\n",
      "SVM Target ind 145 score 0.058225566184240796:\n",
      "SVM Target ind 146 score 0.08454839599216883:\n",
      "SVM Target ind 147 score 0.0377679348222106:\n",
      "SVM Target ind 148 score 0.09599350100645039:\n",
      "SVM Target ind 149 score 0.24742502258266805:\n",
      "SVM Target ind 150 score 0.028325951116658196:\n",
      "SVM Target ind 151 score 0.4140538306189051:\n",
      "SVM Target ind 152 score 0.039341598773136:\n",
      "SVM Target ind 153 score 0.19716547993231362:\n",
      "SVM Target ind 154 score 0.0487835824786884:\n",
      "SVM Target ind 155 score 0.086551517300898:\n",
      "SVM Target ind 156 score 0.1542190671906902:\n",
      "SVM Target ind 157 score 0.16768898436559215:\n",
      "SVM Target ind 158 score 0.047209918527763:\n",
      "SVM Target ind 159 score 0.18726601016012356:\n",
      "SVM Target ind 160 score 0.028325951116658196:\n",
      "SVM Target ind 161 score 0.056651902233315406:\n",
      "SVM Target ind 162 score 0.13218777187773462:\n",
      "SVM Target ind 163 score 0.10778050385643265:\n",
      "SVM Target ind 164 score 0.0755358696444202:\n",
      "SVM Target ind 165 score 0.009441983705553398:\n",
      "SVM Target ind 166 score 0.16208738694531719:\n",
      "SVM Target ind 167 score 0.029899615067583596:\n",
      "SVM Target ind 168 score 0.08812518125182339:\n",
      "SVM Target ind 169 score 0.07518409849826634:\n",
      "SVM Target ind 170 score 0.018883967411105797:\n",
      "SVM Target ind 171 score 0.09358236502516057:\n",
      "SVM Target ind 172 score 0.009441983705553398:\n",
      "SVM Target ind 173 score 0.052083647737664183:\n",
      "SVM Target ind 174 score 0.05822556618424081:\n",
      "SVM Target ind 175 score 0.039341598773136:\n",
      "SVM Target ind 176 score 0.3713846924183954:\n",
      "SVM Target ind 177 score 0.6357643130338573:\n",
      "SVM Target ind 178 score 0.06924121384071859:\n",
      "SVM Target ind 179 score 0.0566519022333154:\n",
      "SVM Target ind 180 score 0.0566519022333154:\n",
      "SVM Target ind 181 score 0.039341598773136:\n",
      "SVM Target ind 182 score 0.4201682748970828:\n",
      "SVM Target ind 183 score 0.039341598773136:\n",
      "SVM Target ind 184 score 0.10727714714417527:\n",
      "SVM Target ind 185 score 0.0094419837055534:\n",
      "SVM Target ind 186 score 0.0298996150675836:\n",
      "SVM Target ind 187 score 0.094419837055525:\n",
      "SVM Target ind 188 score 0.032345693082092376:\n",
      "SVM Target ind 189 score 0.029899615067583596:\n",
      "SVM Target ind 190 score 0.05822556618424079:\n",
      "SVM Target ind 191 score 0.047209918527763:\n",
      "SVM Target ind 192 score 0.011015647656478798:\n",
      "SVM Target ind 193 score 0.0566519022333154:\n",
      "SVM Target ind 194 score 0.1519872334051322:\n",
      "SVM Target ind 195 score 0.028325951116658196:\n",
      "SVM Target ind 196 score 0.009441983705553398:\n",
      "SVM Target ind 197 score 0.039341598773136:\n",
      "SVM Target ind 198 score 0.0755358696444202:\n",
      "SVM Target ind 199 score 0.13812470816611183:\n",
      "SVM Target ind 200 score 0.1148774684175552:\n",
      "SVM Target ind 201 score 0.0094419837055534:\n",
      "SVM Target ind 202 score 0.2317331252523695:\n",
      "SVM Target ind 203 score 0.040915262724061395:\n",
      "SVM Target ind 204 score 0.04019826154166387:\n",
      "SVM Target ind 205 score 0.047209918527763:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_STARTS = 3\n",
    "N_SPLITS = 5\n",
    "\n",
    "svm1_test = np.zeros([len(fn_test),fn_targets.shape[1]])\n",
    "svm1_oof = np.zeros([fn_targets.shape[0],fn_targets.shape[1]]) \n",
    "\n",
    "for ind in tqdm(range(len(target_feats))):\n",
    "\n",
    "    ind_target_sum = targets.drop(\"sig_id\", axis=1).copy().values[:, ind].sum()\n",
    "\n",
    "    if ind_target_sum >= N_SPLITS:\n",
    "\n",
    "        for seed in range(N_STARTS):\n",
    "\n",
    "            skf = StratifiedKFold(n_splits = N_SPLITS, random_state = seed, shuffle = True)\n",
    "\n",
    "            for n, (train_index, val_index) in enumerate(skf.split(svm0_oof, fn_targets[:,ind])):\n",
    "\n",
    "                x_tr, x_val = svm0_oof[train_index, ind].reshape(-1, 1), svm0_oof[val_index, ind].reshape(-1, 1)\n",
    "                y_tr, y_val = fn_targets[train_index,ind], fn_targets[val_index,ind]\n",
    "\n",
    "                model = LogisticRegression(C = 35, max_iter = 1000)\n",
    "                model.fit(x_tr, y_tr)\n",
    "                svm1_test[:, ind] += model.predict_proba(svm0_test[:, ind].reshape(-1, 1))[:, 1] / (N_SPLITS * N_STARTS)\n",
    "                svm1_oof[val_index, ind] += model.predict_proba(x_val)[:, 1] / N_STARTS\n",
    "\n",
    "    score = log_loss(fn_targets[:, ind], svm0_oof[:, ind])\n",
    "    print('SVM Target ind {} score {}:'.format(ind,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T04:16:47.774752Z",
     "iopub.status.busy": "2020-10-11T04:16:47.773688Z",
     "iopub.status.idle": "2020-10-11T04:16:49.033725Z",
     "shell.execute_reply": "2020-10-11T04:16:49.033003Z"
    },
    "papermill": {
     "duration": 1.730724,
     "end_time": "2020-10-11T04:16:49.033863",
     "exception": false,
     "start_time": "2020-10-11T04:16:47.303139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.015288382527672173\n"
     ]
    }
   ],
   "source": [
    "check_svm1 = np.zeros([targets.shape[0], targets.shape[1]-1])\n",
    "check_svm1[cons_train_index,:] = svm1_oof\n",
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(check_svm1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.444476,
     "end_time": "2020-10-11T04:16:49.940061",
     "exception": false,
     "start_time": "2020-10-11T04:16:49.495585",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T04:16:50.903882Z",
     "iopub.status.busy": "2020-10-11T04:16:50.901883Z",
     "iopub.status.idle": "2020-10-11T04:16:50.904598Z",
     "shell.execute_reply": "2020-10-11T04:16:50.905066Z"
    },
    "papermill": {
     "duration": 0.489117,
     "end_time": "2020-10-11T04:16:50.905231",
     "exception": false,
     "start_time": "2020-10-11T04:16:50.416114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weight optimization\n",
    "#class OptimizedRounder(object):\n",
    "#    def __init__(self, length):\n",
    "#        self.coef_ = [0 for i in range(length)]\n",
    "\n",
    "#    def _log_loss(self, coef, Xs, y):\n",
    "#        X_p = np.zeros_like(Xs[0])\n",
    "#        for i in range(len(coef)):\n",
    "#            X_p += coef[i] * Xs[i]\n",
    "#        return log_loss(np.ravel(y), np.ravel(np.array(X_p)))\n",
    "    \n",
    "#    def fit(self, X, y, random_flg = False):\n",
    "#        loss_partial = partial(self._log_loss, X=X, y=y)\n",
    "#        if random_flg:\n",
    "#            initial_coef = [np.random.uniform(0.4,0.5), np.random.uniform(0.5,0.6), np.random.uniform(0.6,0.7)]\n",
    "#        else:\n",
    "#            initial_coef = [1/length for i in range(length)]\n",
    "#        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead') #Powell\n",
    "        \n",
    "#    def predict(self, X, coef):\n",
    "#        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "#    def coefficients(self):\n",
    "#        return self.coef_\n",
    "    \n",
    "#best_score = 100\n",
    "#for i in range(10):\n",
    "#    optR = OptimizedRounder()\n",
    "#    optR.fit(, y, random_flg=False)\n",
    "#    coefficients = optR.coefficients()\n",
    "#    score = qwk(new_train.accuracy_group, final_valid_pred)\n",
    "#    print(i, np.sort(coefficients), score)\n",
    "#    if score > best_score:\n",
    "#        best_score = score\n",
    "#        best_coefficients = coefficients\n",
    "#final_test_pred = pd.cut(np.array(test_exp_accuracy).reshape(-1,), [-np.inf] + list(np.sort(best_coefficients)) + [np.inf], labels = [0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T04:16:51.898801Z",
     "iopub.status.busy": "2020-10-11T04:16:51.897540Z",
     "iopub.status.idle": "2020-10-11T04:16:53.190078Z",
     "shell.execute_reply": "2020-10-11T04:16:53.190720Z"
    },
    "papermill": {
     "duration": 1.795591,
     "end_time": "2020-10-11T04:16:53.190868",
     "exception": false,
     "start_time": "2020-10-11T04:16:51.395277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.014804078351940858\n"
     ]
    }
   ],
   "source": [
    "check = 0.1 * check_svm1 + 0.1 * check_lr1 + 0.2 * check_xgb1 + 0.6 * check_pytorch1\n",
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(check)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T04:16:54.258213Z",
     "iopub.status.busy": "2020-10-11T04:16:54.255025Z",
     "iopub.status.idle": "2020-10-11T04:16:58.556172Z",
     "shell.execute_reply": "2020-10-11T04:16:58.555308Z"
    },
    "papermill": {
     "duration": 4.893335,
     "end_time": "2020-10-11T04:16:58.556287",
     "exception": false,
     "start_time": "2020-10-11T04:16:53.662952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_torch = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "sub_xgb = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "sub_lr = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "sub_svm = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "\n",
    "sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "\n",
    "sub_torch.loc[cons_test_index,target_feats] = pytorch1_test\n",
    "sub_torch.loc[noncons_test_index,target_feats] = 0\n",
    "sub_xgb.loc[cons_test_index,target_feats] = xgb1_test\n",
    "sub_xgb.loc[noncons_test_index,target_feats] = 0\n",
    "sub_lr.loc[cons_test_index,target_feats] = lr1_test\n",
    "sub_lr.loc[noncons_test_index,target_feats] = 0\n",
    "sub_svm.loc[cons_test_index,target_feats] = svm1_test\n",
    "sub_svm.loc[noncons_test_index,target_feats] = 0\n",
    "\n",
    "sub[target_feats] = 0.1 * sub_svm.iloc[:,1:] + 0.1 * sub_lr.iloc[:,1:] + 0.2 * sub_xgb.iloc[:,1:] + 0.6 * sub_torch.iloc[:,1:]\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.443736,
     "end_time": "2020-10-11T04:16:59.439602",
     "exception": false,
     "start_time": "2020-10-11T04:16:58.995866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 4678.419131,
   "end_time": "2020-10-11T04:17:00.490072",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-11T02:59:02.070941",
   "version": "2.1.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "07170421213f491e90cf87fc9c5309c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2bfc086446be4b4fb4fe3583cb54ef29",
       "max": 206.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0a1ff6966a4e410d989ad0e287702101",
       "value": 206.0
      }
     },
     "0a1ff6966a4e410d989ad0e287702101": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "0f6fcbe29a0a4b7591f5153a7898b40e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7b06b81aa1ac41c8af68aa5401fbd309",
       "placeholder": "",
       "style": "IPY_MODEL_f9ced2742e9d4595b17a4debacee9359",
       "value": " 206/206 [01:00&lt;00:00,  3.39it/s]"
      }
     },
     "24b6c62d497c4ae1a9937908bbc07dfa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_957a128303d1487bb8666f0cd67c0e23",
       "placeholder": "",
       "style": "IPY_MODEL_ddee9319a9b24df0a17c7a72240d3769",
       "value": " 206/206 [01:47&lt;00:00,  1.92it/s]"
      }
     },
     "2bfc086446be4b4fb4fe3583cb54ef29": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3a1050d5881046e887b7ff442b7512ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4cb8f2539a2145c1a746af0ba52d5ec7",
       "placeholder": "",
       "style": "IPY_MODEL_fe9e3c94ba8c4818b27ff8ed7ff5dfe7",
       "value": " 206/206 [24:16&lt;00:00,  7.07s/it]"
      }
     },
     "4110694f265f47f89650ebf502cc4dd2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "42864e64ea4945aa92421280d1edd5d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4cb8f2539a2145c1a746af0ba52d5ec7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "61c27f52b75e4c809878adbe80f60fa7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "684c1082887e49edb76516c21f88900e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_be76fccf7462403095c931bcf8d72aff",
        "IPY_MODEL_0f6fcbe29a0a4b7591f5153a7898b40e"
       ],
       "layout": "IPY_MODEL_42864e64ea4945aa92421280d1edd5d6"
      }
     },
     "721237623e674c8481841bd5fe1fd639": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_07170421213f491e90cf87fc9c5309c1",
        "IPY_MODEL_24b6c62d497c4ae1a9937908bbc07dfa"
       ],
       "layout": "IPY_MODEL_e6f5449e0c844b1c85ad1c067b277781"
      }
     },
     "7b06b81aa1ac41c8af68aa5401fbd309": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7fbbd64794354a3a9612892b4508d1f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "957a128303d1487bb8666f0cd67c0e23": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9ed01dec322046a9bdcc7885aadfa2f8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a1f6cb5d8bb9478b8647b97061a7f980": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "be76fccf7462403095c931bcf8d72aff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4110694f265f47f89650ebf502cc4dd2",
       "max": 206.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_61c27f52b75e4c809878adbe80f60fa7",
       "value": 206.0
      }
     },
     "ddee9319a9b24df0a17c7a72240d3769": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e6f5449e0c844b1c85ad1c067b277781": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e9054a60986648e4be2b347ce91d9244": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f53d2cea53934d96a518f42018fe2439",
        "IPY_MODEL_3a1050d5881046e887b7ff442b7512ef"
       ],
       "layout": "IPY_MODEL_9ed01dec322046a9bdcc7885aadfa2f8"
      }
     },
     "f53d2cea53934d96a518f42018fe2439": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7fbbd64794354a3a9612892b4508d1f9",
       "max": 206.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a1f6cb5d8bb9478b8647b97061a7f980",
       "value": 206.0
      }
     },
     "f9ced2742e9d4595b17a4debacee9359": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fe9e3c94ba8c4818b27ff8ed7ff5dfe7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
