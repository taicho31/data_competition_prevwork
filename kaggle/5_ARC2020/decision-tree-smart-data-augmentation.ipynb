{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "- incorporate data augmentation for cases where output size is larger than input size and output size is fixed\n",
    "- delete method2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import combinations,permutations\n",
    "from sklearn.tree import *\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import random\n",
    "from math import floor\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data_path = Path(\"/kaggle/input/abstraction-and-reasoning-challenge\")\n",
    "train_path = data_path/'training'\n",
    "eval_path = data_path/'evaluation'\n",
    "test_path = data_path/'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(task, index): # modify mistakes in data\n",
    "    # 025d127b\n",
    "    if index == \"025d127b.json\":\n",
    "        for i in range(9, 12):\n",
    "            for j in range(3, 8):\n",
    "                task['train'][0]['output'][i][j] = 0\n",
    "        for i in range(7, 10):\n",
    "            for j in range(3, 6):\n",
    "                task['train'][0]['output'][i][j] = 2\n",
    "        task['train'][0]['output'][8][4] = 0\n",
    "    # ef135b50\n",
    "    elif index == \"ef135b50.json\":\n",
    "        task['test'][0]['output'][6][4] = 9\n",
    "    # bd14c3bf\n",
    "    elif index == \"bd14c3bf.json\":\n",
    "        for i in range(3):\n",
    "            for j in range(5):\n",
    "                if task['test'][0]['input'][i][j] == 1:\n",
    "                    task['test'][0]['input'][i][j] = 2\n",
    "    # a8610ef7\n",
    "    elif index == \"a8610ef7.json\":\n",
    "        for i in range(6):\n",
    "            for j in range(6):\n",
    "                if task['test'][0]['output'][i][j] == 8:\n",
    "                    task['test'][0]['output'][i][j] = 5\n",
    "        task['train'][3]['input'][0][1] = 2\n",
    "        task['train'][3]['input'][5][1] = 2\n",
    "    # 54db823b\n",
    "    elif index == \"54db823b.json\":\n",
    "        task['train'][0]['output'][2][3] = 3\n",
    "        task['train'][0]['output'][2][4] = 9\n",
    "    # e5062a87\n",
    "    elif index == \"e5062a87.json\":\n",
    "        for j in range(3, 7):\n",
    "            task['train'][1]['output'][1][j] = 2\n",
    "    # 1b60fb0c\n",
    "    elif index == \"1b60fb0c.json\":\n",
    "        task['train'][1]['output'][8][8] = 0\n",
    "        task['train'][1]['output'][8][9] = 0\n",
    "    # 82819916\n",
    "    elif index == \"82819916.json\":\n",
    "        task['train'][0]['output'][4][5] = 4\n",
    "    # fea12743\n",
    "    elif index == \"fea12743.json\":\n",
    "        for i in range(11, 16):\n",
    "            for j in range(6):\n",
    "                if task['train'][0]['output'][i][j] == 2:\n",
    "                    task['train'][0]['output'][i][j] = 8\n",
    "    # 42a50994\n",
    "    elif index == \"42a50994.json\":\n",
    "        task['train'][0]['output'][1][0] = 8\n",
    "        task['train'][0]['output'][0][1] = 8\n",
    "    # f8be4b64\n",
    "    elif index == \"f8be4b64.json\":\n",
    "        for j in range(19):\n",
    "            if task['test'][0]['output'][12][j] == 0:\n",
    "                task['test'][0]['output'][12][j] = 1\n",
    "        task['test'][0]['output'][12][8] = 0\n",
    "    # d511f180\n",
    "    elif index == \"d511f180.json\":\n",
    "        task['train'][1]['output'][2][2] = 9\n",
    "    # 10fcaaa3\n",
    "    elif index == \"10fcaaa3.json\":\n",
    "        task['train'][1]['output'][4][7] = 8\n",
    "    # cbded52d\n",
    "    elif index == \"cbded52d.json\":\n",
    "        task['train'][0]['input'][4][6] = 1\n",
    "    # 11852cab\n",
    "    elif index == \"11852cab.json\":\n",
    "        task['train'][0]['input'][1][2] = 3\n",
    "    # 868de0fa\n",
    "    elif index == \"868de0fa.json\":\n",
    "        for j in range(2, 9):\n",
    "            task['train'][2]['input'][9][j] = 0\n",
    "            task['train'][2]['input'][10][j] = 1\n",
    "            task['train'][2]['input'][15][j] = 0\n",
    "            task['train'][2]['input'][16][j] = 1\n",
    "        task['train'][2]['input'][15][2] = 1\n",
    "        task['train'][2]['input'][15][8] = 1\n",
    "    # 6d58a25d\n",
    "    elif index == \"6d58a25d.json\":\n",
    "        task['train'][0]['output'][10][0] = 0\n",
    "        task['train'][2]['output'][6][13] = 4\n",
    "    # a9f96cdd\n",
    "    elif index == \"a9f96cdd.json\":\n",
    "        task['train'][3]['output'][1][3] = 0\n",
    "    # 48131b3c\n",
    "    elif index == \"48131b3c.json\":\n",
    "        task['train'][2]['output'][4][4] = 0\n",
    "    # 150deff5\n",
    "    elif index == \"150deff5.json\":\n",
    "        aux = task['train'][2]['output'].copy()\n",
    "        task['train'][2]['output'] = task['train'][2]['input'].copy()\n",
    "        task['train'][2]['input'] = aux\n",
    "    # 17cae0c1\n",
    "    elif index == \"17cae0c1.json\":\n",
    "        for i in range(3):\n",
    "            for j in range(3, 6):\n",
    "                task['test'][0]['output'][i][j] = 9\n",
    "    # e48d4e1a\n",
    "    elif index == \"e48d4e1a.json\":\n",
    "        task['train'][3]['input'][0][9] = 5\n",
    "        task['train'][3]['output'][0][9] = 0\n",
    "    # 8fbca751\n",
    "    elif index == \"8fbca751.json\":\n",
    "        task['train'][1]['output'][1][3] = 2\n",
    "        task['train'][1]['output'][2][3] = 8\n",
    "    # 4938f0c2\n",
    "    elif index == \"4938f0c2.json\":\n",
    "        for i in range(12):\n",
    "            for j in range(6,13):\n",
    "                if task['train'][2]['input'][i][j]==2:\n",
    "                    task['train'][2]['input'][i][j] = 0\n",
    "        for i in range(5,11):\n",
    "            for j in range(7):\n",
    "                if task['train'][2]['input'][i][j]==2:\n",
    "                    task['train'][2]['input'][i][j] = 0\n",
    "    # 9aec4887\n",
    "    elif index == \"9aec4887.json\":\n",
    "        task['train'][0]['output'][1][4] = 8\n",
    "    # b0f4d537\n",
    "    elif index == \"b0f4d537.json\":\n",
    "        for i in range(9):\n",
    "            task['train'][0]['output'][i][3] = 0\n",
    "            task['train'][0]['output'][i][4] = 1\n",
    "        task['train'][0]['output'][2][3] = 3\n",
    "        task['train'][0]['output'][2][4] = 3\n",
    "        task['train'][0]['output'][5][3] = 2\n",
    "    # aa300dc3\n",
    "    elif index == \"aa300dc3.json\":\n",
    "        task['train'][1]['input'][1][7] = 5\n",
    "        task['train'][1]['output'][1][7] = 5\n",
    "        task['train'][1]['input'][8][2] = 5\n",
    "        task['train'][1]['output'][8][2] = 5\n",
    "    # ad7e01d0\n",
    "    elif index == \"ad7e01d0.json\":\n",
    "        task['train'][0]['output'][6][7] = 0\n",
    "    # a8610ef7\n",
    "    elif index == \"a8610ef7.json\":\n",
    "        task['train'][3]['input'][0][1] = 0\n",
    "        task['train'][3]['input'][5][1] = 0\n",
    "        task['train'][3]['output'][0][1] = 0\n",
    "        task['train'][3]['output'][5][1] = 0\n",
    "    # 97239e3d\n",
    "    elif index == \"97239e3d.json\":\n",
    "        task['test'][0]['input'][14][6] = 0\n",
    "        task['test'][0]['input'][14][10] = 0\n",
    "    # d687bc17\n",
    "    elif index == \"d687bc17.json\":\n",
    "        task['train'][2]['output'][7][1] = 4\n",
    "    return task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_check(color_list):\n",
    "    tmp = color_list[0]\n",
    "    for i in range(1,len(color_list)):\n",
    "        if set(tmp) != set(color_list[i]):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def create_df(folder_path):\n",
    "    task_names_list = sorted(os.listdir(folder_path))\n",
    "    task_list = []\n",
    "    for task_name in task_names_list: \n",
    "        task_file = str(folder_path / task_name)\n",
    "        with open(task_file, 'r') as f:\n",
    "            task = json.load(f)\n",
    "            if \"test\" not in str(folder_path):\n",
    "                task = preprocess(task, task_name)\n",
    "            task_list.append(task)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['task_name'] = task_names_list\n",
    "    df['task'] = task_list\n",
    "    df['number_of_train_pairs'] = df['task'].apply(lambda x: len(x['train']))\n",
    "    df['number_of_test_pairs'] = df['task'].apply(lambda x: len(x['test']))\n",
    "    \n",
    "    # Compare image sizes\n",
    "    df['inputs_all_have_same_height'] = df['task'].apply(\n",
    "        lambda task: int(len(set([len(example['input']) for example in task['train']])) == 1)\n",
    "    )\n",
    "    df['inputs_all_have_same_width'] = df['task'].apply(\n",
    "        lambda task: int(len(set([len(example['input'][0]) for example in task['train']])) == 1)\n",
    "    )\n",
    "    df['inputs_all_have_same_shape'] = df['inputs_all_have_same_height'] * df['inputs_all_have_same_width']\n",
    "    df['input_height_if_constant'] = df['task'].apply(\n",
    "        lambda task: len(task['train'][0]['input'])\n",
    "                     if (len(set([len(example['input']) for example in task['train']])) == 1)\n",
    "                     else np.nan\n",
    "    )\n",
    "    df['input_width_if_constant'] = df['task'].apply(\n",
    "        lambda task: len(task['train'][0]['input'][0])\n",
    "                     if (len(set([len(example['input'][0]) for example in task['train']])) == 1)\n",
    "                     else np.nan\n",
    "    )\n",
    "    df['outputs_all_have_same_height'] = df['task'].apply(\n",
    "        lambda task: int(len(set([len(example['output']) for example in task['train']])) == 1)\n",
    "    )\n",
    "    df['outputs_all_have_same_width'] = df['task'].apply(\n",
    "        lambda task: int(len(set([len(example['output'][0]) for example in task['train']])) == 1)\n",
    "    )\n",
    "    df['outputs_all_have_same_shape'] = df['outputs_all_have_same_height'] * df['outputs_all_have_same_width']\n",
    "    df['output_height_if_constant'] = df['task'].apply(\n",
    "        lambda task: len(task['train'][0]['output'])\n",
    "                     if (len(set([len(example['output']) for example in task['train']])) == 1)\n",
    "                     else np.nan\n",
    "    )\n",
    "    df['output_width_if_constant'] = df['task'].apply(\n",
    "        lambda task: len(task['train'][0]['output'][0])\n",
    "                     if (len(set([len(example['output'][0]) for example in task['train']])) == 1)\n",
    "                     else np.nan\n",
    "    )  \n",
    "    df['in_each_pair_shape_doesnt_change'] = df['task'].apply(\n",
    "        lambda task: np.prod([int(len(example['input'][0])==len(example['output'][0])\n",
    "                                  and len(example['input'])==len(example['output'])\n",
    "                                 ) for example in task['train']\n",
    "                            ])\n",
    "    )\n",
    "    df['in_each_pair_shape_ratio_is_the_same'] = df['task'].apply(\n",
    "        lambda task: (len(set([len(example['input'][0]) / len(example['output'][0])\n",
    "                                 for example in task['train']]))==1) * (\n",
    "                      len(set([len(example['input']) / len(example['output'])\n",
    "                                 for example in task['train']]))==1)\n",
    "    )\n",
    "    df['o/i_height_ratio_if_constant'] = df['task'].apply(\n",
    "        lambda task: len(task['train'][0]['output']) / len(task['train'][0]['input'])\n",
    "                     if (len(set([len(example['input']) / len(example['output'])\n",
    "                                 for example in task['train']]))==1)\n",
    "                     else np.nan\n",
    "    )\n",
    "    df['o/i_width_ratio_if_constant'] = df['task'].apply(\n",
    "        lambda task: len(task['train'][0]['output'][0]) / len(task['train'][0]['input'][0])\n",
    "                     if (len(set([len(example['input'][0]) / len(example['output'][0])\n",
    "                                 for example in task['train']]))==1)\n",
    "                     else np.nan\n",
    "    )\n",
    "    \n",
    "    # my idea ---------\n",
    "    df[\"same_color_sum\"] = df['task'].apply(lambda task: \n",
    "                        np.all([int(sum(sum(np.array(example['input'])))== sum(sum(np.array(example['output'])))) for example in task['train']]))\n",
    "    \n",
    "    df[\"same_color_sum_in_edge\"] = df['task'].apply(lambda task: \n",
    "                        np.all([int(sum(np.array(example['input'])[0,:]) +sum(np.array(example['input'])[:,0]) + \n",
    "                                    sum(np.array(example['input'])[-1,:]) +sum(np.array(example['input'])[:,-1])\n",
    "                                    == \n",
    "                                    sum(np.array(example['output'])[0,:]) +sum(np.array(example['output'])[:,0]) + \n",
    "                                    sum(np.array(example['output'])[-1,:]) +sum(np.array(example['output'])[:,-1])) for example in task['train']]))\n",
    "    \n",
    "    df[\"io_color_kind_diff\"] = df['task'].apply(lambda task: [len(np.unique(np.array(example['input']))) - len(np.unique(np.array(example['output']))) for example in task['train']])\n",
    "    df[\"io_color_kind_diff_constant\"] = df['io_color_kind_diff'].apply(lambda task: np.unique(np.array(task))[0] if len(np.unique(np.array(task)))==1 else -1)\n",
    "    df[\"output_not_include_0\"] = df['task'].apply(lambda task: np.all([np.all(np.array(example['output']) > 0) for example in task['train']]))\n",
    "    df[\"increase_color_sum\"] = df['task'].apply(lambda task: \n",
    "                        np.all([int(sum(sum(np.array(example['input']))) < sum(sum(np.array(example['output'])))) for example in task['train']]))\n",
    "    df[\"decrease_color_sum\"] = df['task'].apply(lambda task: \n",
    "                        np.all([int(sum(sum(np.array(example['input']))) > sum(sum(np.array(example['output'])))) for example in task['train']]))\n",
    "    # **\n",
    "    df[\"input_color_change_or_not\"] = df['task'].apply(lambda task: color_check([list(np.unique(np.array(example['input']))) for example in task[\"train\"]] ))\n",
    "    # **\n",
    "    df['color_kind_increase'] = df['task'].apply(\n",
    "        lambda task: np.all([len(np.unique(np.array(example['input']))) < len(np.unique(np.array(example['output']))) for example in task['train']]))\n",
    "    df['color_kind_decrease'] = df['task'].apply(\n",
    "        lambda task: np.all([len(np.unique(np.array(example['input']))) > len(np.unique(np.array(example['output']))) for example in task['train']]))\n",
    "    df['smaller_output'] = df['task'].apply(\n",
    "        lambda task: np.all([(np.array(example['input']).shape[0] >= np.array(example['output']).shape[0] and \n",
    "                             np.array(example['input']).shape[1] >= np.array(example['output']).shape[1] and \n",
    "                             (np.array(example['input']).shape[0] > np.array(example['output']).shape[0] or \n",
    "                             np.array(example['input']).shape[1] > np.array(example['output']).shape[1])) for example in task['train']]))\n",
    "    df['larger_output'] = df['task'].apply(\n",
    "        lambda task: np.all([(np.array(example['input']).shape[0] <= np.array(example['output']).shape[0] and \n",
    "                             np.array(example['input']).shape[1] <= np.array(example['output']).shape[1] and \n",
    "                             (np.array(example['input']).shape[0] < np.array(example['output']).shape[0] or \n",
    "                             np.array(example['input']).shape[1] < np.array(example['output']).shape[1])) for example in task['train']]))\n",
    "    return df\n",
    "\n",
    "training_descriptive_df = create_df(train_path)\n",
    "evaluation_descriptive_df = create_df(eval_path)\n",
    "test_descriptive_df = create_df(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(row):\n",
    "    # same shape and color doesn't change in input and color kind decrease\n",
    "    if row[\"in_each_pair_shape_doesnt_change\"] == 1 and row[\"o/i_height_ratio_if_constant\"] ==1 and row[\"o/i_width_ratio_if_constant\"]==1 and row.input_color_change_or_not==1 and row.color_kind_decrease==1:\n",
    "        return 1\n",
    "    # same shape and color doesn't change in input and color kind increase\n",
    "    elif row[\"in_each_pair_shape_doesnt_change\"] == 1 and row[\"o/i_height_ratio_if_constant\"] ==1 and row[\"o/i_width_ratio_if_constant\"]==1 and row.input_color_change_or_not==1 and row.color_kind_increase==1:\n",
    "        return 2\n",
    "    # same shape and color doesn't change in input and color kind same\n",
    "    elif row[\"in_each_pair_shape_doesnt_change\"] == 1 and row[\"o/i_height_ratio_if_constant\"] ==1 and row[\"o/i_width_ratio_if_constant\"]==1 and row.input_color_change_or_not==1 and row.color_kind_increase==0  and row.color_kind_decrease==0:\n",
    "        return 3\n",
    "    # same shape and decrease color sum â†’ xgboost\n",
    "    if row[\"in_each_pair_shape_doesnt_change\"] == 1 and row[\"o/i_height_ratio_if_constant\"] ==1 and row[\"o/i_width_ratio_if_constant\"]==1 and row.input_color_change_or_not==0 and row.color_kind_decrease==1:\n",
    "        return 4\n",
    "    # different shape and decrease color sum\n",
    "    if row[\"in_each_pair_shape_doesnt_change\"] == 1 and row[\"o/i_height_ratio_if_constant\"] ==1 and row[\"o/i_width_ratio_if_constant\"]==1 and row.input_color_change_or_not==0 and row.color_kind_increase==1:\n",
    "        return 5\n",
    "    # different shape and increase color sum\n",
    "    elif row[\"in_each_pair_shape_doesnt_change\"] == 1 and row[\"o/i_height_ratio_if_constant\"] ==1 and row[\"o/i_width_ratio_if_constant\"]==1 and row.input_color_change_or_not==0 and row.color_kind_increase==0  and row.color_kind_decrease==0:\n",
    "        return 6\n",
    "    # otherwise\n",
    "    elif row[\"smaller_output\"] == 1 and row[\"outputs_all_have_same_shape\"] == 1:\n",
    "        return 7\n",
    "    elif row[\"larger_output\"] == 1 and row[\"outputs_all_have_same_shape\"] == 1:\n",
    "        return 8\n",
    "    else:\n",
    "        return 9\n",
    "\n",
    "training_descriptive_df[\"class\"] = training_descriptive_df.apply(lambda x: classification(x), axis=1)\n",
    "evaluation_descriptive_df[\"class\"] = evaluation_descriptive_df.apply(lambda x: classification(x), axis=1)\n",
    "test_descriptive_df[\"class\"] = test_descriptive_df.apply(lambda x: classification(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(inp,eoup,oup):\n",
    "    \"\"\"\n",
    "    Plots the first train and test pairs of a specified task,\n",
    "    using same color scheme as the ARC app\n",
    "    \"\"\"\n",
    "    cmap = colors.ListedColormap(\n",
    "        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n",
    "         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
    "    norm = colors.Normalize(vmin=0, vmax=9)\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15,15))\n",
    "    \n",
    "    axs[0].imshow(inp, cmap=cmap, norm=norm)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('Input')\n",
    "\n",
    "    axs[1].imshow(eoup, cmap=cmap, norm=norm)\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title('Output')\n",
    "    \n",
    "    axs[2].imshow(oup, cmap=cmap, norm=norm)\n",
    "    axs[2].axis('off')\n",
    "    axs[2].set_title('Model prediction')\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_mats(mats):\n",
    "    cmap = colors.ListedColormap(\n",
    "        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n",
    "         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
    "    norm = colors.Normalize(vmin=0, vmax=9)\n",
    "    fig, axs = plt.subplots(1, len(mats), figsize=(15,15))\n",
    "    \n",
    "    for i in range(len(mats)):\n",
    "        axs[i].imshow(mats[i], cmap=cmap, norm=norm)\n",
    "        axs[i].axis('off')\n",
    "        axs[i].set_title('Fig: '+str(i))\n",
    "    \n",
    "    plt.rc('grid', linestyle=\"-\", color='white')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getiorc(pair):\n",
    "    inp = pair[\"input\"]\n",
    "    return pair[\"input\"],pair[\"output\"],len(inp),len(inp[0])\n",
    "    \n",
    "def getAround(i,j,inp,size=1):\n",
    "    #v = [-1,-1,-1,-1,-1,-1,-1,-1,-1]\n",
    "    r,c = len(inp),len(inp[0])\n",
    "    v = []\n",
    "    sc = [0]\n",
    "    for q in range(size):\n",
    "        sc.append(q+1)\n",
    "        sc.append(-(q+1))\n",
    "    for idx,(x,y) in enumerate(product(sc,sc)):\n",
    "        ii = (i+x)\n",
    "        jj = (j+y)\n",
    "        v.append(-1)\n",
    "        if((0<= ii < r) and (0<= jj < c)):\n",
    "            v[idx] = (inp[ii][jj])\n",
    "    return v\n",
    "\n",
    "def getDiagonal(i,j,r,c):\n",
    "    return\n",
    "        \n",
    "    \n",
    "def getX(inp,i,j,size):\n",
    "    z = []\n",
    "    n_inp = np.array(inp)\n",
    "    z.append(i)\n",
    "    z.append(j)\n",
    "    r,c = len(inp),len(inp[0])\n",
    "    for m in range(5):\n",
    "        z.append(i%(m+1))\n",
    "        z.append(j%(m+1))\n",
    "    z.append(i+j)\n",
    "    z.append(i*j)\n",
    "#     z.append(i%j)\n",
    "#     z.append(j%i)\n",
    "    z.append((i+1)/(j+1))\n",
    "    z.append((j+1)/(i+1))\n",
    "    z.append(r)\n",
    "    z.append(c)\n",
    "    z.append(len(np.unique(n_inp[i,:])))\n",
    "    z.append(len(np.unique(n_inp[:,j])))\n",
    "    arnd = getAround(i,j,inp,size)\n",
    "    z.append(len(np.unique(arnd)))\n",
    "    z.extend(arnd)\n",
    "    return z\n",
    "\n",
    "def getXy(inp,oup,size):\n",
    "    x = []\n",
    "    y = []\n",
    "    r,c = len(inp),len(inp[0])\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            x.append(getX(inp,i,j,size))\n",
    "            y.append(oup[i][j])\n",
    "    return x,y\n",
    "    \n",
    "def getBkgColor(task_json):\n",
    "    color_dict = defaultdict(int)\n",
    "    \n",
    "    for pair in task_json['train']:\n",
    "        inp,oup,r,c = getiorc(pair)\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                color_dict[inp[i][j]]+=1\n",
    "    color = -1\n",
    "    max_count = 0\n",
    "    for col,cnt in color_dict.items():\n",
    "        if(cnt > max_count):\n",
    "            color = col\n",
    "            max_count = cnt\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(tasks):\n",
    "    ans = []\n",
    "    for task in tasks:\n",
    "        tmp = {\"input\": 0, \"output\": 0}\n",
    "        tmp[\"input\"] = task[\"input\"]\n",
    "        saigen = np.zeros(np.array(task[\"input\"]).shape)\n",
    "        cur_h, cur_w = np.array(task[\"output\"]).shape\n",
    "        saigen[:cur_h, :cur_w] = np.array(task[\"output\"])\n",
    "        tmp[\"output\"] = saigen.astype(int).tolist()\n",
    "        ans.append(tmp)\n",
    "    return ans, cur_h, cur_w\n",
    "\n",
    "def input_padding(tasks):\n",
    "    ans = []\n",
    "    for task in tasks:\n",
    "        tmp = {\"input\": 0, \"output\": 0}\n",
    "        tmp[\"output\"] = task[\"output\"]\n",
    "        saigen = np.zeros(np.array(task[\"output\"]).shape)\n",
    "        cur_h, cur_w = np.array(task[\"input\"]).shape\n",
    "        saigen[:cur_h, :cur_w] = np.array(task[\"input\"])\n",
    "        tmp[\"input\"] = saigen.astype(int).tolist()\n",
    "        ans.append(tmp)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_colors(inp,oup,bl_cols):\n",
    "    r,c = len(inp),len(inp[0])\n",
    "    return \n",
    "\n",
    "def replace(inp,uni,perm):\n",
    "    # uni = '234' perm = ['5','7','9']\n",
    "    #print(uni,perm)\n",
    "    r_map = { int(c):int(s) for c,s in zip(uni,perm)}\n",
    "    r,c = len(inp),len(inp[0])\n",
    "    rp = np.array(inp).tolist()\n",
    "    #print(rp)\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            if(rp[i][j] in r_map):\n",
    "                rp[i][j] = r_map[rp[i][j]]\n",
    "    return rp\n",
    "            \n",
    "    \n",
    "def augment(inp,oup,bl_cols):\n",
    "    cols = \"0123456789\"\n",
    "    npr_map = [1,9,72,3024,15120,60480,181440,362880,362880]\n",
    "    uni = \"\".join([str(x) for x in np.unique(inp).tolist()])\n",
    "    for c in bl_cols:\n",
    "        cols=cols.replace(str(c),\"\")\n",
    "        uni=uni.replace(str(c),\"\")\n",
    "\n",
    "    exp_size = len(inp)*len(inp[0])*npr_map[len(uni)]\n",
    "    \n",
    "    mod = floor(exp_size/120000)\n",
    "    mod = 1 if mod==0 else mod\n",
    "    \n",
    "    #print(exp_size,mod,len(uni))\n",
    "    result = []\n",
    "    count = 0\n",
    "    for comb in combinations(cols,len(uni)):\n",
    "        for perm in permutations(comb):\n",
    "            count+=1\n",
    "            if(count % mod == 0):\n",
    "                result.append((replace(inp,uni,perm),replace(oup,uni,perm)))\n",
    "    return result\n",
    "            \n",
    "def get_flips(inp,oup):\n",
    "    result = []\n",
    "    n_inp = np.array(inp)\n",
    "    n_oup = np.array(oup)\n",
    "    result.append((np.fliplr(inp).tolist(),np.fliplr(oup).tolist()))\n",
    "    result.append((np.rot90(np.fliplr(inp),1).tolist(),np.rot90(np.fliplr(oup),1).tolist()))\n",
    "    result.append((np.rot90(np.fliplr(inp),2).tolist(),np.rot90(np.fliplr(oup),2).tolist()))\n",
    "    result.append((np.rot90(np.fliplr(inp),3).tolist(),np.rot90(np.fliplr(oup),3).tolist()))\n",
    "    result.append((np.flipud(inp).tolist(),np.flipud(oup).tolist()))\n",
    "    result.append((np.rot90(np.flipud(inp),1).tolist(),np.rot90(np.flipud(oup),1).tolist()))\n",
    "    result.append((np.rot90(np.flipud(inp),2).tolist(),np.rot90(np.flipud(oup),2).tolist()))\n",
    "    result.append((np.rot90(np.flipud(inp),3).tolist(),np.rot90(np.flipud(oup),3).tolist()))\n",
    "    result.append((np.fliplr(np.flipud(inp)).tolist(),np.fliplr(np.flipud(oup)).tolist()))\n",
    "    result.append((np.flipud(np.fliplr(inp)).tolist(),np.flipud(np.fliplr(oup)).tolist()))\n",
    "    return result\n",
    "    \n",
    "def gettaskxy(task_json,aug,around_size,bl_cols,flip=True):    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for pair in task_json['train']:\n",
    "        inp,oup=pair[\"input\"],pair[\"output\"]\n",
    "        tx,ty = getXy(inp,oup,around_size)\n",
    "        X.extend(tx)\n",
    "        Y.extend(ty)\n",
    "        if(flip):\n",
    "            for ainp,aoup in get_flips(inp,oup):\n",
    "                tx,ty = getXy(ainp,aoup,around_size)\n",
    "                X.extend(tx)\n",
    "                Y.extend(ty)\n",
    "                if(aug):\n",
    "                    augs = augment(ainp,aoup,bl_cols)\n",
    "                    for ainp,aoup in augs:\n",
    "                        tx,ty = getXy(ainp,aoup,around_size)\n",
    "                        X.extend(tx)\n",
    "                        Y.extend(ty)\n",
    "        if(aug):\n",
    "            augs = augment(inp,oup,bl_cols)\n",
    "            for ainp,aoup in augs:\n",
    "                tx,ty = getXy(ainp,aoup,around_size)\n",
    "                X.extend(tx)\n",
    "                Y.extend(ty)\n",
    "    return X,Y\n",
    "\n",
    "def test_predict(task_json,model,size):\n",
    "    inp = task_json['test'][0]['input']\n",
    "    eoup = task_json['test'][0]['output']\n",
    "    r,c = len(inp),len(inp[0])\n",
    "    oup = predict(inp,model,size)\n",
    "    return inp,eoup,oup\n",
    "\n",
    "def predict(inp,model,size):\n",
    "    r,c = len(inp),len(inp[0])\n",
    "    oup = np.zeros([r,c],dtype=int)\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            x = getX(inp,i,j,size)\n",
    "            o = int(model.predict([x]))\n",
    "            o = 0 if o<0 else o\n",
    "            oup[i][j]=o\n",
    "    return oup\n",
    "\n",
    "def submit_predict(task_json,model,size):\n",
    "    pred_map = {}\n",
    "    idx=0\n",
    "    for pair in task_json['test']:\n",
    "        inp = pair[\"input\"]\n",
    "        oup = predict(inp,model,size)\n",
    "        pred_map[idx] = oup.tolist()\n",
    "        idx+=1\n",
    "        #plot_result(inp,oup,oup)\n",
    "    return pred_map\n",
    "\n",
    "def dumb_predict(task_json):\n",
    "    pred_map = {}\n",
    "    idx=0\n",
    "    for pair in task_json['test']:\n",
    "        inp = pair[\"input\"]\n",
    "        pred_map[idx] = [[0,0],[0,0]]\n",
    "        idx+=1\n",
    "    return pred_map\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model,task_json,size):\n",
    "    total = 0\n",
    "    for pair in task_json['train']:\n",
    "        inp,oup=pair[\"input\"],pair[\"output\"]\n",
    "        eoup = predict(inp,model,size)\n",
    "        total+= np.sum((np.array(oup) != np.array(eoup)))\n",
    "    return total\n",
    "\n",
    "def get_test_loss(model,task_json,size):\n",
    "    total = 0\n",
    "    for pair in task_json['test']:\n",
    "        inp,oup=pair[\"input\"],pair[\"output\"]\n",
    "        eoup = predict(inp,model,size)\n",
    "        total+= np.sum((np.array(oup) != np.array(eoup)))\n",
    "    return total\n",
    "\n",
    "def get_a_size(task_json):\n",
    "    return 4;\n",
    "\n",
    "def get_bl_cols(task_json):\n",
    "    result = []\n",
    "    bkg_col = getBkgColor(task_json);\n",
    "    result.append(bkg_col)\n",
    "    # num_input,input_cnt,num_output,output_cnt\n",
    "    met_map = {}\n",
    "    for i in range(10):\n",
    "        met_map[i] = [0,0,0,0]\n",
    "        \n",
    "    total_ex = 0\n",
    "    for pair in task_json['train']:\n",
    "        inp,oup=pair[\"input\"],pair[\"output\"]\n",
    "        u,uc = np.unique(inp, return_counts=True)\n",
    "        inp_cnt_map = dict(zip(u,uc))\n",
    "        u,uc = np.unique(oup, return_counts=True)\n",
    "        oup_cnt_map = dict(zip(u,uc))\n",
    "        \n",
    "        for col,cnt in inp_cnt_map.items():\n",
    "            met_map[col][0] = met_map[col][0] + 1\n",
    "            met_map[col][1] = met_map[col][1] + cnt\n",
    "        for col,cnt in oup_cnt_map.items():\n",
    "            met_map[col][2] = met_map[col][2] + 1\n",
    "            met_map[col][3] = met_map[col][3] + cnt\n",
    "        total_ex+=1\n",
    "    \n",
    "    for col,met in met_map.items():\n",
    "        num_input,input_cnt,num_output,output_cnt = met\n",
    "        if(num_input == total_ex or num_output == total_ex):\n",
    "            result.append(col)\n",
    "        elif(num_input == 0 and num_output > 0):\n",
    "            result.append(col)\n",
    "    \n",
    "    result = np.unique(result).tolist()\n",
    "    if(len(result) == 10):\n",
    "        result.append(bkg_col)\n",
    "    return np.unique(result).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattener(pred):\n",
    "    str_pred = str([row for row in pred])\n",
    "    str_pred = str_pred.replace(', ', '')\n",
    "    str_pred = str_pred.replace('[[', '|')\n",
    "    str_pred = str_pred.replace('][', '|')\n",
    "    str_pred = str_pred.replace(']]', '|')\n",
    "    return str_pred\n",
    "\n",
    "def combine_preds(tid,pm1,pm2):\n",
    "    result = []\n",
    "    for i in range(len(pm1)):\n",
    "        tk_s = tid+\"_\"+str(i)\n",
    "        str_pred = flattener(pm1[i])+\" \"+flattener(pm2[i])\n",
    "        #print(tk_s,str_pred)\n",
    "        result.append([tk_s,str_pred])\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19bb5feb.json\n",
      "2037f2c7.json\n",
      "11e1fe23.json\n",
      "332efdb3.json\n",
      "136b0064.json\n",
      "1c56ad9f.json\n",
      "40f6cd08.json\n",
      "292dd178.json\n",
      "12997ef3.json\n",
      "25094a63.json\n",
      "0bb8deee.json\n",
      "2072aba6.json\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-681bc7de6dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mclass_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mtask_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mtask_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0ma_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_a_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0465de719e07>\u001b[0m in \u001b[0;36minput_padding\u001b[0;34m(tasks)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0msaigen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcur_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'output'"
     ]
    }
   ],
   "source": [
    "def inp_oup_dim_same(task_json):\n",
    "    return all([ len(pair[\"input\"]) == len(pair[\"output\"]) and len(pair[\"input\"][0]) == len(pair[\"output\"][0])\n",
    "                for pair in task_json['train']])\n",
    "    \n",
    "solved_task = 0\n",
    "total_task = 0\n",
    "task_ids = []\n",
    "task_preds = []\n",
    "for task_path in test_path.glob(\"*.json\"):\n",
    "    task_json = json.load(open(task_path))\n",
    "    tk_id = str(task_path).split(\"/\")[-1].split(\".\")[0]\n",
    "    print(tk_id+\".json\")\n",
    "    class_num = test_descriptive_df[test_descriptive_df.task_name == tk_id+\".json\"][\"class\"].values[0]\n",
    "    #if(inp_oup_dim_same(task_json)):\n",
    "    if class_num <= 6:\n",
    "        a_size = get_a_size(task_json)\n",
    "        bl_cols = get_bl_cols(task_json)\n",
    "        \n",
    "        isflip = False\n",
    "        X1,Y1 = gettaskxy(task_json,True,1,bl_cols,isflip)\n",
    "        X3,Y3 = gettaskxy(task_json,True,3,bl_cols,isflip)\n",
    "        X5,Y5 = gettaskxy(task_json,True,5,bl_cols,isflip)\n",
    "        \n",
    "        model_1 = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100).fit(X1, Y1)\n",
    "        #model_3 = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100).fit(X3, Y3)\n",
    "        model_5 = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100).fit(X5, Y5)\n",
    "        \n",
    "        pred_map_1 = submit_predict(task_json,model_1,1)\n",
    "        #pred_map_3 = submit_predict(task_json,model_3,3)\n",
    "        pred_map_5 = submit_predict(task_json,model_5,5)\n",
    "        \n",
    "        for tks,str_pred in combine_preds(tk_id,pred_map_1,pred_map_5):\n",
    "            task_ids.append(tks)\n",
    "            task_preds.append(str_pred)\n",
    "        solved_task+=1\n",
    "        \n",
    "    #elif class_num == 7:\n",
    "    #    task_json[\"train\"] = padding(task_json[\"train\"])\n",
    "        \n",
    "    #    a_size = get_a_size(task_json)\n",
    "    #    bl_cols = get_bl_cols(task_json)\n",
    "        \n",
    "    #    isflip = False\n",
    "    #    X1,Y1 = gettaskxy(task_json,True,1,bl_cols,isflip)\n",
    "    #    X3,Y3 = gettaskxy(task_json,True,3,bl_cols,isflip)\n",
    "    #    X5,Y5 = gettaskxy(task_json,True,5,bl_cols,isflip)\n",
    "        \n",
    "    #    model_1 = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100).fit(X1, Y1)\n",
    "        #model_3 = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100).fit(X3, Y3)\n",
    "    #    model_5 = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100).fit(X5, Y5)\n",
    "        \n",
    "    #    pred_map_1 = submit_predict(task_json,model_1,1)\n",
    "        #pred_map_3 = submit_predict(task_json,model_3,3)\n",
    "    #    pred_map_5 = submit_predict(task_json,model_5,5)\n",
    "        \n",
    "    #    for tks,str_pred in combine_preds(tk_id,pred_map_1,pred_map_5):\n",
    "    #        task_ids.append(tks)\n",
    "    #        task_preds.append(str_pred)\n",
    "    #    solved_task+=1\n",
    "\n",
    "    elif class_num == 8:\n",
    "        task_json[\"train\"] = input_padding(task_json[\"train\"])\n",
    "        task_json[\"test\"] = input_padding(task_json[\"test\"])\n",
    "        \n",
    "        a_size = get_a_size(task_json)\n",
    "        bl_cols = get_bl_cols(task_json)\n",
    "        \n",
    "        isflip = False\n",
    "        X1,Y1 = gettaskxy(task_json,True,1,bl_cols,isflip)\n",
    "        X3,Y3 = gettaskxy(task_json,True,3,bl_cols,isflip)\n",
    "        X5,Y5 = gettaskxy(task_json,True,5,bl_cols,isflip)\n",
    "        \n",
    "        model_1 = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100).fit(X1, Y1)\n",
    "        #model_3 = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100).fit(X3, Y3)\n",
    "        model_5 = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100).fit(X5, Y5)\n",
    "        \n",
    "        pred_map_1 = submit_predict(task_json,model_1,1)\n",
    "        #pred_map_3 = submit_predict(task_json,model_3,3)\n",
    "        pred_map_5 = submit_predict(task_json,model_5,5)\n",
    "        \n",
    "        for tks,str_pred in combine_preds(tk_id,pred_map_1,pred_map_5):\n",
    "            task_ids.append(tks)\n",
    "            task_preds.append(str_pred)\n",
    "        solved_task+=1\n",
    "        \n",
    "    else:\n",
    "        pred_map_1 = dumb_predict(task_json)\n",
    "        #pred_map_3 = dumb_predict(task_json)\n",
    "        pred_map_5 = dumb_predict(task_json)\n",
    "        \n",
    "        for tks,str_pred in combine_preds(tk_id,pred_map_1,pred_map_5):\n",
    "            task_ids.append(tks)\n",
    "            task_preds.append(str_pred)\n",
    "            #print(tks,str_pred)\n",
    "        \n",
    "    total_task+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub1 = pd.DataFrame({\"output_id\":task_ids,'output':task_preds})\n",
    "sample_sub1.to_csv(\"submission.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_sub2 = pd.DataFrame({\"output_id\":task_ids2,'output':task_preds2})\n",
    "\n",
    "#final_sub = sample_sub1\n",
    "#final_sub = final_sub.sort_values(by=\"output_id\").reset_index(drop=True)\n",
    "\n",
    "#sample_sub2 = sample_sub2.sort_values(by=\"output_id\")\n",
    "#out1 = final_sub[\"output\"].astype(str).values\n",
    "#out2 = sample_sub2[\"output\"].astype(str).values\n",
    "\n",
    "#merge_output = []\n",
    "#for o1, o2 in zip(out1, out2):\n",
    "#    o = o1.strip().split(\" \")[:2] + o2.strip().split(\" \")[:1]\n",
    "#    o = \" \".join(o[:3])\n",
    "#    merge_output.append(o)\n",
    "    \n",
    "#final_sub[\"output\"] = merge_output\n",
    "#final_sub[\"output\"] = final_sub[\"output\"].astype(str)\n",
    "#final_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
